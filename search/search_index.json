{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ASF HyP3 \u00b6 Alaska Satellite Facility's Hybrid Pluggable Processing Pipeline HyP3 is a service for processing Synthetic Aperture Radar (SAR) imagery that addresses many common issues for users of SAR data: Most SAR datasets require at least some processing to remove distortions before they are analysis-ready SAR processing is computing-intensive Software for SAR processing is complicated to use and/or prohibitively expensive Producing analysis-ready SAR data has a steep learning curve that acts as a barrier to entry HyP3 solves these problems by providing a free service where people can request SAR processing on-demand. These processing requests are picked up by automated systems, which handle the complexity of SAR processing on behalf of the user. HyP3 doesn't require users to have a lot of knowledge of SAR processing before getting started; users only need to submit the input data and set a few optional parameters if desired. With HyP3, analysis-ready products are just a few clicks away. Getting started \u00b6 On Demand products processed by HyP3 can be requested quickly and easily, either by using a web interface or programmatically. Web Access \u00b6 ASF's Data Search Vertex portal provides a rich interface to explore Sentinel-1 acquisitions and find images to submit for On Demand processing. It also provides tools for selecting pairs and stacks for InSAR analysis. Vertex Programmatic Access \u00b6 Requesting and downloading On Demand products can also be done programmatically: HyP3 SDK for Python HyP3 REST API What's New \u00b6 Follow @ASFHyP3 on Twitter, or check our What's New page, to keep up to date on all things HyP3! Contact Us \u00b6 Want to talk about HyP3? We would love to hear from you! Found a bug? Want to request a feature? open an issue General questions? Suggestions? Or just want to talk to the team? chat with us on gitter You can also reach us by email through ASF User Services: Email ASF User Services","title":"Home"},{"location":"#asf-hyp3","text":"Alaska Satellite Facility's Hybrid Pluggable Processing Pipeline HyP3 is a service for processing Synthetic Aperture Radar (SAR) imagery that addresses many common issues for users of SAR data: Most SAR datasets require at least some processing to remove distortions before they are analysis-ready SAR processing is computing-intensive Software for SAR processing is complicated to use and/or prohibitively expensive Producing analysis-ready SAR data has a steep learning curve that acts as a barrier to entry HyP3 solves these problems by providing a free service where people can request SAR processing on-demand. These processing requests are picked up by automated systems, which handle the complexity of SAR processing on behalf of the user. HyP3 doesn't require users to have a lot of knowledge of SAR processing before getting started; users only need to submit the input data and set a few optional parameters if desired. With HyP3, analysis-ready products are just a few clicks away.","title":"ASF HyP3"},{"location":"#getting-started","text":"On Demand products processed by HyP3 can be requested quickly and easily, either by using a web interface or programmatically.","title":"Getting started"},{"location":"#web-access","text":"ASF's Data Search Vertex portal provides a rich interface to explore Sentinel-1 acquisitions and find images to submit for On Demand processing. It also provides tools for selecting pairs and stacks for InSAR analysis. Vertex","title":"Web Access"},{"location":"#programmatic-access","text":"Requesting and downloading On Demand products can also be done programmatically: HyP3 SDK for Python HyP3 REST API","title":"Programmatic Access"},{"location":"#whats-new","text":"Follow @ASFHyP3 on Twitter, or check our What's New page, to keep up to date on all things HyP3!","title":"What's New"},{"location":"#contact-us","text":"Want to talk about HyP3? We would love to hear from you! Found a bug? Want to request a feature? open an issue General questions? Suggestions? Or just want to talk to the team? chat with us on gitter You can also reach us by email through ASF User Services: Email ASF User Services","title":"Contact Us"},{"location":"CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct \u00b6 Our Pledge \u00b6 We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards \u00b6 Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities \u00b6 Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope \u00b6 This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement by emailing the ASF APD/Tools team at UAF-asf-apd@alaska.edu . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines \u00b6 Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction \u00b6 Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning \u00b6 Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban \u00b6 Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban \u00b6 Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Code of Conduct"},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement by emailing the ASF APD/Tools team at UAF-asf-apd@alaska.edu . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"citing-snippet/","text":"To reference HyP3 in manuscripts, cite our documentation available at ASF's hyp3-docs GitHub repository : Hogenson, K., Kristenson, H., Kennedy, J., Johnston, A., Rine, J., Logan, T., Zhu, J., Williams, F., Herrmann, J., Smale, J., & Meyer, F. (2020). Hybrid Pluggable Processing Pipeline (HyP3): A cloud-native infrastructure for generic processing of SAR data [Computer software]. https://doi.org/10.5281/zenodo.4646138","title":"Citing snippet"},{"location":"contact-snippet/","text":"Want to talk about HyP3? We would love to hear from you! Found a bug? Want to request a feature? open an issue General questions? Suggestions? Or just want to talk to the team? chat with us on gitter You can also reach us by email through ASF User Services: Email ASF User Services","title":"Contact snippet"},{"location":"contact/","text":"Contact Us \u00b6 Want to talk about HyP3? We would love to hear from you! Found a bug? Want to request a feature? open an issue General questions? Suggestions? Or just want to talk to the team? chat with us on gitter You can also reach us by email through ASF User Services: Email ASF User Services","title":"Contact Us"},{"location":"contact/#contact-us","text":"Want to talk about HyP3? We would love to hear from you! Found a bug? Want to request a feature? open an issue General questions? Suggestions? Or just want to talk to the team? chat with us on gitter You can also reach us by email through ASF User Services: Email ASF User Services","title":"Contact Us"},{"location":"contributing/","text":"Contributing \u00b6 Thank you for your interest in helping make custom on-demand SAR processing accessible! We're excited you would like to contribute to HyP3! Whether you're finding bugs, adding new features, fixing anything broken, or improving documentation, get started by submitting an issue or pull request! Please read our Code of Conduct before contributing. Issues and Pull Requests are welcome \u00b6 If you have any questions or ideas, or notice any problems or bugs, and want to open an issue, great! We recommend first searching our open issues to see if the issue has already been submitted (we may already be working on it!). If you think your issue is new, you're welcome to create a new issue in our general issues tracker. If you know the specific repository that your issue pertains to, you can use its issues tracker (see our repositories list below). Found a typo, know how to fix a bug, want to update the docs, want to add a new feature? Even better! The smaller the PR, the easier it is to review and test and the more likely it is to be successful. For major contributions, consider opening an issue describing the contribution so we can help guide and breakup the work into digestible pieces. Pull Request Guidelines \u00b6 We ask that you follow these guidelines with your contributions Style \u00b6 We generally follow python community standards ( PEP8 ), except we allow line lengths up to 120 characters. We recommend trying to keep lines 80--100 characters long, but allow up to 120 when it improves readability. Documentation \u00b6 We are working to improve our documentation! For all public-facing functions/methods (not marked internal use ), please include type hints (when reasonable) and a docstring formatted Google style . Tests \u00b6 All of the automated tests for the project need to pass before your submission will be accepted. If you add new functionality, please consider adding tests for that functionality as well. Commits \u00b6 Make small commits that show the individual changes you are making Write descriptive commit messages that explain your changes Example of a good commit message: Improve contributing guidelines. Fixes #10 Improve contributing docs and consolidate them in the standard location https://help.github.com/articles/setting-guidelines-for-repository-contributors/","title":"Contributing"},{"location":"contributing/#contributing","text":"Thank you for your interest in helping make custom on-demand SAR processing accessible! We're excited you would like to contribute to HyP3! Whether you're finding bugs, adding new features, fixing anything broken, or improving documentation, get started by submitting an issue or pull request! Please read our Code of Conduct before contributing.","title":"Contributing"},{"location":"contributing/#issues-and-pull-requests-are-welcome","text":"If you have any questions or ideas, or notice any problems or bugs, and want to open an issue, great! We recommend first searching our open issues to see if the issue has already been submitted (we may already be working on it!). If you think your issue is new, you're welcome to create a new issue in our general issues tracker. If you know the specific repository that your issue pertains to, you can use its issues tracker (see our repositories list below). Found a typo, know how to fix a bug, want to update the docs, want to add a new feature? Even better! The smaller the PR, the easier it is to review and test and the more likely it is to be successful. For major contributions, consider opening an issue describing the contribution so we can help guide and breakup the work into digestible pieces.","title":"Issues and Pull Requests are welcome"},{"location":"contributing/#pull-request-guidelines","text":"We ask that you follow these guidelines with your contributions","title":"Pull Request Guidelines"},{"location":"contributing/#style","text":"We generally follow python community standards ( PEP8 ), except we allow line lengths up to 120 characters. We recommend trying to keep lines 80--100 characters long, but allow up to 120 when it improves readability.","title":"Style"},{"location":"contributing/#documentation","text":"We are working to improve our documentation! For all public-facing functions/methods (not marked internal use ), please include type hints (when reasonable) and a docstring formatted Google style .","title":"Documentation"},{"location":"contributing/#tests","text":"All of the automated tests for the project need to pass before your submission will be accepted. If you add new functionality, please consider adding tests for that functionality as well.","title":"Tests"},{"location":"contributing/#commits","text":"Make small commits that show the individual changes you are making Write descriptive commit messages that explain your changes Example of a good commit message: Improve contributing guidelines. Fixes #10 Improve contributing docs and consolidate them in the standard location https://help.github.com/articles/setting-guidelines-for-repository-contributors/","title":"Commits"},{"location":"dems/","text":"Digital Elevation Models \u00b6 Digital Elevation Models are required when processing SAR data to higher-level products, such as the Radiometric Terrain Correction (RTC) and Interferometric SAR (InSAR) products available On Demand from ASF. ASF uses DEMs that are publicly available and have wide-ranging coverage. In the past, ASF maintained a collection of DEMs that were pre-processed as appropriate for SAR workflows, and applied a preference hierarchy so that the best available DEM in any given area would be automatically selected for processing. With the public release of the GLO-30 Copernicus DEM , we have changed our default DEM strategy to leverage a cloud-hosted copy of the global Copernicus DEM. This is now the default DEM for processing RTC products, and the only option available for processing InSAR products. Copernicus DEM GLO-30 Updated We use the Copernicus DEM GLO-30 Public dataset as our default DEM for RTC and InSAR processing. We have now updated to the 2021 release of the Copernicus DEM GLO-30 Public dataset , which improves coverage over Norway, and includes 5 additional tiles. For more information, see the 'Releases' section of this article . Users still have the option to use the legacy DEMs when processing RTC jobs On Demand in Vertex and when using the API or SDK , but we recommend using the Copernicus DEM whenever possible. When processing InSAR On Demand products, the Copernicus DEM is the only option available. The legacy DEMs are no longer supported for InSAR processing. Table 1 summarizes ASF's DEM sources. Note that in all cases the DEM is reprojected to the UTM Zone (WGS84) appropriate for the granule location, and a geoid correction is applied before being used for processing. For RTC processing, the DEM is resampled to the pixel spacing of the output product. The Copernicus DEM is the only option available for InSAR processing, and the DEM is resampled to twice the pixel spacing of the output InSAR product (160 m for 20x4 looks, 80 m for 10x2 looks). Resolution DEM Vertical Datum Area Posting Priority Medium GLO-30 EGM2008 Global 1 arc second Default High NED13 NAVD88 CONUS, Hawaii, parts of Alaska 1/3 arc seconds 1 Medium SRTMGL1 EGM96 60 N to 57 S latitude 1 arc second 2 Medium NED1 NAVD88 Canada 1 arc second 3 Low NED2 NAVD88 Parts of Alaska 2 arc seconds 4 Table 1: DEMs used for On Demand processing. For RTC products, the Copernicus 30 m DEM is the default, while the other four DEMs are only used if the legacy option is invoked. The Copernicus DEM is the only option available when processing InSAR products. When ordering On-Demand products, you can choose to include a copy of the DEM used for processing in the output product package. For RTC products, this DEM copy is converted to 16-bit signed integer format, but is otherwise the same as the DEM used in the RTC process. For InSAR products, the DEM copy is output in 32-bit float format, and is upsampled from the DEM resolution used for processing to match the pixel spacing of the output InSAR products. Note that the height values will differ from the original source DEM in all cases, due to the geoid correction applied to prepare the DEM for use in SAR processing. Copernicus DEM \u00b6 The GLO-30 Copernicus DEM provides global coverage (with the current exception of an area covering Armenia and Azerbaijan, see Figure 2) at 30-m pixel spacing. When an On Demand job is requested, we download the required DEM tiles from the Copernicus Digital Elevation Model (DEM) GLO-30 Public dataset available in the Registry of Open Data on AWS , managed by Sinergise . We mosaic the tiles and reproject them to the appropriate UTM Zone for the location of the SAR granule to be processed, resampling them as required for processing. A geoid correction is applied before it is used for On Demand processing. Figure 1 shows the coverage of the Copernicus DEM GLO-30 Public dataset, and Figure 2 details the land area currently not covered. Figure 1: Copernicus DEM GLO-30 coverage map Figure 2: Detail of area currently not covered by Copernicus DEM GLO-30 Legacy DEMs \u00b6 The legacy DEMs were pre-processed by ASF to a consistent raster format (GeoTIFF) from the original source formats: height (*.hgt), ESRI ArcGrid (*.adf), etc. Many of the NASA-provided DEMs were provided as orthometric heights with EGM96 vertical datum. These were converted by ASF to ellipsoid heights using the ASF MapReady tool named geoid_adjust . The pixel reference varied from the center (pixel as point) to a corner (pixel as area). The GAMMA software, used to generate the terrain corrected products, uses pixel as area and adjusts DEM coordinates as needed. These processed DEM collections are stored by ASF in AWS. When an On Demand job is requested, the best-available DEM covering the SAR granule is selected, and the necessary tiles are reprojected to a mosaic in the UTM Zone appropriate for the granule location. If legacy DEM processing is selected, one of the following DEMs will be used: The National Elevation Dataset (NED) \u2153 arc second (about 10 m resolution) DEM covers the continental U.S. (CONUS), Hawaii, and parts of Alaska. Shuttle Radar Topography Mission (SRTM) GL1 data at 30 m resolution is used where NED 13 is not available. 1 arc second NED gives coverage of Canada at about 30 m resolution. 2 arc second NED (about 60 m) covers the remaining parts of Alaska above 60 degrees northern latitude. Since more than one DEM may be available in legacy processing, DEMs are selected in priority order as listed in Table 1. DEM coverage of at least 20% from a single DEM source is required for legacy processing to proceed. In no case will the DEM selected be from more than one source; only the single best source of terrain height values is used for a given scene. Figure 3 shows the coverage of the various legacy DEM sources. Figure 3: Coverage of the various legacy DEM sources used for terrain correction Special Use DEMs \u00b6 AutoRIFT , a process developed by the NASA MEaSUREs ITS_LIVE project, processes use a custom Greenland and Antarctica DEM with a 240 m resolution. The DEM, associated process input files, and their details are available on the ITS_LIVE project website.","title":"Digital Elevation Models"},{"location":"dems/#digital-elevation-models","text":"Digital Elevation Models are required when processing SAR data to higher-level products, such as the Radiometric Terrain Correction (RTC) and Interferometric SAR (InSAR) products available On Demand from ASF. ASF uses DEMs that are publicly available and have wide-ranging coverage. In the past, ASF maintained a collection of DEMs that were pre-processed as appropriate for SAR workflows, and applied a preference hierarchy so that the best available DEM in any given area would be automatically selected for processing. With the public release of the GLO-30 Copernicus DEM , we have changed our default DEM strategy to leverage a cloud-hosted copy of the global Copernicus DEM. This is now the default DEM for processing RTC products, and the only option available for processing InSAR products. Copernicus DEM GLO-30 Updated We use the Copernicus DEM GLO-30 Public dataset as our default DEM for RTC and InSAR processing. We have now updated to the 2021 release of the Copernicus DEM GLO-30 Public dataset , which improves coverage over Norway, and includes 5 additional tiles. For more information, see the 'Releases' section of this article . Users still have the option to use the legacy DEMs when processing RTC jobs On Demand in Vertex and when using the API or SDK , but we recommend using the Copernicus DEM whenever possible. When processing InSAR On Demand products, the Copernicus DEM is the only option available. The legacy DEMs are no longer supported for InSAR processing. Table 1 summarizes ASF's DEM sources. Note that in all cases the DEM is reprojected to the UTM Zone (WGS84) appropriate for the granule location, and a geoid correction is applied before being used for processing. For RTC processing, the DEM is resampled to the pixel spacing of the output product. The Copernicus DEM is the only option available for InSAR processing, and the DEM is resampled to twice the pixel spacing of the output InSAR product (160 m for 20x4 looks, 80 m for 10x2 looks). Resolution DEM Vertical Datum Area Posting Priority Medium GLO-30 EGM2008 Global 1 arc second Default High NED13 NAVD88 CONUS, Hawaii, parts of Alaska 1/3 arc seconds 1 Medium SRTMGL1 EGM96 60 N to 57 S latitude 1 arc second 2 Medium NED1 NAVD88 Canada 1 arc second 3 Low NED2 NAVD88 Parts of Alaska 2 arc seconds 4 Table 1: DEMs used for On Demand processing. For RTC products, the Copernicus 30 m DEM is the default, while the other four DEMs are only used if the legacy option is invoked. The Copernicus DEM is the only option available when processing InSAR products. When ordering On-Demand products, you can choose to include a copy of the DEM used for processing in the output product package. For RTC products, this DEM copy is converted to 16-bit signed integer format, but is otherwise the same as the DEM used in the RTC process. For InSAR products, the DEM copy is output in 32-bit float format, and is upsampled from the DEM resolution used for processing to match the pixel spacing of the output InSAR products. Note that the height values will differ from the original source DEM in all cases, due to the geoid correction applied to prepare the DEM for use in SAR processing.","title":"Digital Elevation Models"},{"location":"dems/#copernicus-dem","text":"The GLO-30 Copernicus DEM provides global coverage (with the current exception of an area covering Armenia and Azerbaijan, see Figure 2) at 30-m pixel spacing. When an On Demand job is requested, we download the required DEM tiles from the Copernicus Digital Elevation Model (DEM) GLO-30 Public dataset available in the Registry of Open Data on AWS , managed by Sinergise . We mosaic the tiles and reproject them to the appropriate UTM Zone for the location of the SAR granule to be processed, resampling them as required for processing. A geoid correction is applied before it is used for On Demand processing. Figure 1 shows the coverage of the Copernicus DEM GLO-30 Public dataset, and Figure 2 details the land area currently not covered. Figure 1: Copernicus DEM GLO-30 coverage map Figure 2: Detail of area currently not covered by Copernicus DEM GLO-30","title":"Copernicus DEM"},{"location":"dems/#legacy-dems","text":"The legacy DEMs were pre-processed by ASF to a consistent raster format (GeoTIFF) from the original source formats: height (*.hgt), ESRI ArcGrid (*.adf), etc. Many of the NASA-provided DEMs were provided as orthometric heights with EGM96 vertical datum. These were converted by ASF to ellipsoid heights using the ASF MapReady tool named geoid_adjust . The pixel reference varied from the center (pixel as point) to a corner (pixel as area). The GAMMA software, used to generate the terrain corrected products, uses pixel as area and adjusts DEM coordinates as needed. These processed DEM collections are stored by ASF in AWS. When an On Demand job is requested, the best-available DEM covering the SAR granule is selected, and the necessary tiles are reprojected to a mosaic in the UTM Zone appropriate for the granule location. If legacy DEM processing is selected, one of the following DEMs will be used: The National Elevation Dataset (NED) \u2153 arc second (about 10 m resolution) DEM covers the continental U.S. (CONUS), Hawaii, and parts of Alaska. Shuttle Radar Topography Mission (SRTM) GL1 data at 30 m resolution is used where NED 13 is not available. 1 arc second NED gives coverage of Canada at about 30 m resolution. 2 arc second NED (about 60 m) covers the remaining parts of Alaska above 60 degrees northern latitude. Since more than one DEM may be available in legacy processing, DEMs are selected in priority order as listed in Table 1. DEM coverage of at least 20% from a single DEM source is required for legacy processing to proceed. In no case will the DEM selected be from more than one source; only the single best source of terrain height values is used for a given scene. Figure 3 shows the coverage of the various legacy DEM sources. Figure 3: Coverage of the various legacy DEM sources used for terrain correction","title":"Legacy DEMs"},{"location":"dems/#special-use-dems","text":"AutoRIFT , a process developed by the NASA MEaSUREs ITS_LIVE project, processes use a custom Greenland and Antarctica DEM with a 240 m resolution. The DEM, associated process input files, and their details are available on the ITS_LIVE project website.","title":"Special Use DEMs"},{"location":"how_it_works/","text":"How it Works \u00b6 HyP3 is built around three core concepts: Platform, Plugins, and Products. Platform \u00b6 The HyP3 platform makes it easy for users to request processing, monitor their requests, and download processed products. The platform delegates each processing request to a plugin on the user's behalf. A deployment of the HyP3 platform can be integrated with any number of plugins. Plugins \u00b6 Plugins are the workhorses of HyP3. Each plugin implements a particular processing workflow and produces a product. At their most basic level, HyP3 plugins are Docker containers that handle the entire processing workflow for a single product, including: Marshaling the required input data performing any needed transformations and computations on the data creating the final product uploading the product to an AWS S3 bucket for distribution Plugins only need to define a simple interface (entrypoint) that HyP3 understands and is used to run the container. By encapsulating the entire workflow for generating a single product, HyP3 can arbitrarily scale to meet user need. Products \u00b6 Products are the end result of processing, typically one or more data files. For more information about our current products, see our products page .","title":"Architecture"},{"location":"how_it_works/#how-it-works","text":"HyP3 is built around three core concepts: Platform, Plugins, and Products.","title":"How it Works"},{"location":"how_it_works/#platform","text":"The HyP3 platform makes it easy for users to request processing, monitor their requests, and download processed products. The platform delegates each processing request to a plugin on the user's behalf. A deployment of the HyP3 platform can be integrated with any number of plugins.","title":"Platform"},{"location":"how_it_works/#plugins","text":"Plugins are the workhorses of HyP3. Each plugin implements a particular processing workflow and produces a product. At their most basic level, HyP3 plugins are Docker containers that handle the entire processing workflow for a single product, including: Marshaling the required input data performing any needed transformations and computations on the data creating the final product uploading the product to an AWS S3 bucket for distribution Plugins only need to define a simple interface (entrypoint) that HyP3 understands and is used to run the container. By encapsulating the entire workflow for generating a single product, HyP3 can arbitrarily scale to meet user need.","title":"Plugins"},{"location":"how_it_works/#products","text":"Products are the end result of processing, typically one or more data files. For more information about our current products, see our products page .","title":"Products"},{"location":"plugins/","text":"Plugins \u00b6 Plugins are the science backbone of HyP3; they do all of the data processing and product generation. Plugins can be added to HyP3 to generate new science products, or support different tools/software/algorithms/options/etc that are not currently supported by HyP3. How plugins work \u00b6 At their most basic level, HyP3 plugins are Docker containers with an interface (entrypoint) HyP3 understands. Plugins handle the entire processing workflow for a single product, including: Marshaling the required input data performing any needed transformations and computations on the data creating the final product uploading the product to an AWS S3 bucket for distribution By encapsulating the entire workflow for generating a single product, HyP3 can arbitrarily scale to meet user need. Developing a plugin \u00b6 To create a new HyP3 plugin, we recommend starting from a Minimal Working Example (MWE) of generating the product you're plugin will generate. Importantly, the MWE should be entirely self contained, and include all the necessary data to generate the product. Once a MWE is developed, it's important to define your plugin's interface -- this is where HyP3 connects the product generation and users. When designing the interface, you may find it helpful to ask yourself: what options do I want to provide to users? what's the minimal set information I need to gather from users? is this information easily input by users? is this information serializable? For example, can the information be written in a JSON file? could I define this information more simply? Once a MWE is developed and an interface is defined, you can use our HyP3 plugin cookiecuter to help you build a plugin that conforms to the plugin requirements. Plugin requirements \u00b6 In order to be supported by HyP3, a plugin must meet a few requirements: the plugin must be a Docker image that is hosted in a repository where HyP3 will be able to pull it the plugin's entrypoint must minimally accept the following arguments --bucket BUCKET-NAME where BUCKET-NAME is the name of an AWS S3 bucket that output products will be uploaded to --bucket-prefix BUCKET-PREFIX where BUCKET-PREFIX is a string appended to the key of any file uploaded to AWS S3 (this is effectively a subfolder in AWS S3) --username USER where USER is the username used to authenticate to EarthData Login --password PASSWORD where PASSWORD is the password used to authenticate to EarthData Login any necessary user input should be able to be provided through entrypoint arguments when uploading files to the S3 Bucket products files must be tagged with filetype: product if you wish to upload thumbnails or browse images, they must be tagged filetype: thumbnail or filetype: browse respectively Note: the aws subpackage of hyp3lib provides helper functions for tagging and uploading files Add the plugin to HyP3 \u00b6 Once the plugin itself is created, it can be added to the HyP3 system by... TBD.","title":"Plugins"},{"location":"plugins/#plugins","text":"Plugins are the science backbone of HyP3; they do all of the data processing and product generation. Plugins can be added to HyP3 to generate new science products, or support different tools/software/algorithms/options/etc that are not currently supported by HyP3.","title":"Plugins"},{"location":"plugins/#how-plugins-work","text":"At their most basic level, HyP3 plugins are Docker containers with an interface (entrypoint) HyP3 understands. Plugins handle the entire processing workflow for a single product, including: Marshaling the required input data performing any needed transformations and computations on the data creating the final product uploading the product to an AWS S3 bucket for distribution By encapsulating the entire workflow for generating a single product, HyP3 can arbitrarily scale to meet user need.","title":"How plugins work"},{"location":"plugins/#developing-a-plugin","text":"To create a new HyP3 plugin, we recommend starting from a Minimal Working Example (MWE) of generating the product you're plugin will generate. Importantly, the MWE should be entirely self contained, and include all the necessary data to generate the product. Once a MWE is developed, it's important to define your plugin's interface -- this is where HyP3 connects the product generation and users. When designing the interface, you may find it helpful to ask yourself: what options do I want to provide to users? what's the minimal set information I need to gather from users? is this information easily input by users? is this information serializable? For example, can the information be written in a JSON file? could I define this information more simply? Once a MWE is developed and an interface is defined, you can use our HyP3 plugin cookiecuter to help you build a plugin that conforms to the plugin requirements.","title":"Developing a plugin"},{"location":"plugins/#plugin-requirements","text":"In order to be supported by HyP3, a plugin must meet a few requirements: the plugin must be a Docker image that is hosted in a repository where HyP3 will be able to pull it the plugin's entrypoint must minimally accept the following arguments --bucket BUCKET-NAME where BUCKET-NAME is the name of an AWS S3 bucket that output products will be uploaded to --bucket-prefix BUCKET-PREFIX where BUCKET-PREFIX is a string appended to the key of any file uploaded to AWS S3 (this is effectively a subfolder in AWS S3) --username USER where USER is the username used to authenticate to EarthData Login --password PASSWORD where PASSWORD is the password used to authenticate to EarthData Login any necessary user input should be able to be provided through entrypoint arguments when uploading files to the S3 Bucket products files must be tagged with filetype: product if you wish to upload thumbnails or browse images, they must be tagged filetype: thumbnail or filetype: browse respectively Note: the aws subpackage of hyp3lib provides helper functions for tagging and uploading files","title":"Plugin requirements"},{"location":"plugins/#add-the-plugin-to-hyp3","text":"Once the plugin itself is created, it can be added to the HyP3 system by... TBD.","title":"Add the plugin to HyP3"},{"location":"products/","text":"Available HyP3 Products \u00b6 RTC \u00b6 SAR datasets inherently contain geometric and radiometric distortions due to terrain being imaged by a side-looking instrument. Radiometric Terrain Correction (RTC) removes these distortions and creates analysis-ready data suitable for use in GIS applications. RTC processing is a required first step for many amplitude-based SAR applications. Sentinel-1 RTC products are generated leveraging GAMMA Software. Products are distributed as UTM-projected GeoTIFFs with a pixel spacing of 30 meters. To learn more, visit the ASF Sentinel-1 RTC Product Guide . For step-by-step instructions for searching for, ordering, downloading and using On Demand RTC products, visit our RTC On Demand! story map. A Digital Elevation Model (DEM) is required for processing RTC. ASF uses the best publicly-available DEM with full coverage of the processing area. To learn more, visit Digital Elevation Models . InSAR \u00b6 Interferometric SAR (InSAR) uses the phase differences from repeat passes over the same area to identify regions where the distance between the sensor and the Earth's surface has changed. This allows for the detection and quantification of deformation or movement. To learn more, visit the ASF Sentinel-1 InSAR Product Guide . Use caution when generating interferograms for areas with extensive/dense vegetation cover. Because Sentinel-1 is a C-band sensor, the waves will not penetrate very deeply into vegetation. Imagery of densely vegetated areas likely represents the top of the canopy rather than the actual terrain. In addition, vegetated areas tend to have low coherence, because plants can grow or move from one acquisition to the next. For step-by-step instructions for searching for, ordering and downloading On Demand InSAR products, visit our InSAR On Demand! story map. A Digital Elevation Model (DEM) is required for processing InSAR. ASF uses the best publicly-available DEM with full coverage of the processing area. To learn more, visit Digital Elevation Models . autoRIFT \u00b6 AutoRIFT produces a velocity map from observed motion using a feature tracking algorithm developed as part of the NASA MEaSUREs ITS_LIVE project. To learn more, visit the ITS_LIVE project website. A Digital Elevation Model (DEM) is required for autoRIFT processing. ASF uses the best publicly-available DEM with full coverage of the processing area. To learn more, visit Digital Elevation Models .","title":"Products"},{"location":"products/#available-hyp3-products","text":"","title":"Available HyP3 Products"},{"location":"products/#rtc","text":"SAR datasets inherently contain geometric and radiometric distortions due to terrain being imaged by a side-looking instrument. Radiometric Terrain Correction (RTC) removes these distortions and creates analysis-ready data suitable for use in GIS applications. RTC processing is a required first step for many amplitude-based SAR applications. Sentinel-1 RTC products are generated leveraging GAMMA Software. Products are distributed as UTM-projected GeoTIFFs with a pixel spacing of 30 meters. To learn more, visit the ASF Sentinel-1 RTC Product Guide . For step-by-step instructions for searching for, ordering, downloading and using On Demand RTC products, visit our RTC On Demand! story map. A Digital Elevation Model (DEM) is required for processing RTC. ASF uses the best publicly-available DEM with full coverage of the processing area. To learn more, visit Digital Elevation Models .","title":"RTC"},{"location":"products/#insar","text":"Interferometric SAR (InSAR) uses the phase differences from repeat passes over the same area to identify regions where the distance between the sensor and the Earth's surface has changed. This allows for the detection and quantification of deformation or movement. To learn more, visit the ASF Sentinel-1 InSAR Product Guide . Use caution when generating interferograms for areas with extensive/dense vegetation cover. Because Sentinel-1 is a C-band sensor, the waves will not penetrate very deeply into vegetation. Imagery of densely vegetated areas likely represents the top of the canopy rather than the actual terrain. In addition, vegetated areas tend to have low coherence, because plants can grow or move from one acquisition to the next. For step-by-step instructions for searching for, ordering and downloading On Demand InSAR products, visit our InSAR On Demand! story map. A Digital Elevation Model (DEM) is required for processing InSAR. ASF uses the best publicly-available DEM with full coverage of the processing area. To learn more, visit Digital Elevation Models .","title":"InSAR"},{"location":"products/#autorift","text":"AutoRIFT produces a velocity map from observed motion using a feature tracking algorithm developed as part of the NASA MEaSUREs ITS_LIVE project. To learn more, visit the ITS_LIVE project website. A Digital Elevation Model (DEM) is required for autoRIFT processing. ASF uses the best publicly-available DEM with full coverage of the processing area. To learn more, visit Digital Elevation Models .","title":"autoRIFT"},{"location":"tutorials/","text":"HyP3 Tutorials \u00b6 Jupyter Notebooks \u00b6 We provide step-by-step tutorials for using HyP3 programmatically via Jupyter Notebooks. Using the HyP3 Python SDK -- This notebook walks through ordering and accessing RTC, InSAR, and autoRIFT products in Python using the HyP3 SDK. Time series analysis with HyP3 and MintPy -- This notebook walks through performing a time-series analysis of the 2019 Ridgecrest, CA earthquake with HyP3 On Demand InSAR products and MintPy. StoryMaps \u00b6 ASF provides a variety of interactive StoryMap tutorials focused on accessing and using Synthetic Aperture Radar (SAR) data available from ASF. They can all be accessed here: StoryMap Tutorials The StoryMap collection includes step-by-step tutorials for ordering and accessing RTC and InSAR products in Vertex.","title":"Tutorials"},{"location":"tutorials/#hyp3-tutorials","text":"","title":"HyP3 Tutorials"},{"location":"tutorials/#jupyter-notebooks","text":"We provide step-by-step tutorials for using HyP3 programmatically via Jupyter Notebooks. Using the HyP3 Python SDK -- This notebook walks through ordering and accessing RTC, InSAR, and autoRIFT products in Python using the HyP3 SDK. Time series analysis with HyP3 and MintPy -- This notebook walks through performing a time-series analysis of the 2019 Ridgecrest, CA earthquake with HyP3 On Demand InSAR products and MintPy.","title":"Jupyter Notebooks"},{"location":"tutorials/#storymaps","text":"ASF provides a variety of interactive StoryMap tutorials focused on accessing and using Synthetic Aperture Radar (SAR) data available from ASF. They can all be accessed here: StoryMap Tutorials The StoryMap collection includes step-by-step tutorials for ordering and accessing RTC and InSAR products in Vertex.","title":"StoryMaps"},{"location":"usage_guidelines/","text":"Product Usage Guidelines \u00b6 When using this data in a publication or presentation, we ask that you include the acknowledgement provided with each product. DOIs are also provided for citation when discussing the HyP3 software or plugins. For multi-file products, the acknowledgement and relevant DOIs are included in the *.README.md.txt file. For netCDF products, the acknowledgement is included in the source global attribute and the DOIs are included in the references global attribute. To reference HyP3 in manuscripts, cite our documentation available at ASF's hyp3-docs GitHub repository : Hogenson, K., Kristenson, H., Kennedy, J., Johnston, A., Rine, J., Logan, T., Zhu, J., Williams, F., Herrmann, J., Smale, J., & Meyer, F. (2020). Hybrid Pluggable Processing Pipeline (HyP3): A cloud-native infrastructure for generic processing of SAR data [Computer software]. https://doi.org/10.5281/zenodo.4646138","title":"Usage Guidelines"},{"location":"usage_guidelines/#product-usage-guidelines","text":"When using this data in a publication or presentation, we ask that you include the acknowledgement provided with each product. DOIs are also provided for citation when discussing the HyP3 software or plugins. For multi-file products, the acknowledgement and relevant DOIs are included in the *.README.md.txt file. For netCDF products, the acknowledgement is included in the source global attribute and the DOIs are included in the references global attribute. To reference HyP3 in manuscripts, cite our documentation available at ASF's hyp3-docs GitHub repository : Hogenson, K., Kristenson, H., Kennedy, J., Johnston, A., Rine, J., Logan, T., Zhu, J., Williams, F., Herrmann, J., Smale, J., & Meyer, F. (2020). Hybrid Pluggable Processing Pipeline (HyP3): A cloud-native infrastructure for generic processing of SAR data [Computer software]. https://doi.org/10.5281/zenodo.4646138","title":"Product Usage Guidelines"},{"location":"using-snippet/","text":"On Demand products processed by HyP3 can be requested quickly and easily, either by using a web interface or programmatically. Web Access \u00b6 ASF's Data Search Vertex portal provides a rich interface to explore Sentinel-1 acquisitions and find images to submit for On Demand processing. It also provides tools for selecting pairs and stacks for InSAR analysis. Vertex Programmatic Access \u00b6 Requesting and downloading On Demand products can also be done programmatically: HyP3 SDK for Python HyP3 REST API","title":"Using snippet"},{"location":"using-snippet/#web-access","text":"ASF's Data Search Vertex portal provides a rich interface to explore Sentinel-1 acquisitions and find images to submit for On Demand processing. It also provides tools for selecting pairs and stacks for InSAR analysis. Vertex","title":"Web Access"},{"location":"using-snippet/#programmatic-access","text":"Requesting and downloading On Demand products can also be done programmatically: HyP3 SDK for Python HyP3 REST API","title":"Programmatic Access"},{"location":"using/","text":"Using ASF HyP3 \u00b6 On Demand products processed by HyP3 can be requested quickly and easily, either by using a web interface or programmatically. Web Access \u00b6 ASF's Data Search Vertex portal provides a rich interface to explore Sentinel-1 acquisitions and find images to submit for On Demand processing. It also provides tools for selecting pairs and stacks for InSAR analysis. Vertex Programmatic Access \u00b6 Requesting and downloading On Demand products can also be done programmatically: HyP3 SDK for Python HyP3 REST API Citing HyP3 \u00b6 To reference HyP3 in manuscripts, cite our documentation available at ASF's hyp3-docs GitHub repository : Hogenson, K., Kristenson, H., Kennedy, J., Johnston, A., Rine, J., Logan, T., Zhu, J., Williams, F., Herrmann, J., Smale, J., & Meyer, F. (2020). Hybrid Pluggable Processing Pipeline (HyP3): A cloud-native infrastructure for generic processing of SAR data [Computer software]. https://doi.org/10.5281/zenodo.4646138 See the Usage Guidelines section for more information on citing and/or acknowledging On Demand products.","title":"Using HyP3"},{"location":"using/#using-asf-hyp3","text":"On Demand products processed by HyP3 can be requested quickly and easily, either by using a web interface or programmatically.","title":"Using ASF HyP3"},{"location":"using/#web-access","text":"ASF's Data Search Vertex portal provides a rich interface to explore Sentinel-1 acquisitions and find images to submit for On Demand processing. It also provides tools for selecting pairs and stacks for InSAR analysis. Vertex","title":"Web Access"},{"location":"using/#programmatic-access","text":"Requesting and downloading On Demand products can also be done programmatically: HyP3 SDK for Python HyP3 REST API","title":"Programmatic Access"},{"location":"using/#citing-hyp3","text":"To reference HyP3 in manuscripts, cite our documentation available at ASF's hyp3-docs GitHub repository : Hogenson, K., Kristenson, H., Kennedy, J., Johnston, A., Rine, J., Logan, T., Zhu, J., Williams, F., Herrmann, J., Smale, J., & Meyer, F. (2020). Hybrid Pluggable Processing Pipeline (HyP3): A cloud-native infrastructure for generic processing of SAR data [Computer software]. https://doi.org/10.5281/zenodo.4646138 See the Usage Guidelines section for more information on citing and/or acknowledging On Demand products.","title":"Citing HyP3"},{"location":"v2-transition/","text":"Welcome to HyP3 v2 \u00b6 As of September 30, 2021, our beta HyP3 service available at https://hyp3.asf.alaska.edu/ has been retired in favor of our new On Demand service powered by HyP3 version 2 (hereafter, just \"HyP3\"). On Demand processing through HyP3 is now available directly in Vertex , ASF's data search portal. Vertex provides a friendly interface to request processing jobs and review previous jobs. To learn how to request jobs through Vertex, please consult the following resources: Vertex On Demand video tutorial InSAR On Demand story map RTC On Demand story map For more information, check out our full documentation at https://hyp3-docs.asf.alaska.edu/ . If you have any comments, questions or concerns, please reach out to us! We love feedback. Contact Us \u00b6 Want to talk about HyP3? We would love to hear from you! Found a bug? Want to request a feature? open an issue General questions? Suggestions? Or just want to talk to the team? chat with us on gitter You can also reach us by email through ASF User Services: Email ASF User Services","title":"Welcome to HyP3 v2"},{"location":"v2-transition/#welcome-to-hyp3-v2","text":"As of September 30, 2021, our beta HyP3 service available at https://hyp3.asf.alaska.edu/ has been retired in favor of our new On Demand service powered by HyP3 version 2 (hereafter, just \"HyP3\"). On Demand processing through HyP3 is now available directly in Vertex , ASF's data search portal. Vertex provides a friendly interface to request processing jobs and review previous jobs. To learn how to request jobs through Vertex, please consult the following resources: Vertex On Demand video tutorial InSAR On Demand story map RTC On Demand story map For more information, check out our full documentation at https://hyp3-docs.asf.alaska.edu/ . If you have any comments, questions or concerns, please reach out to us! We love feedback.","title":"Welcome to HyP3 v2"},{"location":"v2-transition/#contact-us","text":"Want to talk about HyP3? We would love to hear from you! Found a bug? Want to request a feature? open an issue General questions? Suggestions? Or just want to talk to the team? chat with us on gitter You can also reach us by email through ASF User Services: Email ASF User Services","title":"Contact Us"},{"location":"whats_new/","text":"What's New \u00b6 Follow @ASFHyP3 on Twitter to keep up to date on all things HyP3! Tweets by ASFHyP3","title":"What's New"},{"location":"whats_new/#whats-new","text":"Follow @ASFHyP3 on Twitter to keep up to date on all things HyP3! Tweets by ASFHyP3","title":"What's New"},{"location":"guides/insar_product_guide/","text":"Sentinel-1 InSAR Product Guide \u00b6 This document is a guide for users of Interferometric Synthetic Aperture Radar (InSAR) Sentinel-1 products generated by the Alaska Satellite Facility (ASF). Users can request InSAR products On Demand in ASF's Vertex data portal, or make use of our HyP3 Python SDK or API . Input pair selection in Vertex uses either the Baseline Tool or the SBAS Tool search interfaces. For a step-by-step tutorial on ordering On-Demand InSAR Products using Vertex, visit our InSAR On Demand story map . To learn more about the files included in the On Demand InSAR product packages and how to work with them, refer to our Exploring Sentinel-1 InSAR story map Users are cautioned to read the sections on limitations and error sources in InSAR products before attempting to use InSAR data. For a more complete description of the properties of SAR, see our Introduction to SAR guide. Introduction \u00b6 Interferometric Synthetic Aperture Radar (InSAR) processing uses two SAR images collected over the same area to determine geometric properties of the surface. Missions such as Sentinel-1 are designed for monitoring surface deformation using InSAR, which is optimal when acquisitions are made from a consistent location in space ( short perpendicular baseline ) over regular time intervals. The phase measurements of two SAR images acquired at different times from the same place in orbit are differenced to detect and quantify surface changes, such as deformation caused by earthquakes, volcanoes, or groundwater subsidence. InSAR can also be used to generate digital elevation models, but the optimal mission for DEM generation has the opposite characteristics of the Sentinel-1 mission. Topography is best mapped when the two acquisitions are obtained as close together as possible in time ( short temporal baseline ), but from different vantage points in space (larger perpendicular baseline than would be optimal for deformation mapping). Brief Overview of InSAR \u00b6 SAR is an active sensor that transmits pulses and listens for echoes. These echoes are recorded in phase and amplitude, with the phase being used to determine the distance from the sensor to the target and the amplitude yielding information about the roughness and dielectric constant of that target. Figure 1: Two passes of an imaging SAR taken at time T 0 and T 0 + \u2206t, will give two distances to the ground, R 1 and R 2 . A difference between R 1 and R 2 shows motion on the ground. In this case, a subsidence makes R 2 greater than R 1 . Credit: TRE ALTAMIRA InSAR exploits the phase difference between two SAR images to create an interferogram that shows where the phase and, therefore, the distance to the target has changed from one pass to the next, as illustrated in Figure 1. There are several factors that influence the interferogram, including earth curvature, topographic effects, atmospheric delays, surface motion, and noise. With proper processing, Sentinel-1 InSAR can be used to detect changes in the earth's surface down to the centimeter scale. Applications include volcanic deformation, subsidence, landslide detection, and earthquake assessment. Wavelengths \u00b6 The SAR sensors on the Sentinel-1 satellites transmit C-band signals, with a wavelength of 5.6 cm. The signal wavelength impacts the penetration capability of the signal, so it is important to be aware of the sensor wavelength when working with SAR datasets. C-band SAR will penetrate more deeply into canopy or surfaces than an X-band signal, but not nearly as deep as an L-band SAR signal, which, with a wavelength on the order of 25 cm, is better able to penetrate canopy and return signals from the forest floor. Different wavelengths are also sensitive to different levels of deformation. To detect very small changes over relatively short periods of time, you may require a signal with a smaller wavelength (such as X-band). However, signals with shorter wavelengths are also more prone to decorrelation due to small changes in surface conditions such as vegetation growth. For slower processes that require a longer time interval to detect movement, longer wavelengths (such as L-band) may be necessary. C-band sits in the middle. It can detect fairly small changes over fairly short periods of time, but is not as sensitive to small changes as X-band or as able to monitor surface dynamics under canopy as L-band. Polarizations \u00b6 Polarization refers to the direction of travel of an electromagnetic wave. A horizontal wave is transmitted so that it oscillates in a plane parallel to the surface imaged, while a vertical wave oscillates in a plane perpendicular to the surface imaged. Most modern SAR systems can transmit chirps with either a horizontal or vertical polarization. In addition, some of these sensors can listen for either horizontal or vertical backscatter. This gives rise to 4 different types of returns: HH, HV, VV, and VH, with the first letter indicating the transmission method and the second the receive method. For example, VH is a vertically polarized transmit signal with horizontally polarized echoes recorded. For InSAR applications, processing is generally performed on the co-pol (VV or HH) data and not on the cross-pol (VH or HV) data. Also, each image used in an InSAR pair is required to be the same polarization - two HH images of the same area could form a valid pair, while a single HH with a single VV of the same area would not. Baselines \u00b6 Perpendicular Baseline \u00b6 The term baseline refers to the physical distance between the two vantage points from which images used as an InSAR pair are acquired. The baseline is decomposed into perpendicular (also called normal) and parallel components, as shown in Figure 2. To monitor surface deformation, the perpendicular baseline for the two acquisitions should be very small in order to maximize the coherence of the phase measurements. In order to determine topography, two slightly different vantage points are required. Sensitivity to topography depends on the perpendicular baseline, the sensor wavelength, the distance between the satellite and the ground, and the sensor look angle. Figure 2: Geometry of InSAR baselines. Two satellite passes image the same area on the ground from positions S 1 and S 2 , resulting in a baseline of B, which can be decomposed into perpendicular (B \u27c2 ) and parallel (B \u2225 ) components. Here Y is the direction of travel, referred to as the along-track or azimuth direction, and X is the direction perpendicular to motion, referred to as the cross-track or range direction. Credit: ASF Temporal Baseline \u00b6 In contrast to the (physical) baseline, the temporal baseline refers to the time separation between imaging passes. Along-track interferometry measures motion in the millisecond to second range. This technique can detect ocean currents and rapidly moving objects like boats. Differential interferometry is the standard method used to detect motion in the range of days to years. This is the type of interferometry that is performed by the Sentinel-1 HyP3 InSAR processing algorithm. Table 1 lists different temporal baselines, their common names, and what they can be used to measure. Duration Known as Measurement of ms to sec along-track ocean currents, moving object detection, MTI days differential glacier/ice fields/lava flows, surface water extent, hydrology days to years differential subsidence, seismic events, volcanic activity, crustal displacement Table 1: Temporal baselines and what they measure. Different geophysical phenomena can be detected based upon the temporal baseline. In general, the longer the temporal baseline, the smaller the motion that can be detected. Critical Baseline \u00b6 Large baselines are better than small for topographic mapping. However, as the baseline increases, coherence decreases. At some point, it is impossible to create an interferogram because of baseline decorrelation. The maximum viable baseline per platform, referred to as the critical baseline , is a function of the distance to the ground, the wavelength, and the viewing geometry of the platform. For Sentinel-1, this critical baseline is about 5 km. In practice, if the perpendicular baseline between images is more than 3/4 of the critical baseline, interferogram creation will be problematic due to the level of noise. For deformation mapping, it is best to minimize the perpendicular baseline whenever possible, but there may be tradeoffs in terms of finding suitable temporal baselines. In most cases, however, pairs selected for deformation mapping will have perpendicular baselines much smaller than the critical baseline. Ordering On Demand InSAR Products \u00b6 On Demand InSAR products are generated using ASF's HyP3 platform. Jobs can be submitted for processing using the Vertex data portal, the HyP3 Python SDK or the HyP3 API . Vertex \u00b6 InSAR pairs are selected in Vertex using either the Baseline Search or the SBAS Search interface. The Baseline tool is the best option for selecting a specific single InSAR pair. Use the Geographic Search to find an image that covers your time and area of interest, select that item in the results, and click the Baseline button in the center panel. The Baseline tool then displays all of the scenes that could be used to generate an interferogram using the selected image. Scroll through the results to find pairs to add to the On Demand queue, or click on items displayed in the plot to highlight that particular image pair. The SBAS tool is designed for generating time series of InSAR pairs. As with the Baseline search, you can launch the SBAS search from the center panel of a Geographic Search result. It will display all of the valid InSAR pairs through time based on the acquisition location of the input scene. This functionality is designed for processing a series of interferograms to be used in SBAS (Small BAseline Subset) analysis. The results can be adjusted based on baseline criteria (both perpendicular and temporal), and restricted to specific periods of time. Once the list is refined, you have the option to add all of the InSAR pairs displayed in the results to the On Demand queue. HyP3 SDK and API \u00b6 The HyP3 SDK provides support for processing nearest-neighbor interferograms for a selected granule. Specifying nearest-neighbor processing will find the next appropriate scene back in time to use as the reference granule for generating an interferogram with the selected granule. You can specify up to 2 nearest neighbors, which will pair the scene closest in time and next-closest in time to the selected granule for generating InSAR products, as demonstrated in this sample HyP3 SDK Jupyter Notebook . You may still find the Geographic, Baseline and SBAS searches in Vertex useful for finding reference scenes or picking specific pairs to use when submitting InSAR jobs via the SDK or API . Considerations for Selecting an InSAR Pair \u00b6 When selecting an InSAR pair, observe the following required conditions: Images from an identical orbit direction (either ascending or descending) Images with identical incidence angles and beam mode Images with identical resolution and wavelength (usually from the same sensor) Images with the same viewing geometry (same path and frame) Images with identical polarizations (both HH or VV) In addition, the following suggestions may be helpful: Use images from similar seasons/growth/weather conditions For deformation mapping: limited spatial separation of acquisition locations (small physical baseline) For topographic mapping: limited time separation between images (small temporal baseline) To analyze deformation caused by a single discrete event, such as an earthquake, select images that bracket the event as closely in time as possible. Keeping the window narrowly focused on the time of the event will reduce the impacts of other processes that may mask the signal of the event of interest. Processing Options \u00b6 New Water Masking Option Available! InSAR products can now be phase unwrapped using a water mask. The option to \"Apply water mask\" sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping. This reduces phase unwrapping errors and outputs a less noisy unwrapped interferogram. This option is currently available in the API and SDK , and is coming soon in Vertex! Change in Displacement Map Options There is now a single option for including displacement maps. Both line-of-sight and vertical displacement maps will only be added to the product package if the option to \"Include Displacement Maps\" is selected when submitting On-Demand InSAR jobs. Use caution when referencing the values included in the displacement maps, as the values are calculated relative to an arbitrary reference point. Refer to the Phase Unwrapping Reference Point section for more information. There are several options users can set when ordering InSAR On Demand products. Currently, users can choose the number of looks to take (which drives the resolution and pixel spacing of the products), and which optional products to include in the output package. The options are described below: The number of looks drives the resolution and pixel spacing of the output products. Selecting 10x2 looks will yield larger products with 80 m resolution and pixel spacing of 40 m. Selecting 20x4 looks reduces the resolution to 160 m and reduces the size of the products (roughly 1/4 the size of 10x2 look products), with a pixel spacing of 80 m. The default is 20x4 looks. The look vectors are stored in two files. The lv_theta (\u03b8) indicates the SAR look vector elevation angle at each pixel, ranging from -\u03c0/2 (down) to \u03c0/2 (up). The look vector elevation angle is defined as the angle between the horizontal surface and the look vector with positive angles indicating sensor positions above the surface. The lv_phi (\u03c6) map indicates the SAR look vector orientation angle at each pixel, ranging from 0 (east) to \u03c0/2 (north). The look vector orientation angle is defined as the angle between the East direction and the projection of the look vector on the horizontal surface plane. The orientation angle increases towards north, with the North direction corresponding to \u03c0/2 (and south to -\u03c0/2). Both angles are expressed in radians. The default is to not include these files in the output product bundle. The displacement maps convert the phase difference values from the unwrapped interferogram into measurements of ground displacement in meters. The line-of-sight displacement map indicates the amount of movement away from or towards the sensor. The vertical displacement calculates the vertical component of the line-of-sight displacement, using the assumption that all deformation is in the vertical direction. These files are excluded from the product package by default. The wrapped phase GeoTIFF can be included in the output package. The browse version of this GeoTIFF (_color_phase.png) is always included, but the GeoTIFF version is not included by default. The specific color ramp displayed in the png is most valuable for many users, but some may wish to work with the actual wrapped phase values. The incidence angle maps indicate the angle of the radar signal. The local incidence angle is defined as the angle between the incident radar signal and the local surface normal, expressed in radians, while the ellipsoid incidence angle indicates the angle between the incident radar beam and the direction perpendicular to the WGS84 ellipsoid model. These files are excluded from the product package by default. A copy of the DEM used for processing can optionally be included in the product package. The height values will differ from the original Copernicus DEM dataset, as a geoid correction has been applied, and it has been projected to UTM Zone coordinates. The source DEM is also downsampled to twice the pixel spacing of the output product to smooth it for use in processing, then resampled again to match the pixel spacing of the InSAR product. The DEM is excluded by default. There is an option to apply a water mask . This mask includes coastal waters and large inland waterbodies. There is a 3-km buffer applied to the shoreline mask so that most of the waterbody is masked without inadvertently masking near-shore features. Masking waterbodies can have a significant impact during the phase unwrapping, as water can sometimes exhibit enough coherence between acquisitions to allow for unwrapping to occur over waterbodies, which is invalid. A GeoTIFF of the water mask is always included with the InSAR product package, but when this option is selected, the conditional water mask will be applied along with coherence and intensity thresholds during the phase unwrapping process. Water masking is turned off by default. InSAR Workflow \u00b6 The InSAR workflow used in HyP3 was developed by ASF using GAMMA software. The steps include pre-processing steps, interferogram preparation, and product creation. Once these steps are performed, an output product package will be created. See product packaging for details on the individual files included in the package. Pre-Processing \u00b6 Pre-processing steps prepare the SAR images to be used in interferometry. The pre-processing steps include image selection, ingest (including calibration), creation of a suitable DEM, and calculation of the burst overlap. Select an InSAR Pair \u00b6 Although it is possible to start from RAW data, Sentinel-1 InSAR processing is typically done using Interferometric Wide swath Single Look Complex (IW SLC) data as the input. This means that the data has been formed into an image through SAR processing, but has not been multi-looked. The SLC pair is defined by the user , either through the Vertex interface, or using the HyP3 API or SDK. To ensure consistency, the older SLC image is always used as the reference image, and the younger SLC image is always used as the secondary image. This means that positive values in the resulting unwrapped interferogram represent movement away from the SAR platform and negative values represent movement towards the SAR platform. However, these values are relative to the reference point of the unwrapped interferogram. See the phase unwrapping section for more details. Ingest SLC data into GAMMA format \u00b6 Once the InSAR pair has been identified, the selected SLC data are ingested into GAMMA internal format. This is performed by the GAMMA program par_s1_slc . GAMMA format has raw data files (only data, no headers or line leaders) with metadata stored in external files with a .par extension. During ingest into GAMMA's internal format, the SLC data is calibrated by applying the calibration coefficients that are supplied with each product. This process puts the SAR backscatter into a known scale where the diffuse volume scattering of the Amazon rainforest is a constant -6.5 dB. Immediately after ingesting the SLC, the state vectors are updated to use the best available state vectors. The state vector types in order of absolute correctness are original predicted (O), restituted (R), and precision (P). In practice, one will never receive an InSAR product that uses the original predicted orbit - only granules for which a restituted or precision orbit is available can be used in HyP3 InSAR processing. The orbit type used for generating the InSAR product is indicated in the product filename, as shown in Figure 3. Figure 3: Position of the orbit type in the HyP3 product name. Prepare the DEM File \u00b6 In order to create differential InSAR products that show motion on the ground, one must subtract the topographic phase from the interferogram. The topographic phase, in this case, is replicated by using an existing DEM to calculate the actual topographic phase. This phase is then removed from the interferogram leaving just the motion or deformation signal (plus atmospheric delays and noise). The DEM that is used for HyP3 InSAR processing is the 2021 Release of the Copernicus GLO-30 Public DEM dataset publicly available on AWS . This DEM provides global coverage at 30-m pixel spacing, and provides higher-quality products over a wider area than the older DEMs (SRTM and NED) previously used to generate ASF's On Demand products. For details on the Copernicus GLO-30 DEM , refer to the Product Handbook . For more information about the 2021 updates, see the 'Releases' section of this article . The DEM tiles necessary to cover the input granules for the InSAR product are downloaded. A geoid correction is applied to the DEM, and it is resampled to match the output resolution of the InSAR product (160 m for 20x4 products, 80 m for 10x2 products) and projected to the appropriate UTM Zone for the granule location. Calculate Overlapping Bursts \u00b6 The IW SLC Sentinel-1 data comes in three sub-swaths. However, a further subdivision is made in the data, wherein bursts occur. Bursts are the fundamental building block for Sentinel-1 imagery. Each one is a portion of the final image, around 1500 lines long and one sub-swath width wide. Thus, the more busts, the longer the file is in length. Each burst is precisely timed to repeat at a given time interval. This consistent repeat combined with precise velocity control gives rise to the fact that the bursts start at the same time on each pass around the globe. For example, a burst images a piece of the Gal\u00e1pagos Islands. The next time that same piece of the island is imaged, the time of day will be the same, to within few milliseconds. Only the frames containing overlapping bursts can be used to perform InSAR processing. This means that if there is no burst overlap in the pair selected as input, the InSAR process will not run . Repeatable burst timing is exploited by HyP3 in order to calculate the bursts that overlap between two scenes. These overlapping bursts are the only ones used in the rest of the InSAR process. The rest are discarded. Interferogram Creation, Co-registration and Refinement \u00b6 Before the interferogram is created, the lookup table that maps from the SLC image space into a ground range image space is created. At this time, the interferogram of the topography is simulated using the previously prepared DEM. Once these steps have been performed, the two SLCs are co-registered to within 0.02 pixels. This is done by iteratively using the following steps: Resample the secondary SLC using previously calculated offset polynomial Match the reference and secondary SLC images using intensity cross-correlation Estimate range and azimuth offset polynomial coefficients from results of matching Create a differential interferogram using the co-registered SLCs and the simulated interferogram Update offset polynomial by adding the current estimates Note that these steps are automatically run 4 times. At that point, if the last offset calculated was more than 0.02 pixels, then the procedure will fail to complete . Provided the images passed the check for convergence, the next co-registration step employs the Enhanced Spectral Diversity (ESD) algorithm to match the two scenes to better than 1/100th of a pixel. This is accomplished by examining the overlap area between subsequent bursts. If there is even a small offset, the phase between the bursts will not match. This phase mismatch is then used to calculate the corresponding azimuth offset. To finish interferogram processing, steps 1 through 4 are run once again, this time with the offsets from the ESD included. The output of this entire process is a wrapped interferogram . Phase Unwrapping \u00b6 All of the phase differences in wrapped interferograms lie between -\u03c0 and \u03c0. Phase unwrapping attempts to assign multiples of 2\u03c0 to add to each pixel in the interferogram to restrict the number of 2\u03c0 jumps in the phase to the regions where they may actually occur. These regions are areas of radar layover or areas of deformation exceeding half a wavelength in the sensor's line of sight. Thermal noise and interferometric decorrelation can also result in these 2\u03c0 phase discontinuities called residues . The phase unwrapping algorithm used for these products is Minimum Cost Flow (MCF) and Triangulation. Refer to this Technical Report from GAMMA Remote Sensing for more information on the MCF phase unwrapping approach. Filtering \u00b6 Before the interferogram can be unwrapped, it must be filtered to remove noise. This is accomplished using an adaptive spectral filtering algorithm. This adaptive interferogram filtering aims to reduce phase noise, increase the accuracy of the interferometric phase, and reduce the number of interferogram residues as an aid to phase unwrapping. In this case, residues are points in the interferogram where the sum of the phase differences between pixels around a closed path is not 0.0, which indicates a jump in phase. Masking \u00b6 Another step before unwrapping is to create a validity mask to guide the phase unwrapping process. This mask is generated by applying thresholds to the coherence and/or amplitude (backscatter intensity) values for an image pair. For On Demand InSAR products, we set the amplitude threshold to be 0.0 (in power scale), so that data is only excluded based on the coherence threshold. Coherence is estimated from the normalized interferogram. The pixel values in this file range from 0.0 (total decorrelation) to 1.0 (perfectly coherent). Any input pixel with a coherence value less than 0.1 is given a validity mask value of zero and not used during unwrapping. Change to Validity Mask Thresholds In the past, we also used an amplitude threshold of 0.2 (in power scale) when generating the validity mask. While this approach tends to mask out inland waters, providing a less noisy interferogram in some cases, it also masks arid regions that have low amplitude values but reasonably high coherence. As of March 2022, we have set the amplitude threshold to 0.0, so that coherence is the only driver of the validity mask. When the water masking option is applied, the validity mask is further amended to apply 0 values to any pixels classified as water in the water mask. In some cases, pixels over water may still meet the coherence and amplitude threshold criteria for inclusion, even though they are not valid for use during phase unwrapping. When processing scenes with extensive coverage by coastal waters or large inland waterbodies, there can be erroneous phase jumps introduced if unwrapping proceeds over water as if it were land. In such cases, choosing the option to apply the water mask can improve the results. Reference point \u00b6 In order to perform phase unwrapping, a reference point must be selected. The unwrapping will proceed relative to this reference point; the 2\u03c0 integer multiples will be applied to the wrapped phase using this pixel as the starting point. The unwrapped phase value is set to 0 at the reference point. Ideally, the reference point for phase unwrapping would be located in an area with high coherence in a stable region close to an area with surface deformation. Choosing an optimal reference point requires knowledge of the site characteristics and examination of the interferogram, which is not practical in an automated, global workflow. By default, ASF's On Demand InSAR products use the location of the pixel with the highest coherence value as the reference point. The coherence map is examined to determine the maximum value, and all pixels with this value are examined using a 9-pixel window. The pixel with the highest sum of values within its 9-pixel window is selected as the reference point. If more than one pixel has the same 9-pixel sum, the pixel closest to the origin pixel (bottom left corner for ascending scenes, top right corner for descending scenes) is selected. This may be an appropriate reference point location in many cases, as it meets the criteria of having high coherence, and stable areas have higher coherence than areas undergoing significant deformation. If a user wants to set a different location as the phase unwrapping reference point, however, a correction can be applied to the unwrapped interferogram. For more information on the impact of the phase unwrapping reference point location on unwrapped phase and displacement measurements, refer to the Limitations section of this document, which also includes instructions for applying a correction based on a custom reference point. Geocoding and Product Creation \u00b6 After the phase is unwrapped, the final steps are geocoding and product creation. Geocoding \u00b6 Geocoding is the process of reprojecting pixels from SAR slant range space (where all the calculations have been performed) into map-projected ground range space (where analysis of products is simplest). Using the look up table previously computed, this process takes each pixel in the input product and relocates it to the UTM zone of the DEM used in processing. This is accomplished using nearest-neighbor resampling so that original pixel values are preserved. Product Creation \u00b6 Files are next exported from GAMMA internal format into the widely-used GeoTIFF format, complete with geolocation information. GeoTIFFs are created for amplitude, coherence, and unwrapped phase by default, and a water mask GeoTIFF is also included in the product package. Optionally, GeoTIFFs of wrapped phase, look vectors, displacement maps (line-of-sight and vertical), and incidence angle maps can be included, as can a copy of the DEM used for processing. Product Packaging \u00b6 HyP3 InSAR output is a zip file containing various files, including GeoTIFFs, PNG browse images with geolocation information, Google Earth KMZ files, a metadata file, and a README file. Naming Convention \u00b6 The InSAR product names are packed with information pertaining to the processing of the data, presented in the following order, as illustrated in Figure 4. The platform names, either Sentinel-1A or Sentinel-1B, are abbreviated \"A\" or \"B\", indicating the reference and secondary granule's imaging platform The reference start date and time and the secondary start date and time, with the date and time separated by the letter T The polarizations for the pair, either HH or VV, the orbit type, and the days of separation for the pair The product type (always INT for InSAR) and the pixel spacing, which will be either 80 or 40, based upon the number of looks selected when the job was submitted for processing The software package used for processing is always GAMMA for GAMMA InSAR products User-defined options are denoted by three characters indicating whether the product is water masked (w) or not (u), the scene is clipped (e for entire area, c for clipped), and whether a single sub-swath was processed or the entire granule (either 1, 2, 3, or F for full swath) Currently, only the water masking is available as a user-selected option; the products always include the full granule extent with all three sub-swaths The filename ends with the ASF product ID, a 4 digit hexadecimal number Figure 4: Breakdown of ASF InSAR naming scheme. Image Files \u00b6 All of the main InSAR product files are 32-bit floating-point single-band GeoTIFFs. To learn more about the rasters included in the product package, refer to the Exploring Sentinel-1 InSAR Story Map . The amplitude image is the calibrated radiometric backscatter from the reference granule in sigma-nought power. The image is terrain corrected using a geometric correction, but not radiometrically corrected. The coherence file pixel values range from 0.0 to 1.0, with 0.0 being completely non-coherent and 1.0 being perfectly coherent. The unwrapped phase file shows the results of the phase unwrapping process. Negative values indicate movement towards the sensor, and positive values indicate movement away from the sensor. This is the main interferogram output. The wrapped phase file indicates the interferogram phase after applying the adaptive filter immediately before unwrapping. Values range from negative pi to positive pi. (optional) The line-of-sight displacement file indicates the displacement in meters along the look direction of the sensor. The sign is opposite to that of the unwrapped phase: positive values indicate motion towards the sensor and negative values indicate motion away from the sensor. (optional) The vertical displacement is generated from the line of sight displacement values, and makes the assumption that deformation only occurs in the vertical direction. Note that this assumption may not hold true in cases where the deformation also has a horizontal component. Positive values indicate uplift, and negative values indicate subsidence. (optional) The look vectors theta (\u03b8) and phi (\u03c6) describe the elevation and orientation angles of the sensor's look direction. (optional) The incidence angle maps indicate the angle between the incident signal and the surface normal of either the terrain (local incidence angle) or the ellipsoid (ellipsoid incidence angle). (optional) The DEM file gives the local terrain heights in meters, with a geoid correction applied. (optional) The water mask file indicates coastal waters and large inland waterbodies beyond 3 km from the shoreline. Pixel values of 1 indicate land and 0 indicate water. This file is in 8-bit unsigned integer format. If the water mask option is selected, the water mask is applied prior to phase unwrapping to exclude water pixels from the process. The water mask is not precise; the coastal shorelines from the GSHHG are buffered out to 3 km from shore to reduce the possibility of near-shore features being excluded while reducing the number of water pixels that may impact the quality of the phase unwrapping process. Inland waters are not masked. Browse images are included for the wrapped (color_phase) and unwrapped (unw_phase) phase files, which are in PNG format and are each 2048 pixels wide. The browse images are displayed using a cyclic color ramp to generate fringes. Each fringe in a wrapped (color_phase) browse image represents a 2-pi phase difference, and the line-of-sight displacement for each fringe is equivalent to half the wavelength of the sensor. The wavelength of Sentinel-1 is about 5.6 cm, so each 2-pi fringe represents a line-of-sight displacement of about 2.8 cm. Each fringe in an unwrapped (unw_phase) browse image represents a phase difference of 6 pi. Because each 2-pi difference is equivalent to half the wavelength of the sensor, each 6-pi fringe represents about 8.3 cm of line-of-sight displacement for these Sentinel-1 products. KMZ files are included for the wrapped (color_phase) and unwrapped (unw_phase) phase images, which allow users to view the outputs in Google Earth or other platforms that support kmz files. The tags and extensions used and example file names for each raster are listed in Table 2 below. Extension Description Example _amp.tif Amplitude S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _amp.tif _corr.tif Normalized coherence file S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _corr.tif _unw_phase.tif Unwrapped geocoded interferogram S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _unw_phase.tif _wrapped_phase.tif Wrapped geocoded interferogram S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _wrapped_phase.tif _los_disp.tif Line-of-sight displacement S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _los_disp.tif _vert_disp.tif Vertical displacement S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _vert_disp.tif _lv_phi.tif Look vector \u03c6 (orientation) S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _lv_phi.tif _lv_theta.tif Look vector \u03b8 (elevation) S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _lv_theta.tif _dem.tif Digital elevation model S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _dem.tif _inc_map_ell.tif Ellipsoid incidence angle S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _inc_map_ell.tif _inc_map.tif Local incidence angle S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _inc_map.tif _water_mask.tif Water mask S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _water_mask.tif _color_phase.kmz Wrapped phase kmz file S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _color_phase.kmz _unw_phase.kmz Unwrapped phase kmz file S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _unw_phase.kmz _color_phase.png Wrapped phase browse image S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _color_phase.png _unw_phase.png Unwrapped phase browse image S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _unw_phase.png Table 2: Image files in product package Metadata Files \u00b6 The product package also includes a number of metadata files. Extension Description Example .README.md.txt Main README file for GAMMA InSAR S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 .README.md.txt .txt Parameters and metadata for the InSAR pair S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 .txt .tif.xml ArcGIS compliant XML metadata for GeoTIFF files S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _unw_phase.tif.xml .png.xml ArcGIS compliant XML metadata for PNG files S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _color_phase.png.xml .png.aux.xml Geolocation information for png browse images S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _color_phase.png.aux.xml Table 3: Metadata files in product package README File \u00b6 The text file with extension .README.md.txt explains the files included in the folder, and is customized to reflect that particular product. Users unfamiliar with InSAR products should start by reading this README file, which will give some background on each of the files included in the product folder. InSAR Parameter File \u00b6 The text file with extension .txt includes processing parameters used to generate the InSAR product as well as metadata attributes for the InSAR pair. These are detailed in Table 4. Name Description Possible Value Reference Granule ESA granule name for reference scene (of the two scenes in the pair, the dataset with the oldest timestamp) S1A _IW _SLC __1SDV _20200116T032559 _20200116T032627 _030820 _038928 _F5DC Secondary Granule ESA granule name for secondary scene (of the two scenes in the pair, the dataset with the newest timestamp) S1B _IW _SLC __1SDV _20200128T032559 _20200128T032627 _030995 _038F51 _7D4F Reference Pass Direction Orbit direction of the reference scene DESCENDING Reference Orbit Number Absolute orbit number of the reference scene 30741 Secondary Pass Direction Orbit direction of the reference scene DESCENDING Secondary Orbit Number Absolute orbit number of the secondary scene 31091 Baseline Perpendicular baseline in meters 58.3898 UTCTime Time in the UTC time zone in seconds 12360.691361 Heading Spacecraft heading measured in degrees clockwise from north 193.2939317 Spacecraft height Height in meters of the spacecraft above nadir point 700618.6318999995 Earth radius at nadir Ellipsoidal earth radius in meters at the point directly below the satellite 6370250.0667 Slant range near Distance in meters from satellite to nearest point imaged 799517.4338 Slant range center Distance in meters from satellite to the center point imaged 879794.1404 Slant range far Distance in meters from satellite to farthest point imaged 960070.8469 Range looks Number of looks taken in the range direction 20 Azimuth looks Number of looks taken in the azimuth direction 4 InSAR phase filter Name of the phase filter used adf Phase filter parameter Dampening factor 0.6 Resolution of output (m) Pixel spacing in meters for output products 80 Range bandpass filter Range bandpass filter applied no Azimuth bandpass filter Azimuth bandpass filter applied no DEM source DEM used in processing GLO-30 DEM resolution Pixel spacing in meters for DEM used to process this scene 160 Unwrapping type Phase unwrapping algorithm used mcf Phase at Reference Point Original unwrapped phase value at the reference point (set to 0 in output unwrapped phase raster) -4.21967 Azimuth line of the reference point in SAR space Row number (in SAR space) of the reference point 2737.0 Range pixel of the reference point in SAR space Column number (in SAR space) of the reference point 739.0 Y coordinate of the reference point in the map projection Latitude of the reference point in projected coordinates (UTM Zone - meters) 4112453.3223 X coordinate of the reference point in the map projection Longitude of the reference point in projected coordinates (UTM Zone - meters) 589307.6248 Latitude of the reference point (WGS84) Latitude of the reference point in WGS84 Geographic Coordinate System (degrees) 37.1542125 Longitude of the reference point (WGS84) Longitude of the reference point in WGS84 Geographic Coordinate System (degrees) 40.00574707 Unwrapping threshold Minimum coherence required to unwrap a given pixel none Speckle filter Speckle filter applied no Table 4: List of InSAR parameters included in the parameter text file ArcGIS-Compatible XML Files \u00b6 There is an ArcGIS-compatible xml file for each raster in the product folder. When ArcGIS Desktop users view any of the rasters in ArcCatalog or the Catalog window in ArcMap, they can open the Item Description to view the contents of the associated xml file. ArcGIS Pro users can access the information from the Metadata tab. These files will not appear as separate items in ArcCatalog, though if you use Windows Explorer to look at the contents of the folder you will see them listed individually. Because each one is named identically to the product it describes (with the addition of the .xml extension), ArcGIS recognizes the appropriate file as the raster\u2019s associated metadata, and integrates the metadata accordingly. ArcGIS users should take care not to change these xml files outside of the ArcGIS environment; changing the filename or content directly may render the files unreadable by ArcGIS. Those not using ArcGIS will still find the contents of these xml files useful, but will have to contend with the xml tagging when viewing the files as text or in a browser. Auxiliary Geolocation Files \u00b6 Geolocation XML files (aux files) are included for each of the PNG browse images to allow for proper display in GIS platforms. Limitations \u00b6 Baseline Calculation \u00b6 The baseline is defined as the difference of the platform positions when a given area is imaged. HyP3 baselines are calculated using the best state vectors available. If precise orbits are not yet available for the input granules, restituted orbits will be used. The original predicted orbits are not used for InSAR processing in HyP3. If no restituted or precise state vectors are available, the process will not run. Coherence \u00b6 The phase measurements in the two images used in InSAR must be coherent in order to detect change. Random changes in phase from one acquisition to the next can mask actual surface deformation. Vegetation is a common driver of decorrelation, as changes can easily take place in the interval between two acquisitions due to growth, seasonal changes, or wind effects. It will be difficult to generate valid interferograms with C-band data in heavily vegetated regions due to lack of coherence even with fairly short time intervals. Consider seasonality when selecting image pairs. Decorrelation can be particularly high when comparing phase from different seasons. Changes in the condition of vegetation (especially deciduous canopies), snow, moisture, or freeze/thaw state can impact phase measurements. In cases where a temporal baseline is required that spans seasons, it may be better to use an annual interferogram if possible so that the images are more comparable in terms of seasonality. Line-of-Sight Measurements \u00b6 When looking at a single interferogram, the deformation measurements in the line-of-sight orientation of the sensor indicate relative motion towards or away from the sensor. InSAR is not sensitive to motion in the azimuth direction of the satellite, so motion that occurs in the same direction as the satellite's direction of travel will not be detected. A single interferogram cannot be used to determine the relative contributions of vertical and horizontal movement to the line-of-sight displacement measurement. The vertical displacement map is generated based on the assumption that the movement is entirely in the vertical direction, which may not be realistic for some processes. To determine how much of the signal is driven by vertical vs. horizontal movement, you must either use a time series of interferograms, or use reference measurements with known vertical and horizontal components (such as GNSS measurements from the region of deformation) to deconstruct the line-of-sight displacement. All displacement values are calculated relative to a reference point , which may or may not be an appropriate benchmark for measuring particular areas of displacement within the interferogram. Phase Unwrapping Reference Point \u00b6 The reference point for phase unwrapping is set to be the location of the pixel with the highest coherence value. As described in the phase unwrapping section , this may not always be an ideal location to use as a reference point. If it is located in an area undergoing deformation, or in a patch of coherent pixels that is separated from the area undergoing deformation by a gap of incoherent pixels, the unwrapping may be of lower quality than if the reference point was in a more suitable location. Even when there are not phase unwrapping errors introduced by phase discontinuities, it is important to be aware that unwrapped phase differences and displacement values are all calculated relative to the reference point. The phase difference value of the reference point is set to 0 during phase unwrapping, so any displacement values will be relative to that benchmark. If the location of the default reference point is in the middle of an area that underwent deformation, displacement values may be different than expected. If you are interested in the amount of displacement in a particular area, you may wish to choose your own reference point. The ideal reference point would be in an area of high coherence beyond where deformation has occurred. The unwrapped phase measurements can be adjusted to be relative to this new reference point, and displacement values can be recalculated accordingly. To adjust the values in the unwrapped phase GeoTIFF, simply select a reference point that is optimal for your use case and subtract the unwrapped phase value of that reference point from each pixel in the unwrapped phase raster: \u0394\u03a8 * = \u0394\u03a8 - \u0394\u03c8 ref where \u0394\u03a8 * is the adjusted unwrapped phase, \u0394\u03a8 is the original unwrapped phase, and \u0394\u03c8 ref is the unwrapped phase value at the new reference point. Impacts on Displacement Measurements \u00b6 The measurements in the displacement maps are calculated from the unwrapped phase values, so will similarly be impacted by the location of the reference point. You may wish to recalculate the displacement values relative to a new reference point. The approach for correcting the displacement maps will be different for the line-of-sight and vertical measurements. Correcting Line-of-Sight Displacement Maps \u00b6 If you have already corrected the unwrapped phase raster, you can calculate a new line-of-sight (LOS) displacement map by applying the following calculation on a pixel-by-pixel basis using the unwrapped phase GeoTIFF: \u0394\u03a9 * = - \u0394\u03a8 * \u03bb / 4\u03c0 where \u0394\u03a9 * is the adjusted line-of-sight displacement in meters, \u0394\u03a8 * is the adjusted unwrapped phase , and \u03bb is the wavelength of the sensor in meters (0.055465763 for Sentinel-1). Setting the \u0394\u03a8 * value to be negative reverses the sign so that the difference is relative to the earth rather than the sensor. A positive phase difference value indicates subsidence, which is unintuitive when thinking about movement on the earth's surface. Applying the negative will return positive displacement values for uplift and negative values for subsidence. If you are not interested in adjusted unwrapped phase values, you can also directly correct the LOS Displacement map included optionally in the InSAR product package: \u0394\u03a9 * = \u0394\u03a9 - \u0394\u03c9 ref where \u0394\u03a9 * is the adjusted line-of-sight displacement in meters, \u0394\u03a9 is the original line-of-sight displacement in meters, and \u0394\u03c9 ref is the line-of-sight displacement value at the new reference point. Correcting Vertical Displacement Maps \u00b6 Vertical displacement maps cannot be adjusted directly, and must be recalculated from the adjusted unwrapped phase image. You will also need the \u03b8 look vector map (lv_theta GeoTIFF) for this calculation. The look vector maps are not included in the InSAR product package by default; the option to Include Look Vectors must be selected when ordering the product. To calculate an adjusted vertical displacement raster, calculate the adjusted unwrapped phase , then apply the following: \u0394\u03d2 * = - \u0394\u03a8 * \u03bb cos(\u00bd\u03c0 - LV \u03b8 ) / 4\u03c0 where \u0394\u03d2 * is the adjusted vertical displacement in meters, \u0394\u03a8 * is the adjusted unwrapped phase, \u03bb is the wavelength of the sensor in meters (0.055465763 for Sentinel-1), and LV \u03b8 is the theta look vector (from the lv_theta GeoTIFF). As with the LOS Displacement maps, setting the \u0394\u03a8 * value to be negative reverses the sign so that the difference is relative to the earth rather than the sensor. Applying the negative will return positive displacement values for uplift and negative values for subsidence. Displacement Values from a Single Interferogram \u00b6 In general, calculating displacement values from a single interferogram is not recommended. While the displacement rasters provided with ASF's On Demand InSAR products can be helpful in visualizing changes, we do not recommend that you rely on a single interferogram when coming to conclusions about surface displacement, even if you apply a correction based on a manually selected reference point. It will be more robust to use a time series approach to more accurately determine the pattern of movement. When using SAR time-series software such as MintPy , you have the option to select a specific reference point, and the values of the input rasters will be adjusted accordingly. Error Sources \u00b6 On Demand InSAR products do not currently correct for some common sources of error in interferometry, such as atmospheric effects. Further processing or time series analysis can be performed by the user to identify or reduce the impact of some of these errors when using On Demand InSAR products for analysis. Atmospheric Delay \u00b6 While SAR signals can penetrate clouds, atmospheric conditions can delay the transmission of the signal. This results in phase differences that can look like surface deformation signals but are actually driven by differences in the atmospheric conditions between the pair of acquisitions used to generate the interferogram. In some cases, atmospheric errors can be corrected by using an atmospheric model to remove the impacts of the turbulent delay from the interferogram. Another approach is to use time series analysis to identify outliers. Always doubt your interferogram first! View the interferogram critically, and consider if fringe patterns could potentially be driven by atmospheric effects. In general, it is best to avoid drawing conclusions from the outcome of a single interferogram. Turbulent Delay \u00b6 These delays are generally caused by differences in water vapor distribution from one image to the next. They often manifest as wobbly or sausage-shaped fringes, and can potentially mask the signal of a small earthquake. Stratified Delay \u00b6 This type of delay is driven mostly by pressure and temperature differences or gradients through the atmospheric column, and often correlates with topography. This atmospheric signature can be confused with movement caused by volcanic activity. If there are multiple volcanoes in an image and they all exhibit similar patterns, it is likely being driven by this type of atmospheric delay. DEM Errors \u00b6 A DEM is used to remove topographic phase impacts, but if there are inaccuracies in the DEM, residual impacts of those errors can remain in the interferogram. Orbit Uncertainties \u00b6 This is generally not an issue for Sentinel-1 data, as the orbits are very precise and generally reliable. On Demand InSAR products are only processed once restituted or precise orbits are available. Orbit uncertainties are more problematic when working with datasets from older missions.","title":"Product Guide"},{"location":"guides/insar_product_guide/#sentinel-1-insar-product-guide","text":"This document is a guide for users of Interferometric Synthetic Aperture Radar (InSAR) Sentinel-1 products generated by the Alaska Satellite Facility (ASF). Users can request InSAR products On Demand in ASF's Vertex data portal, or make use of our HyP3 Python SDK or API . Input pair selection in Vertex uses either the Baseline Tool or the SBAS Tool search interfaces. For a step-by-step tutorial on ordering On-Demand InSAR Products using Vertex, visit our InSAR On Demand story map . To learn more about the files included in the On Demand InSAR product packages and how to work with them, refer to our Exploring Sentinel-1 InSAR story map Users are cautioned to read the sections on limitations and error sources in InSAR products before attempting to use InSAR data. For a more complete description of the properties of SAR, see our Introduction to SAR guide.","title":"Sentinel-1 InSAR Product Guide"},{"location":"guides/insar_product_guide/#introduction","text":"Interferometric Synthetic Aperture Radar (InSAR) processing uses two SAR images collected over the same area to determine geometric properties of the surface. Missions such as Sentinel-1 are designed for monitoring surface deformation using InSAR, which is optimal when acquisitions are made from a consistent location in space ( short perpendicular baseline ) over regular time intervals. The phase measurements of two SAR images acquired at different times from the same place in orbit are differenced to detect and quantify surface changes, such as deformation caused by earthquakes, volcanoes, or groundwater subsidence. InSAR can also be used to generate digital elevation models, but the optimal mission for DEM generation has the opposite characteristics of the Sentinel-1 mission. Topography is best mapped when the two acquisitions are obtained as close together as possible in time ( short temporal baseline ), but from different vantage points in space (larger perpendicular baseline than would be optimal for deformation mapping).","title":"Introduction"},{"location":"guides/insar_product_guide/#brief-overview-of-insar","text":"SAR is an active sensor that transmits pulses and listens for echoes. These echoes are recorded in phase and amplitude, with the phase being used to determine the distance from the sensor to the target and the amplitude yielding information about the roughness and dielectric constant of that target. Figure 1: Two passes of an imaging SAR taken at time T 0 and T 0 + \u2206t, will give two distances to the ground, R 1 and R 2 . A difference between R 1 and R 2 shows motion on the ground. In this case, a subsidence makes R 2 greater than R 1 . Credit: TRE ALTAMIRA InSAR exploits the phase difference between two SAR images to create an interferogram that shows where the phase and, therefore, the distance to the target has changed from one pass to the next, as illustrated in Figure 1. There are several factors that influence the interferogram, including earth curvature, topographic effects, atmospheric delays, surface motion, and noise. With proper processing, Sentinel-1 InSAR can be used to detect changes in the earth's surface down to the centimeter scale. Applications include volcanic deformation, subsidence, landslide detection, and earthquake assessment.","title":"Brief Overview of InSAR"},{"location":"guides/insar_product_guide/#wavelengths","text":"The SAR sensors on the Sentinel-1 satellites transmit C-band signals, with a wavelength of 5.6 cm. The signal wavelength impacts the penetration capability of the signal, so it is important to be aware of the sensor wavelength when working with SAR datasets. C-band SAR will penetrate more deeply into canopy or surfaces than an X-band signal, but not nearly as deep as an L-band SAR signal, which, with a wavelength on the order of 25 cm, is better able to penetrate canopy and return signals from the forest floor. Different wavelengths are also sensitive to different levels of deformation. To detect very small changes over relatively short periods of time, you may require a signal with a smaller wavelength (such as X-band). However, signals with shorter wavelengths are also more prone to decorrelation due to small changes in surface conditions such as vegetation growth. For slower processes that require a longer time interval to detect movement, longer wavelengths (such as L-band) may be necessary. C-band sits in the middle. It can detect fairly small changes over fairly short periods of time, but is not as sensitive to small changes as X-band or as able to monitor surface dynamics under canopy as L-band.","title":"Wavelengths"},{"location":"guides/insar_product_guide/#polarizations","text":"Polarization refers to the direction of travel of an electromagnetic wave. A horizontal wave is transmitted so that it oscillates in a plane parallel to the surface imaged, while a vertical wave oscillates in a plane perpendicular to the surface imaged. Most modern SAR systems can transmit chirps with either a horizontal or vertical polarization. In addition, some of these sensors can listen for either horizontal or vertical backscatter. This gives rise to 4 different types of returns: HH, HV, VV, and VH, with the first letter indicating the transmission method and the second the receive method. For example, VH is a vertically polarized transmit signal with horizontally polarized echoes recorded. For InSAR applications, processing is generally performed on the co-pol (VV or HH) data and not on the cross-pol (VH or HV) data. Also, each image used in an InSAR pair is required to be the same polarization - two HH images of the same area could form a valid pair, while a single HH with a single VV of the same area would not.","title":"Polarizations"},{"location":"guides/insar_product_guide/#baselines","text":"","title":"Baselines"},{"location":"guides/insar_product_guide/#perpendicular-baseline","text":"The term baseline refers to the physical distance between the two vantage points from which images used as an InSAR pair are acquired. The baseline is decomposed into perpendicular (also called normal) and parallel components, as shown in Figure 2. To monitor surface deformation, the perpendicular baseline for the two acquisitions should be very small in order to maximize the coherence of the phase measurements. In order to determine topography, two slightly different vantage points are required. Sensitivity to topography depends on the perpendicular baseline, the sensor wavelength, the distance between the satellite and the ground, and the sensor look angle. Figure 2: Geometry of InSAR baselines. Two satellite passes image the same area on the ground from positions S 1 and S 2 , resulting in a baseline of B, which can be decomposed into perpendicular (B \u27c2 ) and parallel (B \u2225 ) components. Here Y is the direction of travel, referred to as the along-track or azimuth direction, and X is the direction perpendicular to motion, referred to as the cross-track or range direction. Credit: ASF","title":"Perpendicular Baseline"},{"location":"guides/insar_product_guide/#temporal-baseline","text":"In contrast to the (physical) baseline, the temporal baseline refers to the time separation between imaging passes. Along-track interferometry measures motion in the millisecond to second range. This technique can detect ocean currents and rapidly moving objects like boats. Differential interferometry is the standard method used to detect motion in the range of days to years. This is the type of interferometry that is performed by the Sentinel-1 HyP3 InSAR processing algorithm. Table 1 lists different temporal baselines, their common names, and what they can be used to measure. Duration Known as Measurement of ms to sec along-track ocean currents, moving object detection, MTI days differential glacier/ice fields/lava flows, surface water extent, hydrology days to years differential subsidence, seismic events, volcanic activity, crustal displacement Table 1: Temporal baselines and what they measure. Different geophysical phenomena can be detected based upon the temporal baseline. In general, the longer the temporal baseline, the smaller the motion that can be detected.","title":"Temporal Baseline"},{"location":"guides/insar_product_guide/#critical-baseline","text":"Large baselines are better than small for topographic mapping. However, as the baseline increases, coherence decreases. At some point, it is impossible to create an interferogram because of baseline decorrelation. The maximum viable baseline per platform, referred to as the critical baseline , is a function of the distance to the ground, the wavelength, and the viewing geometry of the platform. For Sentinel-1, this critical baseline is about 5 km. In practice, if the perpendicular baseline between images is more than 3/4 of the critical baseline, interferogram creation will be problematic due to the level of noise. For deformation mapping, it is best to minimize the perpendicular baseline whenever possible, but there may be tradeoffs in terms of finding suitable temporal baselines. In most cases, however, pairs selected for deformation mapping will have perpendicular baselines much smaller than the critical baseline.","title":"Critical Baseline"},{"location":"guides/insar_product_guide/#ordering-on-demand-insar-products","text":"On Demand InSAR products are generated using ASF's HyP3 platform. Jobs can be submitted for processing using the Vertex data portal, the HyP3 Python SDK or the HyP3 API .","title":"Ordering On Demand InSAR Products"},{"location":"guides/insar_product_guide/#vertex","text":"InSAR pairs are selected in Vertex using either the Baseline Search or the SBAS Search interface. The Baseline tool is the best option for selecting a specific single InSAR pair. Use the Geographic Search to find an image that covers your time and area of interest, select that item in the results, and click the Baseline button in the center panel. The Baseline tool then displays all of the scenes that could be used to generate an interferogram using the selected image. Scroll through the results to find pairs to add to the On Demand queue, or click on items displayed in the plot to highlight that particular image pair. The SBAS tool is designed for generating time series of InSAR pairs. As with the Baseline search, you can launch the SBAS search from the center panel of a Geographic Search result. It will display all of the valid InSAR pairs through time based on the acquisition location of the input scene. This functionality is designed for processing a series of interferograms to be used in SBAS (Small BAseline Subset) analysis. The results can be adjusted based on baseline criteria (both perpendicular and temporal), and restricted to specific periods of time. Once the list is refined, you have the option to add all of the InSAR pairs displayed in the results to the On Demand queue.","title":"Vertex"},{"location":"guides/insar_product_guide/#hyp3-sdk-and-api","text":"The HyP3 SDK provides support for processing nearest-neighbor interferograms for a selected granule. Specifying nearest-neighbor processing will find the next appropriate scene back in time to use as the reference granule for generating an interferogram with the selected granule. You can specify up to 2 nearest neighbors, which will pair the scene closest in time and next-closest in time to the selected granule for generating InSAR products, as demonstrated in this sample HyP3 SDK Jupyter Notebook . You may still find the Geographic, Baseline and SBAS searches in Vertex useful for finding reference scenes or picking specific pairs to use when submitting InSAR jobs via the SDK or API .","title":"HyP3 SDK and API"},{"location":"guides/insar_product_guide/#considerations-for-selecting-an-insar-pair","text":"When selecting an InSAR pair, observe the following required conditions: Images from an identical orbit direction (either ascending or descending) Images with identical incidence angles and beam mode Images with identical resolution and wavelength (usually from the same sensor) Images with the same viewing geometry (same path and frame) Images with identical polarizations (both HH or VV) In addition, the following suggestions may be helpful: Use images from similar seasons/growth/weather conditions For deformation mapping: limited spatial separation of acquisition locations (small physical baseline) For topographic mapping: limited time separation between images (small temporal baseline) To analyze deformation caused by a single discrete event, such as an earthquake, select images that bracket the event as closely in time as possible. Keeping the window narrowly focused on the time of the event will reduce the impacts of other processes that may mask the signal of the event of interest.","title":"Considerations for Selecting an InSAR Pair"},{"location":"guides/insar_product_guide/#processing-options","text":"New Water Masking Option Available! InSAR products can now be phase unwrapped using a water mask. The option to \"Apply water mask\" sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping. This reduces phase unwrapping errors and outputs a less noisy unwrapped interferogram. This option is currently available in the API and SDK , and is coming soon in Vertex! Change in Displacement Map Options There is now a single option for including displacement maps. Both line-of-sight and vertical displacement maps will only be added to the product package if the option to \"Include Displacement Maps\" is selected when submitting On-Demand InSAR jobs. Use caution when referencing the values included in the displacement maps, as the values are calculated relative to an arbitrary reference point. Refer to the Phase Unwrapping Reference Point section for more information. There are several options users can set when ordering InSAR On Demand products. Currently, users can choose the number of looks to take (which drives the resolution and pixel spacing of the products), and which optional products to include in the output package. The options are described below: The number of looks drives the resolution and pixel spacing of the output products. Selecting 10x2 looks will yield larger products with 80 m resolution and pixel spacing of 40 m. Selecting 20x4 looks reduces the resolution to 160 m and reduces the size of the products (roughly 1/4 the size of 10x2 look products), with a pixel spacing of 80 m. The default is 20x4 looks. The look vectors are stored in two files. The lv_theta (\u03b8) indicates the SAR look vector elevation angle at each pixel, ranging from -\u03c0/2 (down) to \u03c0/2 (up). The look vector elevation angle is defined as the angle between the horizontal surface and the look vector with positive angles indicating sensor positions above the surface. The lv_phi (\u03c6) map indicates the SAR look vector orientation angle at each pixel, ranging from 0 (east) to \u03c0/2 (north). The look vector orientation angle is defined as the angle between the East direction and the projection of the look vector on the horizontal surface plane. The orientation angle increases towards north, with the North direction corresponding to \u03c0/2 (and south to -\u03c0/2). Both angles are expressed in radians. The default is to not include these files in the output product bundle. The displacement maps convert the phase difference values from the unwrapped interferogram into measurements of ground displacement in meters. The line-of-sight displacement map indicates the amount of movement away from or towards the sensor. The vertical displacement calculates the vertical component of the line-of-sight displacement, using the assumption that all deformation is in the vertical direction. These files are excluded from the product package by default. The wrapped phase GeoTIFF can be included in the output package. The browse version of this GeoTIFF (_color_phase.png) is always included, but the GeoTIFF version is not included by default. The specific color ramp displayed in the png is most valuable for many users, but some may wish to work with the actual wrapped phase values. The incidence angle maps indicate the angle of the radar signal. The local incidence angle is defined as the angle between the incident radar signal and the local surface normal, expressed in radians, while the ellipsoid incidence angle indicates the angle between the incident radar beam and the direction perpendicular to the WGS84 ellipsoid model. These files are excluded from the product package by default. A copy of the DEM used for processing can optionally be included in the product package. The height values will differ from the original Copernicus DEM dataset, as a geoid correction has been applied, and it has been projected to UTM Zone coordinates. The source DEM is also downsampled to twice the pixel spacing of the output product to smooth it for use in processing, then resampled again to match the pixel spacing of the InSAR product. The DEM is excluded by default. There is an option to apply a water mask . This mask includes coastal waters and large inland waterbodies. There is a 3-km buffer applied to the shoreline mask so that most of the waterbody is masked without inadvertently masking near-shore features. Masking waterbodies can have a significant impact during the phase unwrapping, as water can sometimes exhibit enough coherence between acquisitions to allow for unwrapping to occur over waterbodies, which is invalid. A GeoTIFF of the water mask is always included with the InSAR product package, but when this option is selected, the conditional water mask will be applied along with coherence and intensity thresholds during the phase unwrapping process. Water masking is turned off by default.","title":"Processing Options"},{"location":"guides/insar_product_guide/#insar-workflow","text":"The InSAR workflow used in HyP3 was developed by ASF using GAMMA software. The steps include pre-processing steps, interferogram preparation, and product creation. Once these steps are performed, an output product package will be created. See product packaging for details on the individual files included in the package.","title":"InSAR Workflow"},{"location":"guides/insar_product_guide/#pre-processing","text":"Pre-processing steps prepare the SAR images to be used in interferometry. The pre-processing steps include image selection, ingest (including calibration), creation of a suitable DEM, and calculation of the burst overlap.","title":"Pre-Processing"},{"location":"guides/insar_product_guide/#select-an-insar-pair","text":"Although it is possible to start from RAW data, Sentinel-1 InSAR processing is typically done using Interferometric Wide swath Single Look Complex (IW SLC) data as the input. This means that the data has been formed into an image through SAR processing, but has not been multi-looked. The SLC pair is defined by the user , either through the Vertex interface, or using the HyP3 API or SDK. To ensure consistency, the older SLC image is always used as the reference image, and the younger SLC image is always used as the secondary image. This means that positive values in the resulting unwrapped interferogram represent movement away from the SAR platform and negative values represent movement towards the SAR platform. However, these values are relative to the reference point of the unwrapped interferogram. See the phase unwrapping section for more details.","title":"Select an InSAR Pair"},{"location":"guides/insar_product_guide/#ingest-slc-data-into-gamma-format","text":"Once the InSAR pair has been identified, the selected SLC data are ingested into GAMMA internal format. This is performed by the GAMMA program par_s1_slc . GAMMA format has raw data files (only data, no headers or line leaders) with metadata stored in external files with a .par extension. During ingest into GAMMA's internal format, the SLC data is calibrated by applying the calibration coefficients that are supplied with each product. This process puts the SAR backscatter into a known scale where the diffuse volume scattering of the Amazon rainforest is a constant -6.5 dB. Immediately after ingesting the SLC, the state vectors are updated to use the best available state vectors. The state vector types in order of absolute correctness are original predicted (O), restituted (R), and precision (P). In practice, one will never receive an InSAR product that uses the original predicted orbit - only granules for which a restituted or precision orbit is available can be used in HyP3 InSAR processing. The orbit type used for generating the InSAR product is indicated in the product filename, as shown in Figure 3. Figure 3: Position of the orbit type in the HyP3 product name.","title":"Ingest SLC data into GAMMA format"},{"location":"guides/insar_product_guide/#prepare-the-dem-file","text":"In order to create differential InSAR products that show motion on the ground, one must subtract the topographic phase from the interferogram. The topographic phase, in this case, is replicated by using an existing DEM to calculate the actual topographic phase. This phase is then removed from the interferogram leaving just the motion or deformation signal (plus atmospheric delays and noise). The DEM that is used for HyP3 InSAR processing is the 2021 Release of the Copernicus GLO-30 Public DEM dataset publicly available on AWS . This DEM provides global coverage at 30-m pixel spacing, and provides higher-quality products over a wider area than the older DEMs (SRTM and NED) previously used to generate ASF's On Demand products. For details on the Copernicus GLO-30 DEM , refer to the Product Handbook . For more information about the 2021 updates, see the 'Releases' section of this article . The DEM tiles necessary to cover the input granules for the InSAR product are downloaded. A geoid correction is applied to the DEM, and it is resampled to match the output resolution of the InSAR product (160 m for 20x4 products, 80 m for 10x2 products) and projected to the appropriate UTM Zone for the granule location.","title":"Prepare the DEM File"},{"location":"guides/insar_product_guide/#calculate-overlapping-bursts","text":"The IW SLC Sentinel-1 data comes in three sub-swaths. However, a further subdivision is made in the data, wherein bursts occur. Bursts are the fundamental building block for Sentinel-1 imagery. Each one is a portion of the final image, around 1500 lines long and one sub-swath width wide. Thus, the more busts, the longer the file is in length. Each burst is precisely timed to repeat at a given time interval. This consistent repeat combined with precise velocity control gives rise to the fact that the bursts start at the same time on each pass around the globe. For example, a burst images a piece of the Gal\u00e1pagos Islands. The next time that same piece of the island is imaged, the time of day will be the same, to within few milliseconds. Only the frames containing overlapping bursts can be used to perform InSAR processing. This means that if there is no burst overlap in the pair selected as input, the InSAR process will not run . Repeatable burst timing is exploited by HyP3 in order to calculate the bursts that overlap between two scenes. These overlapping bursts are the only ones used in the rest of the InSAR process. The rest are discarded.","title":"Calculate Overlapping Bursts"},{"location":"guides/insar_product_guide/#interferogram-creation-co-registration-and-refinement","text":"Before the interferogram is created, the lookup table that maps from the SLC image space into a ground range image space is created. At this time, the interferogram of the topography is simulated using the previously prepared DEM. Once these steps have been performed, the two SLCs are co-registered to within 0.02 pixels. This is done by iteratively using the following steps: Resample the secondary SLC using previously calculated offset polynomial Match the reference and secondary SLC images using intensity cross-correlation Estimate range and azimuth offset polynomial coefficients from results of matching Create a differential interferogram using the co-registered SLCs and the simulated interferogram Update offset polynomial by adding the current estimates Note that these steps are automatically run 4 times. At that point, if the last offset calculated was more than 0.02 pixels, then the procedure will fail to complete . Provided the images passed the check for convergence, the next co-registration step employs the Enhanced Spectral Diversity (ESD) algorithm to match the two scenes to better than 1/100th of a pixel. This is accomplished by examining the overlap area between subsequent bursts. If there is even a small offset, the phase between the bursts will not match. This phase mismatch is then used to calculate the corresponding azimuth offset. To finish interferogram processing, steps 1 through 4 are run once again, this time with the offsets from the ESD included. The output of this entire process is a wrapped interferogram .","title":"Interferogram Creation, Co-registration and Refinement"},{"location":"guides/insar_product_guide/#phase-unwrapping","text":"All of the phase differences in wrapped interferograms lie between -\u03c0 and \u03c0. Phase unwrapping attempts to assign multiples of 2\u03c0 to add to each pixel in the interferogram to restrict the number of 2\u03c0 jumps in the phase to the regions where they may actually occur. These regions are areas of radar layover or areas of deformation exceeding half a wavelength in the sensor's line of sight. Thermal noise and interferometric decorrelation can also result in these 2\u03c0 phase discontinuities called residues . The phase unwrapping algorithm used for these products is Minimum Cost Flow (MCF) and Triangulation. Refer to this Technical Report from GAMMA Remote Sensing for more information on the MCF phase unwrapping approach.","title":"Phase Unwrapping"},{"location":"guides/insar_product_guide/#filtering","text":"Before the interferogram can be unwrapped, it must be filtered to remove noise. This is accomplished using an adaptive spectral filtering algorithm. This adaptive interferogram filtering aims to reduce phase noise, increase the accuracy of the interferometric phase, and reduce the number of interferogram residues as an aid to phase unwrapping. In this case, residues are points in the interferogram where the sum of the phase differences between pixels around a closed path is not 0.0, which indicates a jump in phase.","title":"Filtering"},{"location":"guides/insar_product_guide/#masking","text":"Another step before unwrapping is to create a validity mask to guide the phase unwrapping process. This mask is generated by applying thresholds to the coherence and/or amplitude (backscatter intensity) values for an image pair. For On Demand InSAR products, we set the amplitude threshold to be 0.0 (in power scale), so that data is only excluded based on the coherence threshold. Coherence is estimated from the normalized interferogram. The pixel values in this file range from 0.0 (total decorrelation) to 1.0 (perfectly coherent). Any input pixel with a coherence value less than 0.1 is given a validity mask value of zero and not used during unwrapping. Change to Validity Mask Thresholds In the past, we also used an amplitude threshold of 0.2 (in power scale) when generating the validity mask. While this approach tends to mask out inland waters, providing a less noisy interferogram in some cases, it also masks arid regions that have low amplitude values but reasonably high coherence. As of March 2022, we have set the amplitude threshold to 0.0, so that coherence is the only driver of the validity mask. When the water masking option is applied, the validity mask is further amended to apply 0 values to any pixels classified as water in the water mask. In some cases, pixels over water may still meet the coherence and amplitude threshold criteria for inclusion, even though they are not valid for use during phase unwrapping. When processing scenes with extensive coverage by coastal waters or large inland waterbodies, there can be erroneous phase jumps introduced if unwrapping proceeds over water as if it were land. In such cases, choosing the option to apply the water mask can improve the results.","title":"Masking"},{"location":"guides/insar_product_guide/#reference-point","text":"In order to perform phase unwrapping, a reference point must be selected. The unwrapping will proceed relative to this reference point; the 2\u03c0 integer multiples will be applied to the wrapped phase using this pixel as the starting point. The unwrapped phase value is set to 0 at the reference point. Ideally, the reference point for phase unwrapping would be located in an area with high coherence in a stable region close to an area with surface deformation. Choosing an optimal reference point requires knowledge of the site characteristics and examination of the interferogram, which is not practical in an automated, global workflow. By default, ASF's On Demand InSAR products use the location of the pixel with the highest coherence value as the reference point. The coherence map is examined to determine the maximum value, and all pixels with this value are examined using a 9-pixel window. The pixel with the highest sum of values within its 9-pixel window is selected as the reference point. If more than one pixel has the same 9-pixel sum, the pixel closest to the origin pixel (bottom left corner for ascending scenes, top right corner for descending scenes) is selected. This may be an appropriate reference point location in many cases, as it meets the criteria of having high coherence, and stable areas have higher coherence than areas undergoing significant deformation. If a user wants to set a different location as the phase unwrapping reference point, however, a correction can be applied to the unwrapped interferogram. For more information on the impact of the phase unwrapping reference point location on unwrapped phase and displacement measurements, refer to the Limitations section of this document, which also includes instructions for applying a correction based on a custom reference point.","title":"Reference point"},{"location":"guides/insar_product_guide/#geocoding-and-product-creation","text":"After the phase is unwrapped, the final steps are geocoding and product creation.","title":"Geocoding and Product Creation"},{"location":"guides/insar_product_guide/#geocoding","text":"Geocoding is the process of reprojecting pixels from SAR slant range space (where all the calculations have been performed) into map-projected ground range space (where analysis of products is simplest). Using the look up table previously computed, this process takes each pixel in the input product and relocates it to the UTM zone of the DEM used in processing. This is accomplished using nearest-neighbor resampling so that original pixel values are preserved.","title":"Geocoding"},{"location":"guides/insar_product_guide/#product-creation","text":"Files are next exported from GAMMA internal format into the widely-used GeoTIFF format, complete with geolocation information. GeoTIFFs are created for amplitude, coherence, and unwrapped phase by default, and a water mask GeoTIFF is also included in the product package. Optionally, GeoTIFFs of wrapped phase, look vectors, displacement maps (line-of-sight and vertical), and incidence angle maps can be included, as can a copy of the DEM used for processing.","title":"Product Creation"},{"location":"guides/insar_product_guide/#product-packaging","text":"HyP3 InSAR output is a zip file containing various files, including GeoTIFFs, PNG browse images with geolocation information, Google Earth KMZ files, a metadata file, and a README file.","title":"Product Packaging"},{"location":"guides/insar_product_guide/#naming-convention","text":"The InSAR product names are packed with information pertaining to the processing of the data, presented in the following order, as illustrated in Figure 4. The platform names, either Sentinel-1A or Sentinel-1B, are abbreviated \"A\" or \"B\", indicating the reference and secondary granule's imaging platform The reference start date and time and the secondary start date and time, with the date and time separated by the letter T The polarizations for the pair, either HH or VV, the orbit type, and the days of separation for the pair The product type (always INT for InSAR) and the pixel spacing, which will be either 80 or 40, based upon the number of looks selected when the job was submitted for processing The software package used for processing is always GAMMA for GAMMA InSAR products User-defined options are denoted by three characters indicating whether the product is water masked (w) or not (u), the scene is clipped (e for entire area, c for clipped), and whether a single sub-swath was processed or the entire granule (either 1, 2, 3, or F for full swath) Currently, only the water masking is available as a user-selected option; the products always include the full granule extent with all three sub-swaths The filename ends with the ASF product ID, a 4 digit hexadecimal number Figure 4: Breakdown of ASF InSAR naming scheme.","title":"Naming Convention"},{"location":"guides/insar_product_guide/#image-files","text":"All of the main InSAR product files are 32-bit floating-point single-band GeoTIFFs. To learn more about the rasters included in the product package, refer to the Exploring Sentinel-1 InSAR Story Map . The amplitude image is the calibrated radiometric backscatter from the reference granule in sigma-nought power. The image is terrain corrected using a geometric correction, but not radiometrically corrected. The coherence file pixel values range from 0.0 to 1.0, with 0.0 being completely non-coherent and 1.0 being perfectly coherent. The unwrapped phase file shows the results of the phase unwrapping process. Negative values indicate movement towards the sensor, and positive values indicate movement away from the sensor. This is the main interferogram output. The wrapped phase file indicates the interferogram phase after applying the adaptive filter immediately before unwrapping. Values range from negative pi to positive pi. (optional) The line-of-sight displacement file indicates the displacement in meters along the look direction of the sensor. The sign is opposite to that of the unwrapped phase: positive values indicate motion towards the sensor and negative values indicate motion away from the sensor. (optional) The vertical displacement is generated from the line of sight displacement values, and makes the assumption that deformation only occurs in the vertical direction. Note that this assumption may not hold true in cases where the deformation also has a horizontal component. Positive values indicate uplift, and negative values indicate subsidence. (optional) The look vectors theta (\u03b8) and phi (\u03c6) describe the elevation and orientation angles of the sensor's look direction. (optional) The incidence angle maps indicate the angle between the incident signal and the surface normal of either the terrain (local incidence angle) or the ellipsoid (ellipsoid incidence angle). (optional) The DEM file gives the local terrain heights in meters, with a geoid correction applied. (optional) The water mask file indicates coastal waters and large inland waterbodies beyond 3 km from the shoreline. Pixel values of 1 indicate land and 0 indicate water. This file is in 8-bit unsigned integer format. If the water mask option is selected, the water mask is applied prior to phase unwrapping to exclude water pixels from the process. The water mask is not precise; the coastal shorelines from the GSHHG are buffered out to 3 km from shore to reduce the possibility of near-shore features being excluded while reducing the number of water pixels that may impact the quality of the phase unwrapping process. Inland waters are not masked. Browse images are included for the wrapped (color_phase) and unwrapped (unw_phase) phase files, which are in PNG format and are each 2048 pixels wide. The browse images are displayed using a cyclic color ramp to generate fringes. Each fringe in a wrapped (color_phase) browse image represents a 2-pi phase difference, and the line-of-sight displacement for each fringe is equivalent to half the wavelength of the sensor. The wavelength of Sentinel-1 is about 5.6 cm, so each 2-pi fringe represents a line-of-sight displacement of about 2.8 cm. Each fringe in an unwrapped (unw_phase) browse image represents a phase difference of 6 pi. Because each 2-pi difference is equivalent to half the wavelength of the sensor, each 6-pi fringe represents about 8.3 cm of line-of-sight displacement for these Sentinel-1 products. KMZ files are included for the wrapped (color_phase) and unwrapped (unw_phase) phase images, which allow users to view the outputs in Google Earth or other platforms that support kmz files. The tags and extensions used and example file names for each raster are listed in Table 2 below. Extension Description Example _amp.tif Amplitude S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _amp.tif _corr.tif Normalized coherence file S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _corr.tif _unw_phase.tif Unwrapped geocoded interferogram S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _unw_phase.tif _wrapped_phase.tif Wrapped geocoded interferogram S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _wrapped_phase.tif _los_disp.tif Line-of-sight displacement S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _los_disp.tif _vert_disp.tif Vertical displacement S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _vert_disp.tif _lv_phi.tif Look vector \u03c6 (orientation) S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _lv_phi.tif _lv_theta.tif Look vector \u03b8 (elevation) S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _lv_theta.tif _dem.tif Digital elevation model S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _dem.tif _inc_map_ell.tif Ellipsoid incidence angle S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _inc_map_ell.tif _inc_map.tif Local incidence angle S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _inc_map.tif _water_mask.tif Water mask S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _water_mask.tif _color_phase.kmz Wrapped phase kmz file S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _color_phase.kmz _unw_phase.kmz Unwrapped phase kmz file S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _unw_phase.kmz _color_phase.png Wrapped phase browse image S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _color_phase.png _unw_phase.png Unwrapped phase browse image S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _unw_phase.png Table 2: Image files in product package","title":"Image Files"},{"location":"guides/insar_product_guide/#metadata-files","text":"The product package also includes a number of metadata files. Extension Description Example .README.md.txt Main README file for GAMMA InSAR S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 .README.md.txt .txt Parameters and metadata for the InSAR pair S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 .txt .tif.xml ArcGIS compliant XML metadata for GeoTIFF files S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _unw_phase.tif.xml .png.xml ArcGIS compliant XML metadata for PNG files S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _color_phase.png.xml .png.aux.xml Geolocation information for png browse images S1AB _20171111T150004 _20171117T145926 _VVP006 _INT80 _G _ueF _4D09 _color_phase.png.aux.xml Table 3: Metadata files in product package","title":"Metadata Files"},{"location":"guides/insar_product_guide/#readme-file","text":"The text file with extension .README.md.txt explains the files included in the folder, and is customized to reflect that particular product. Users unfamiliar with InSAR products should start by reading this README file, which will give some background on each of the files included in the product folder.","title":"README File"},{"location":"guides/insar_product_guide/#insar-parameter-file","text":"The text file with extension .txt includes processing parameters used to generate the InSAR product as well as metadata attributes for the InSAR pair. These are detailed in Table 4. Name Description Possible Value Reference Granule ESA granule name for reference scene (of the two scenes in the pair, the dataset with the oldest timestamp) S1A _IW _SLC __1SDV _20200116T032559 _20200116T032627 _030820 _038928 _F5DC Secondary Granule ESA granule name for secondary scene (of the two scenes in the pair, the dataset with the newest timestamp) S1B _IW _SLC __1SDV _20200128T032559 _20200128T032627 _030995 _038F51 _7D4F Reference Pass Direction Orbit direction of the reference scene DESCENDING Reference Orbit Number Absolute orbit number of the reference scene 30741 Secondary Pass Direction Orbit direction of the reference scene DESCENDING Secondary Orbit Number Absolute orbit number of the secondary scene 31091 Baseline Perpendicular baseline in meters 58.3898 UTCTime Time in the UTC time zone in seconds 12360.691361 Heading Spacecraft heading measured in degrees clockwise from north 193.2939317 Spacecraft height Height in meters of the spacecraft above nadir point 700618.6318999995 Earth radius at nadir Ellipsoidal earth radius in meters at the point directly below the satellite 6370250.0667 Slant range near Distance in meters from satellite to nearest point imaged 799517.4338 Slant range center Distance in meters from satellite to the center point imaged 879794.1404 Slant range far Distance in meters from satellite to farthest point imaged 960070.8469 Range looks Number of looks taken in the range direction 20 Azimuth looks Number of looks taken in the azimuth direction 4 InSAR phase filter Name of the phase filter used adf Phase filter parameter Dampening factor 0.6 Resolution of output (m) Pixel spacing in meters for output products 80 Range bandpass filter Range bandpass filter applied no Azimuth bandpass filter Azimuth bandpass filter applied no DEM source DEM used in processing GLO-30 DEM resolution Pixel spacing in meters for DEM used to process this scene 160 Unwrapping type Phase unwrapping algorithm used mcf Phase at Reference Point Original unwrapped phase value at the reference point (set to 0 in output unwrapped phase raster) -4.21967 Azimuth line of the reference point in SAR space Row number (in SAR space) of the reference point 2737.0 Range pixel of the reference point in SAR space Column number (in SAR space) of the reference point 739.0 Y coordinate of the reference point in the map projection Latitude of the reference point in projected coordinates (UTM Zone - meters) 4112453.3223 X coordinate of the reference point in the map projection Longitude of the reference point in projected coordinates (UTM Zone - meters) 589307.6248 Latitude of the reference point (WGS84) Latitude of the reference point in WGS84 Geographic Coordinate System (degrees) 37.1542125 Longitude of the reference point (WGS84) Longitude of the reference point in WGS84 Geographic Coordinate System (degrees) 40.00574707 Unwrapping threshold Minimum coherence required to unwrap a given pixel none Speckle filter Speckle filter applied no Table 4: List of InSAR parameters included in the parameter text file","title":"InSAR Parameter File"},{"location":"guides/insar_product_guide/#arcgis-compatible-xml-files","text":"There is an ArcGIS-compatible xml file for each raster in the product folder. When ArcGIS Desktop users view any of the rasters in ArcCatalog or the Catalog window in ArcMap, they can open the Item Description to view the contents of the associated xml file. ArcGIS Pro users can access the information from the Metadata tab. These files will not appear as separate items in ArcCatalog, though if you use Windows Explorer to look at the contents of the folder you will see them listed individually. Because each one is named identically to the product it describes (with the addition of the .xml extension), ArcGIS recognizes the appropriate file as the raster\u2019s associated metadata, and integrates the metadata accordingly. ArcGIS users should take care not to change these xml files outside of the ArcGIS environment; changing the filename or content directly may render the files unreadable by ArcGIS. Those not using ArcGIS will still find the contents of these xml files useful, but will have to contend with the xml tagging when viewing the files as text or in a browser.","title":"ArcGIS-Compatible XML Files"},{"location":"guides/insar_product_guide/#auxiliary-geolocation-files","text":"Geolocation XML files (aux files) are included for each of the PNG browse images to allow for proper display in GIS platforms.","title":"Auxiliary Geolocation Files"},{"location":"guides/insar_product_guide/#limitations","text":"","title":"Limitations"},{"location":"guides/insar_product_guide/#baseline-calculation","text":"The baseline is defined as the difference of the platform positions when a given area is imaged. HyP3 baselines are calculated using the best state vectors available. If precise orbits are not yet available for the input granules, restituted orbits will be used. The original predicted orbits are not used for InSAR processing in HyP3. If no restituted or precise state vectors are available, the process will not run.","title":"Baseline Calculation"},{"location":"guides/insar_product_guide/#coherence","text":"The phase measurements in the two images used in InSAR must be coherent in order to detect change. Random changes in phase from one acquisition to the next can mask actual surface deformation. Vegetation is a common driver of decorrelation, as changes can easily take place in the interval between two acquisitions due to growth, seasonal changes, or wind effects. It will be difficult to generate valid interferograms with C-band data in heavily vegetated regions due to lack of coherence even with fairly short time intervals. Consider seasonality when selecting image pairs. Decorrelation can be particularly high when comparing phase from different seasons. Changes in the condition of vegetation (especially deciduous canopies), snow, moisture, or freeze/thaw state can impact phase measurements. In cases where a temporal baseline is required that spans seasons, it may be better to use an annual interferogram if possible so that the images are more comparable in terms of seasonality.","title":"Coherence"},{"location":"guides/insar_product_guide/#line-of-sight-measurements","text":"When looking at a single interferogram, the deformation measurements in the line-of-sight orientation of the sensor indicate relative motion towards or away from the sensor. InSAR is not sensitive to motion in the azimuth direction of the satellite, so motion that occurs in the same direction as the satellite's direction of travel will not be detected. A single interferogram cannot be used to determine the relative contributions of vertical and horizontal movement to the line-of-sight displacement measurement. The vertical displacement map is generated based on the assumption that the movement is entirely in the vertical direction, which may not be realistic for some processes. To determine how much of the signal is driven by vertical vs. horizontal movement, you must either use a time series of interferograms, or use reference measurements with known vertical and horizontal components (such as GNSS measurements from the region of deformation) to deconstruct the line-of-sight displacement. All displacement values are calculated relative to a reference point , which may or may not be an appropriate benchmark for measuring particular areas of displacement within the interferogram.","title":"Line-of-Sight Measurements"},{"location":"guides/insar_product_guide/#phase-unwrapping-reference-point","text":"The reference point for phase unwrapping is set to be the location of the pixel with the highest coherence value. As described in the phase unwrapping section , this may not always be an ideal location to use as a reference point. If it is located in an area undergoing deformation, or in a patch of coherent pixels that is separated from the area undergoing deformation by a gap of incoherent pixels, the unwrapping may be of lower quality than if the reference point was in a more suitable location. Even when there are not phase unwrapping errors introduced by phase discontinuities, it is important to be aware that unwrapped phase differences and displacement values are all calculated relative to the reference point. The phase difference value of the reference point is set to 0 during phase unwrapping, so any displacement values will be relative to that benchmark. If the location of the default reference point is in the middle of an area that underwent deformation, displacement values may be different than expected. If you are interested in the amount of displacement in a particular area, you may wish to choose your own reference point. The ideal reference point would be in an area of high coherence beyond where deformation has occurred. The unwrapped phase measurements can be adjusted to be relative to this new reference point, and displacement values can be recalculated accordingly. To adjust the values in the unwrapped phase GeoTIFF, simply select a reference point that is optimal for your use case and subtract the unwrapped phase value of that reference point from each pixel in the unwrapped phase raster: \u0394\u03a8 * = \u0394\u03a8 - \u0394\u03c8 ref where \u0394\u03a8 * is the adjusted unwrapped phase, \u0394\u03a8 is the original unwrapped phase, and \u0394\u03c8 ref is the unwrapped phase value at the new reference point.","title":"Phase Unwrapping Reference Point"},{"location":"guides/insar_product_guide/#impacts-on-displacement-measurements","text":"The measurements in the displacement maps are calculated from the unwrapped phase values, so will similarly be impacted by the location of the reference point. You may wish to recalculate the displacement values relative to a new reference point. The approach for correcting the displacement maps will be different for the line-of-sight and vertical measurements.","title":"Impacts on Displacement Measurements"},{"location":"guides/insar_product_guide/#displacement-values-from-a-single-interferogram","text":"In general, calculating displacement values from a single interferogram is not recommended. While the displacement rasters provided with ASF's On Demand InSAR products can be helpful in visualizing changes, we do not recommend that you rely on a single interferogram when coming to conclusions about surface displacement, even if you apply a correction based on a manually selected reference point. It will be more robust to use a time series approach to more accurately determine the pattern of movement. When using SAR time-series software such as MintPy , you have the option to select a specific reference point, and the values of the input rasters will be adjusted accordingly.","title":"Displacement Values from a Single Interferogram"},{"location":"guides/insar_product_guide/#error-sources","text":"On Demand InSAR products do not currently correct for some common sources of error in interferometry, such as atmospheric effects. Further processing or time series analysis can be performed by the user to identify or reduce the impact of some of these errors when using On Demand InSAR products for analysis.","title":"Error Sources"},{"location":"guides/insar_product_guide/#atmospheric-delay","text":"While SAR signals can penetrate clouds, atmospheric conditions can delay the transmission of the signal. This results in phase differences that can look like surface deformation signals but are actually driven by differences in the atmospheric conditions between the pair of acquisitions used to generate the interferogram. In some cases, atmospheric errors can be corrected by using an atmospheric model to remove the impacts of the turbulent delay from the interferogram. Another approach is to use time series analysis to identify outliers. Always doubt your interferogram first! View the interferogram critically, and consider if fringe patterns could potentially be driven by atmospheric effects. In general, it is best to avoid drawing conclusions from the outcome of a single interferogram.","title":"Atmospheric Delay"},{"location":"guides/insar_product_guide/#turbulent-delay","text":"These delays are generally caused by differences in water vapor distribution from one image to the next. They often manifest as wobbly or sausage-shaped fringes, and can potentially mask the signal of a small earthquake.","title":"Turbulent Delay"},{"location":"guides/insar_product_guide/#stratified-delay","text":"This type of delay is driven mostly by pressure and temperature differences or gradients through the atmospheric column, and often correlates with topography. This atmospheric signature can be confused with movement caused by volcanic activity. If there are multiple volcanoes in an image and they all exhibit similar patterns, it is likely being driven by this type of atmospheric delay.","title":"Stratified Delay"},{"location":"guides/insar_product_guide/#dem-errors","text":"A DEM is used to remove topographic phase impacts, but if there are inaccuracies in the DEM, residual impacts of those errors can remain in the interferogram.","title":"DEM Errors"},{"location":"guides/insar_product_guide/#orbit-uncertainties","text":"This is generally not an issue for Sentinel-1 data, as the orbits are very precise and generally reliable. On Demand InSAR products are only processed once restituted or precise orbits are available. Orbit uncertainties are more problematic when working with datasets from older missions.","title":"Orbit Uncertainties"},{"location":"guides/introduction_to_sar/","text":"Introduction to SAR \u00b6 How SAR Operates \u00b6 SAR is an active sensor that transmits pulses and listens for echoes, called backscatter. The backscatter is recorded in both phase and amplitude. The phase is used to determine the distance from the sensor to a target, and amplitude indicates the amount of the sent signal that returns to the sensor. Amplitude measurements provide information about the roughness, geometry, wetness, and dielectric constant of that target, while phase measurements are used for SAR interferometry. Propagation of EM Waves \u00b6 At the most fundamental level, SAR transmits an encoded burst, called a chirp, of electro-magnetic energy (Figure 1) and then listens for the return signal, called echoes. The wavelength of this chirp is in the centimeter range, with X-band (~3 cm), C-band (~6 cm), and L-band (~23 cm) all in common use. Figure 1: The spectrum of electromagnetic radiation. SAR is imaged using microwave wavelengths. The microwave range extends from about 1 mm to 1 m in wavelength, with most radar applications using bands within the 3 mm to 30 cm range. Polarizations \u00b6 Polarization refers to the direction of travel of an electromagnetic wave. A horizontal wave is transmitted so that it oscillates in a plane parallel to the surface imaged, while a vertical wave oscillates in a plane perpendicular to the surface imaged. There are four different polarization combinations commonly used by SAR sensors: VV, VH, HV and HH, as listed in Table 1. The first letter indicates the polarization used to transmit the signal, and the second letter indicates the polarization of the measured return, as illustrated in Figure 2. Table 1: SAR Polarizations Polarization Code Transmit Signal Polarization Return Signal Polarization VV Vertical Vertical VH Vertical Horizontal HV Horizontal Vertical HH Horizontal Horizontal Figure 2: SAR signals are transmitted and received either vertically (V) or horizontally (H). This gives the potential for four different polarization combinations (transmit listed first, receive second): VV, VH, HH, and HV. Credit: ASF Different SAR sensors have different polarization capabilities. Single-pol sensors send out a signal in one polarization and can only measure returns that are in that same polarization (VV or HH). Dual-pol sensors send out a signal in one polarization, but can measure returns that are in that same polarization (co-pol: VV or HH) as well as returns that are in the other polarization (cross-pol: VH or HV). Some SAR systems can transmit chirps with both a horizontal or vertical polarization and listen for both horizontal or vertical returns, giving full quad-pol capabilities (VV, VH, HV, HH). Polarimetry is an emerging field of SAR processing which is used in a number of applications such as measuring vegetation properties and changes of vegetation over time. Additional applications include oceanography, geology, and disaster response. Backscatter Contributors \u00b6 Many factors influence the backscatter received by the SAR sensor. The wavelength used by the SAR influences the signal's penetration, and, thus, what is being imaged. Surface roughness will modulate the backscatter returns from nothing up to a strong return, decreasing or increasing the brightness of the resulting pixel. Scattering mechanisms like volume scattering or double bounce can strongly influence the brightness of the SAR image as well, sometimes resulting in total saturation by the received signal. Wavelength \u00b6 The wavelength of the SAR system influences the amount of ground penetration that occurs. As shown in Figure 3, X-band has the least penetration, scattering from the top of the canopy in vegetated areas. All three bands will penetrate dry sand, with stronger returns from both C-band and L-band. L-band has the most penetration overall, with returns from the ground in vegetated areas, strong returns from substances under dry alluvium, and deep penetration of ice and snow. Figure 3: Effects of the SAR band on penetration of surfaces. The longer the wavelength, the deeper the penetration through most land types. Credit: The SAR Handbook Surface Roughness \u00b6 The strength of the return, or backscatter, is partially based upon relative roughness of the surface imaged. The smoother the surface, the more reflection away from the sensor, while rough surfaces give a much stronger return towards the imaging platform. As can be seen in Figure 4, if the height of the surface's roughness is less than 1/32 of the wavelength, mostly specular reflection occurs. If the height of the surface's roughness is greater than 1/2 the wavelength used, the echoes are scattered in all directions, giving a strong return back to the sensor. Figure 4: The amount of backscatter from a surface depends largely on the surface's roughness, with smooth surfaces getting the least returns and rough surfaces getting the strongest returns. Credit: The SAR Handbook Types of Scattering \u00b6 Figure 5: Scattering mechanisms. Rough surfaces give bright returns due to the wide scattering. Vegetated surfaces cause volumetric scattering, which gives a darker return to the imaging platform. Double bounce returns, found mostly in urban areas, give the brightest return, as the majority of the energy is re-directed back towards the sensor. Credit: The SAR Handbook The resolution of Sentinel-1 SAR images is roughly 10 m. This means that a square of 10 meters on the ground is represented by a single pixel in the SAR image. The relative roughness of this patch of ground compared to the wavelength used will affect the backscatter strength (see Figure 4). However, there are additional types of bounce mechanisms beyond specular and diffuse, as shown in Figure 5. In vegetation, volumetric scattering occurs when signals bounce around inside the vegetation imaged. The double bounce mechanism which occurs in urban areas and is exploited by corner reflectors, causes chirp to be reflected directly back to the sensor, causing a very strong backscatter. Double bounce returns are so strong in some places that they cause over saturation of the sensor, resulting in visible sidelobes. These sidelobes are evidenced by bright crosses surrounding the double bounce target. SAR Scale \u00b6 SAR backscatter are recorded in both return strength and phase. Each pixel in a single-look complex SAR image represents these values as an imaginary number (I,Q). To create the visible images we are used to looking at, the SAR image is detected . This process calculates the square root of the sum of the squares of the I and Q values found in an SLC image, creating a so called intensity image. This image is real valued, and, when calibrated, gives the absolute backscatter of the surface imaged. Detected images can be stored using several different scales, including power, amplitude, and dB. Note the default scale of Sentinel-1 RTC products from HyP3 is power. However, in some cases, it may be desirable to convert the actual pixel values to a different scale. Two other scales commonly used for SAR data are amplitude and dB. Power Scale \u00b6 The values in this scale are generally very close to zero, so the dynamic range of the SAR image can be easily skewed by a few bright scatterers in the image. Power scale is appropriate for statistical analysis of the SAR dataset, but may not always be the best option for data visualization. When viewing a SAR image in power scale in a GIS environment, it may appear mostly or all black, and you may need to adjust the stretch to see features in the image. Often applying a stretch of 2 standard deviations, or setting the Min-Max stretch values to 0 and 0.3, will greatly improve the appearance of the image. You can adjust the stretch as desired to display your image to full advantage. Be aware that this does not change the actual pixel values. Amplitude Scale \u00b6 Amplitude scale is the square root of the power scale values. This brightens the darker pixels and darkens the brighter pixels, narrowing the dynamic range of the image. In many cases, amplitude scale presents a pleasing grayscale display of RTC images. Amplitude scale works well for calculating log difference ratios (see ASF Sentinel-1 RTC Product Guide ). dB Scale \u00b6 The dB scale is calculated by multiplying 10 times the Log10 of the power scale values. This scale brightens the pixels, allowing for better differentiation among very dark pixels. When identifying water on the landscape, this is often a good scale to use; the water pixels generally remain very dark, while the terrestrial pixels are even brighter (see Identifying Surface Water ). This scale is not always the best choice for general visualization of SAR products, as it can give a washed-out appearance, and because it is in a log scale, it is not appropriate for all types of statistical analyses. Geometric Distortions \u00b6 There are a number of distortions inherent to SAR data due to the side-looking nature of the sensor, and these impacts will be more prevalent in areas with rugged terrain. The process of radiometric terrain correction addresses the geometric distortions that lead to geolocation errors in terrain features, and also normalizes the backscatter values based on the actual area contributing returns. This process generates an image that aligns well with other geospatial data and is suitable for GIS applications or time-series analysis. The key distortions present in SAR images are foreshortening, layover and shadow (Figure 6). Figure 6: Distortions induced by side-looking SAR. Ground points a, b, c are \u2018seen\u2019 by radar as points a\u2019, b\u2019, c\u2019 in the slant range. Credit: Franz J. Meyer In the case of foreshortening , the backscatter from the front side of the mountain is compressed, with returns from a large area arriving back to the sensor at about the same time. This results in the front slope being displayed as a narrow, bright band. When layover occurs, returns from the front slope (and potentially even some of the area before the slope starts) are received at the same time as returns from the back slope. Thus, area in the front of the slope is projected onto the back side in the slant range image. In this case, the data from the front slope cannot be extracted from the returns. Another condition that results in missing data is radar shadow . In this case, the angle of the back slope is such that the sensor can not image it at all. These areas with steep back slopes offer no information to the SAR sensor. When RTC is performed, foreshortened areas are corrected based on the DEM. Areas impacted by layover or shadow, however, do not actually have data returns to correct. In this case, the pixels in the resulting RTC image will have a value of No Data. We do not interpolate missing data; users who would like to fill holes with estimated values will need to do so as appropriate for their particular application. Speckle \u00b6 In most cases, the patch of ground illuminated by the SAR transmitter will not be homogeneous. Instead it will be comprised of many different types of individual scatterers. The scatterers may interfere with each other either strengthening the return or weakening it. This creates a grainy (salt & pepper) appearance in SAR imagery. This a result of the nature of SAR and, thus, occurs in all SAR scenes. Speckle in SAR images can be mitigated by multi-looking, which, in effect, uses averaging to smooth out the image, resulting in a more homogeneous appearance at the expense of resolution.","title":"Introduction to SAR"},{"location":"guides/introduction_to_sar/#introduction-to-sar","text":"","title":"Introduction to SAR"},{"location":"guides/introduction_to_sar/#how-sar-operates","text":"SAR is an active sensor that transmits pulses and listens for echoes, called backscatter. The backscatter is recorded in both phase and amplitude. The phase is used to determine the distance from the sensor to a target, and amplitude indicates the amount of the sent signal that returns to the sensor. Amplitude measurements provide information about the roughness, geometry, wetness, and dielectric constant of that target, while phase measurements are used for SAR interferometry.","title":"How SAR Operates"},{"location":"guides/introduction_to_sar/#propagation-of-em-waves","text":"At the most fundamental level, SAR transmits an encoded burst, called a chirp, of electro-magnetic energy (Figure 1) and then listens for the return signal, called echoes. The wavelength of this chirp is in the centimeter range, with X-band (~3 cm), C-band (~6 cm), and L-band (~23 cm) all in common use. Figure 1: The spectrum of electromagnetic radiation. SAR is imaged using microwave wavelengths. The microwave range extends from about 1 mm to 1 m in wavelength, with most radar applications using bands within the 3 mm to 30 cm range.","title":"Propagation of EM Waves"},{"location":"guides/introduction_to_sar/#polarizations","text":"Polarization refers to the direction of travel of an electromagnetic wave. A horizontal wave is transmitted so that it oscillates in a plane parallel to the surface imaged, while a vertical wave oscillates in a plane perpendicular to the surface imaged. There are four different polarization combinations commonly used by SAR sensors: VV, VH, HV and HH, as listed in Table 1. The first letter indicates the polarization used to transmit the signal, and the second letter indicates the polarization of the measured return, as illustrated in Figure 2. Table 1: SAR Polarizations Polarization Code Transmit Signal Polarization Return Signal Polarization VV Vertical Vertical VH Vertical Horizontal HV Horizontal Vertical HH Horizontal Horizontal Figure 2: SAR signals are transmitted and received either vertically (V) or horizontally (H). This gives the potential for four different polarization combinations (transmit listed first, receive second): VV, VH, HH, and HV. Credit: ASF Different SAR sensors have different polarization capabilities. Single-pol sensors send out a signal in one polarization and can only measure returns that are in that same polarization (VV or HH). Dual-pol sensors send out a signal in one polarization, but can measure returns that are in that same polarization (co-pol: VV or HH) as well as returns that are in the other polarization (cross-pol: VH or HV). Some SAR systems can transmit chirps with both a horizontal or vertical polarization and listen for both horizontal or vertical returns, giving full quad-pol capabilities (VV, VH, HV, HH). Polarimetry is an emerging field of SAR processing which is used in a number of applications such as measuring vegetation properties and changes of vegetation over time. Additional applications include oceanography, geology, and disaster response.","title":"Polarizations"},{"location":"guides/introduction_to_sar/#backscatter-contributors","text":"Many factors influence the backscatter received by the SAR sensor. The wavelength used by the SAR influences the signal's penetration, and, thus, what is being imaged. Surface roughness will modulate the backscatter returns from nothing up to a strong return, decreasing or increasing the brightness of the resulting pixel. Scattering mechanisms like volume scattering or double bounce can strongly influence the brightness of the SAR image as well, sometimes resulting in total saturation by the received signal.","title":"Backscatter Contributors"},{"location":"guides/introduction_to_sar/#wavelength","text":"The wavelength of the SAR system influences the amount of ground penetration that occurs. As shown in Figure 3, X-band has the least penetration, scattering from the top of the canopy in vegetated areas. All three bands will penetrate dry sand, with stronger returns from both C-band and L-band. L-band has the most penetration overall, with returns from the ground in vegetated areas, strong returns from substances under dry alluvium, and deep penetration of ice and snow. Figure 3: Effects of the SAR band on penetration of surfaces. The longer the wavelength, the deeper the penetration through most land types. Credit: The SAR Handbook","title":"Wavelength"},{"location":"guides/introduction_to_sar/#surface-roughness","text":"The strength of the return, or backscatter, is partially based upon relative roughness of the surface imaged. The smoother the surface, the more reflection away from the sensor, while rough surfaces give a much stronger return towards the imaging platform. As can be seen in Figure 4, if the height of the surface's roughness is less than 1/32 of the wavelength, mostly specular reflection occurs. If the height of the surface's roughness is greater than 1/2 the wavelength used, the echoes are scattered in all directions, giving a strong return back to the sensor. Figure 4: The amount of backscatter from a surface depends largely on the surface's roughness, with smooth surfaces getting the least returns and rough surfaces getting the strongest returns. Credit: The SAR Handbook","title":"Surface Roughness"},{"location":"guides/introduction_to_sar/#types-of-scattering","text":"Figure 5: Scattering mechanisms. Rough surfaces give bright returns due to the wide scattering. Vegetated surfaces cause volumetric scattering, which gives a darker return to the imaging platform. Double bounce returns, found mostly in urban areas, give the brightest return, as the majority of the energy is re-directed back towards the sensor. Credit: The SAR Handbook The resolution of Sentinel-1 SAR images is roughly 10 m. This means that a square of 10 meters on the ground is represented by a single pixel in the SAR image. The relative roughness of this patch of ground compared to the wavelength used will affect the backscatter strength (see Figure 4). However, there are additional types of bounce mechanisms beyond specular and diffuse, as shown in Figure 5. In vegetation, volumetric scattering occurs when signals bounce around inside the vegetation imaged. The double bounce mechanism which occurs in urban areas and is exploited by corner reflectors, causes chirp to be reflected directly back to the sensor, causing a very strong backscatter. Double bounce returns are so strong in some places that they cause over saturation of the sensor, resulting in visible sidelobes. These sidelobes are evidenced by bright crosses surrounding the double bounce target.","title":"Types of Scattering"},{"location":"guides/introduction_to_sar/#sar-scale","text":"SAR backscatter are recorded in both return strength and phase. Each pixel in a single-look complex SAR image represents these values as an imaginary number (I,Q). To create the visible images we are used to looking at, the SAR image is detected . This process calculates the square root of the sum of the squares of the I and Q values found in an SLC image, creating a so called intensity image. This image is real valued, and, when calibrated, gives the absolute backscatter of the surface imaged. Detected images can be stored using several different scales, including power, amplitude, and dB. Note the default scale of Sentinel-1 RTC products from HyP3 is power. However, in some cases, it may be desirable to convert the actual pixel values to a different scale. Two other scales commonly used for SAR data are amplitude and dB.","title":"SAR Scale"},{"location":"guides/introduction_to_sar/#power-scale","text":"The values in this scale are generally very close to zero, so the dynamic range of the SAR image can be easily skewed by a few bright scatterers in the image. Power scale is appropriate for statistical analysis of the SAR dataset, but may not always be the best option for data visualization. When viewing a SAR image in power scale in a GIS environment, it may appear mostly or all black, and you may need to adjust the stretch to see features in the image. Often applying a stretch of 2 standard deviations, or setting the Min-Max stretch values to 0 and 0.3, will greatly improve the appearance of the image. You can adjust the stretch as desired to display your image to full advantage. Be aware that this does not change the actual pixel values.","title":"Power Scale"},{"location":"guides/introduction_to_sar/#amplitude-scale","text":"Amplitude scale is the square root of the power scale values. This brightens the darker pixels and darkens the brighter pixels, narrowing the dynamic range of the image. In many cases, amplitude scale presents a pleasing grayscale display of RTC images. Amplitude scale works well for calculating log difference ratios (see ASF Sentinel-1 RTC Product Guide ).","title":"Amplitude Scale"},{"location":"guides/introduction_to_sar/#db-scale","text":"The dB scale is calculated by multiplying 10 times the Log10 of the power scale values. This scale brightens the pixels, allowing for better differentiation among very dark pixels. When identifying water on the landscape, this is often a good scale to use; the water pixels generally remain very dark, while the terrestrial pixels are even brighter (see Identifying Surface Water ). This scale is not always the best choice for general visualization of SAR products, as it can give a washed-out appearance, and because it is in a log scale, it is not appropriate for all types of statistical analyses.","title":"dB Scale"},{"location":"guides/introduction_to_sar/#geometric-distortions","text":"There are a number of distortions inherent to SAR data due to the side-looking nature of the sensor, and these impacts will be more prevalent in areas with rugged terrain. The process of radiometric terrain correction addresses the geometric distortions that lead to geolocation errors in terrain features, and also normalizes the backscatter values based on the actual area contributing returns. This process generates an image that aligns well with other geospatial data and is suitable for GIS applications or time-series analysis. The key distortions present in SAR images are foreshortening, layover and shadow (Figure 6). Figure 6: Distortions induced by side-looking SAR. Ground points a, b, c are \u2018seen\u2019 by radar as points a\u2019, b\u2019, c\u2019 in the slant range. Credit: Franz J. Meyer In the case of foreshortening , the backscatter from the front side of the mountain is compressed, with returns from a large area arriving back to the sensor at about the same time. This results in the front slope being displayed as a narrow, bright band. When layover occurs, returns from the front slope (and potentially even some of the area before the slope starts) are received at the same time as returns from the back slope. Thus, area in the front of the slope is projected onto the back side in the slant range image. In this case, the data from the front slope cannot be extracted from the returns. Another condition that results in missing data is radar shadow . In this case, the angle of the back slope is such that the sensor can not image it at all. These areas with steep back slopes offer no information to the SAR sensor. When RTC is performed, foreshortened areas are corrected based on the DEM. Areas impacted by layover or shadow, however, do not actually have data returns to correct. In this case, the pixels in the resulting RTC image will have a value of No Data. We do not interpolate missing data; users who would like to fill holes with estimated values will need to do so as appropriate for their particular application.","title":"Geometric Distortions"},{"location":"guides/introduction_to_sar/#speckle","text":"In most cases, the patch of ground illuminated by the SAR transmitter will not be homogeneous. Instead it will be comprised of many different types of individual scatterers. The scatterers may interfere with each other either strengthening the return or weakening it. This creates a grainy (salt & pepper) appearance in SAR imagery. This a result of the nature of SAR and, thus, occurs in all SAR scenes. Speckle in SAR images can be mitigated by multi-looking, which, in effect, uses averaging to smooth out the image, resulting in a more homogeneous appearance at the expense of resolution.","title":"Speckle"},{"location":"guides/rtc_atbd/","text":"RTC Algorithm Theoretical Basis \u00b6","title":"Theoretical Basis"},{"location":"guides/rtc_atbd/#rtc-algorithm-theoretical-basis","text":"","title":"RTC Algorithm Theoretical Basis"},{"location":"guides/rtc_product_guide/","text":"Sentinel-1 RTC Product Guide \u00b6 This document is a guide for users of Radiometrically Terrain Corrected (RTC) Sentinel-1 products generated by the Alaska Satellite Facility (ASF). Users can request RTC products On Demand in ASF's Vertex data portal, or make use of our Python SDK or API . SAR datasets inherently contain geometric and radiometric distortions due to terrain being imaged by a side-looking instrument. Radiometric terrain correction corrects these distortions and creates analysis-ready data suitable for use in GIS applications or time-series analysis. RTC processing is a required first step for many amplitude-based SAR applications. ASF's Sentinel-1 On-Demand RTC products are generated using GAMMA Software . Products are distributed as GeoTIFFs (one for each available polarization) projected to the appropriate UTM Zone for the location of the scene. A Digital Elevation Model (DEM) is required for processing RTC. ASF uses the best publicly-available DEM with full coverage of the processing area. For a step-by-step tutorial on ordering On-Demand RTC Products using Vertex, visit our RTC On Demand story map , which also includes links to sample workflows using Sentinel-1 RTC products for GIS applications. Introduction \u00b6 Sentinel-1 Mission \u00b6 The Sentinel-1 mission collects C-band band SAR from a pair of polar-orbiting satellites launched by the European Space Agency (ESA) as part of the Copernicus program . The Sentinel-1A satellite was launched April 3, 2014, and the Sentinel-1B satellite was launched April 25, 2016. The two Sentinel-1 satellites each have a 12-day repeat cycle, but their orbits are offset 180 degrees so that one or the other will pass over the same location on earth every 6 days. Most areas of the earth will still only have imagery collected every 12 days, but Europe and select areas of interest are imaged with a 6-day interval, as described in the mission observation scenario . Because this is a polar-orbiting satellite constellation, areas near the poles may have a number of overlapping paths, resulting in even more frequent acquisitions with similar footprints. The relatively short interval between acquisitions makes this SAR dataset a very useful tool for monitoring rapid or sudden landscape changes. In addition, SAR can image the earth's surface through cloud or smoke cover and does not require sunlight, so valid imagery can be collected on every pass. This is particularly useful for monitoring conditions during natural disasters such as hurricanes or wildfires, or in areas that are prone to frequent cloud cover. SAR Distortions \u00b6 There are a number of distortions inherent to SAR data due to the side-looking nature of the sensor, and these impacts will be more prevalent in areas with rugged terrain. The process of radiometric terrain correction addresses the geometric distortions that lead to geolocation errors in terrain features, and also normalizes the backscatter values based on the actual area contributing returns. This process generates an image that aligns well with other geospatial data and is suitable for GIS applications or time-series analysis. The key distortions present in SAR images are foreshortening, layover and shadow (Figure 1). Figure 1: Distortions induced by side-looking SAR. Ground points a, b, c are \u2018seen\u2019 by radar as points a\u2019, b\u2019, c\u2019 in the slant range. Credit: Franz J. Meyer In the case of foreshortening , the backscatter from the front side of the mountain is compressed, with returns from a large area arriving back to the sensor at about the same time. This results in the front slope being displayed as a narrow, bright band. When layover occurs, returns from the front slope (and potentially even some of the area before the slope starts) are received at the same time as returns from the back slope. Thus, area in the front of the slope is projected onto the back side in the slant range image. In this case, the data from the front slope cannot be extracted from the returns. Another condition that results in missing data is radar shadow . In this case, the angle of the back slope is such that the sensor can not image it at all. These areas with steep back slopes offer no information to the SAR sensor. When RTC is performed, foreshortened areas are corrected based on the DEM. Areas impacted by layover or shadow, however, do not actually have data returns to correct. In this case, the pixels in the resulting RTC image will have a value of No Data. We do not interpolate missing data; users who would like to fill holes with estimated values will need to do so as appropriate for their particular application. The RTC product package includes a Layover-Shadow mask (see Image Files section ) If you find that there are No Data pixels in your image, you can refer to that reference raster to see if the missing pixels are due to layover or shadow effects. Digital Elevation Models \u00b6 The quality of the terrain corrections are directly related to the quality of the digital elevation models (DEMs) used in the process of geometrically and radiometrically correcting the SAR imagery. We use DEMs that are publicly available and have wide-ranging coverage. In the past, ASF maintained a collection of DEMs that were pre-processed as appropriate for SAR workflows, and applied a preference hierarchy so that the best available DEM in any given area would be automatically selected for processing. With the public release of the GLO-30 Copernicus DEM , we have changed our default DEM strategy to leverage a cloud-hosted copy of the global Copernicus DEM. This is now the default DEM for processing RTC products. Copernicus DEM GLO-30 Updated We use the Copernicus DEM GLO-30 Public dataset as our default DEM for RTC and InSAR processing. We have now updated to the 2021 release of the Copernicus DEM GLO-30 Public dataset , which improves coverage over Norway, and includes 5 additional tiles. For more information, see the 'Releases' section of this article . Users still have the option to use the legacy DEMs when processing RTC jobs On Demand in Vertex and when using the API or SDK , but we recommend using the Copernicus DEM whenever possible. Table 1 summarizes ASF's DEM sources. Note that in each case, the DEM is resampled to RTC spacing and reprojected to a UTM Zone (WGS84), and a geoid correction is applied before being used for RTC processing. Resolution DEM Vertical Datum Area Posting Priority Medium GLO-30 EGM2008 Global 1 arc second Default High NED13 NAVD88 CONUS, Hawaii, parts of Alaska 1/3 arc seconds 1 Medium SRTMGL1 EGM96 60 N to 57 S latitude 1 arc second 2 Medium NED1 NAVD88 Canada 1 arc second 3 Low NED2 NAVD88 Parts of Alaska 2 arc seconds 4 Table 1: DEMs used for RTC processing. Note that the Copernicus 30 m DEM is the default, while the other four DEMs are only used if the legacy option is invoked. When ordering On-Demand RTC products, you can choose to include a copy of the DEM used for RTC processing in the RTC product package. This DEM copy is converted to 16-bit signed integer format, but is otherwise the same as the DEM used in the RTC process. Note that the height values will differ from the original source DEM in all cases, due to the geoid correction applied to prepare the DEM for use in RTC processing. Copernicus DEM \u00b6 The GLO-30 Copernicus DEM provides global coverage (with the current exception of an area covering Armenia and Azerbaijan, see Figure 3) at 30-m pixel spacing. When an RTC job is requested, we download the required DEM tiles from the Copernicus Digital Elevation Model (DEM) GLO-30 Public dataset available in the Registry of Open Data on AWS , managed by Sinergise . We mosaic the tiles and reproject them to the appropriate UTM Zone for the location of the SAR granule to be processed, resampling them to match the pixel spacing and alignment of the RTC product. A geoid correction is applied before it is used for RTC processing. Figure 2 shows the coverage of the Copernicus DEM GLO-30 Public dataset, and Figure 3 details the land area currently not covered. Figure 2: Copernicus DEM GLO-30 coverage map Figure 3: Detail of area currently not covered by Copernicus DEM GLO-30 Legacy DEMs \u00b6 The legacy DEMs were pre-processed by ASF to a consistent raster format (GeoTIFF) from the original source formats: height (*.hgt), ESRI ArcGrid (*.adf), etc. Many of the NASA-provided DEMs were provided as orthometric heights with EGM96 vertical datum. These were converted by ASF to ellipsoid heights using the ASF MapReady tool named geoid_adjust . The pixel reference varied from the center (pixel as point) to a corner (pixel as area). The GAMMA software used to generate the RTC products uses pixel as area and adjusts DEM coordinates as needed. These processed DEM collections are stored by ASF in AWS. When an RTC job is requested, the best-available DEM covering the SAR granule is selected, and the necessary tiles are reprojected to a mosaic in the UTM Zone appropriate for the granule location. If legacy DEM processing is selected, one of the following DEMs will be used: The National Elevation Dataset (NED) \u2153 arc second (about 10 m resolution) DEM covers the continental U.S. (CONUS), Hawaii, and parts of Alaska. Shuttle Radar Topography Mission (SRTM) GL1 data at 30 m resolution is used where NED 13 is not available. 1 arc second NED gives coverage of Canada at about 30 m resolution. 2 arc second NED (about 60 m) covers the remaining parts of Alaska above 60 degrees northern latitude. Since more than one DEM may be available in legacy processing, DEMs are selected in priority order as listed in Table 1. DEM coverage of at least 20% from a single DEM source is required for legacy processing to proceed. In no case will the DEM selected be from more than one source; only the single best source of terrain height values is used for a given scene. Figure 4 shows the coverage of the various legacy DEM sources. Figure 4: Coverage of the various legacy DEM sources used for terrain correction Radiometric Terrain Correction Workflow \u00b6 Pre-processing \u00b6 The first step of pre-processing is the selection of the best DEM for the terrain correction. The DEM tiles are assembled to ensure sufficient coverage for the terrain correction of the Sentinel-1 granule. The application of the calibration parameters and multi-looking are the only pre-processing steps applied to the SAR image. Terrain Correction \u00b6 The terrain correction is performed in slant range geometry. A mapping function is created, mapping from DEM space into SAR space. The actual mapping of the initial image into projected space is only applied once to mitigate the propagation of any resampling errors. All intermediate steps only update the look-up table used for the mapping. By default, images are not coregistered to the DEM. While RTC results can be improved by matching imagery to a high-quality DEM, different acquisitions over the same area may not always be matched to the DEM in the same way, due in part to the presence of speckle. This can introduce spatial inconsistencies to the dataset, especially when viewing a time-series of RTC images. For consistency, we use the geolocation from the Sentinel-1 state vectors rather than matching the geolocation based on DEM features. When custom-ordering imagery, however, the DEM Matching option is available for selection. In this case, the first step is the co-registration of the SAR image with a simulated SAR image derived from the DEM. An initial offset is first attempted as a single match; if it fails, a larger number of image chips are used to determine an average offset in azimuth and range direction. This initial offset is then refined using strict matching criteria. Matching may fail for three different reasons: (1) no match can be found, (2) the magnitude of the residual offset errors is greater than 2 pixels, or (3) the maximum calculated offset is greater than 50 m. In any of these cases, the dead reckoning approach is taken when matching fails. This approach solely relies on the geolocations calculated from state vectors (the same approach used when DEM matching is not selected as an option) - no geolocation refinement is applied. Radiometric Correction \u00b6 During processing, a surface scattering area image for the scene is calculated and saved. This projected area image is used to create the RTC product - the SAR image is multiplied by the ratio of an ellipsoidal scattering image (used during calibration) and this scattering area image. Note that this image is always projected to gamma-nought (\u03b3 0 ). Geocoding \u00b6 In a final step, the RTC product is geocoded into map-projected space. Thus, radiometric terrain correction results in a geocoded radiometrically calibrated multi-looked image with gamma-nought (\u03b3 0 ) power scale values by default, though there are options to process to sigma-nought (\u03c3 0 ) radiometry and amplitude scale. Post-Processing \u00b6 After the terrain correction is completed, the RTC products are exported to GeoTIFF format. If the scene being processed is dual polarization, users have the option to add a full-resolution RGB Decomposition GeoTIFF to the RTC product package. Side products including the DEM, layover shadow map, scattering area map, and incidence angle map are converted into GeoTIFF format. In addition, a README text file, browse images, item-specific ArcGIS-compatible XML metadata files, a log file, and a shapefile indicating the data extent are generated for the product. Product Packaging \u00b6 Naming Convention \u00b6 The naming convention for the RTC products follows this pattern for its base names: S1x_yy_aaaaaaaaTbbbbbb_ppo_RTCzz_u_defklm_ssss Example: S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A Element Definition Example x Mission: A or B A yy Beam Mode IW aaaaaaaa Start Year-Month-Day 20180128 bbbbbb Start Hour-Minute-Second 161201 pp Polarization: Dual-pol (D) vs. Single-pol (S), Primary Polarization (H or V) DV o Orbit Type: Precise (P), Restituted (R), or Original Predicted (O) P zz Terrain Correction Pixel Spacing (m) 30 u Software Package Used: GAMMA (G) G d Gamma-0 (g) or Sigma-0 (s) Output g e Power (p) or Amplitude (a) Output p f Unmasked (u) or Water Masked (w) u k Not Filtered (n) or Filtered (f) n l Entire Area (e) or Clipped Area (c) e m Dead Reckoning (d) or DEM Matching (m) d ssss Product ID FD6A Table 2: Naming convention for RTC products Default Settings \u00b6 The default settings for RTC products are as follows: Setting Default Radiometry Gamma-0 (g) Scale Power (p) Water Mask No water mask applied (u) Speckle Filter Not filtered (n) Clipping Entire extent of input granule (e) DEM Matching No matching; dead reckoning is used (d) Table 3: Default settings for RTC products Image Files \u00b6 All files are stored in a folder named using the above convention, and the base name for each file matches the folder name. Multiple types of image files are present in this folder, and some of the files are optional. Users can choose to exclude the RGB Decomposition GeoTIFF, scattering area map, DEM, and incidence angle map rasters when ordering On-Demand RTC products. Extension Description Example _VV.tif, _VH.tif, _HH.tif, _HV.tif Terrain corrected product stored in separate files for each available polarization in GeoTIFF format S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_VV.tif .png Greyscale browse image S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A.png _rgb.png Color browse image S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_rgb.png .kmz Zipped Google Earth image S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A.kmz _rgb.kmz Zipped Google Earth color image S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_rgb.kmz _rgb.tif Color decomposition in GeoTIFF format (optional) S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_rgb.tif _area.tif Scattering area map in GeoTIFF format (optional) S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_area.tif _dem.tif DEM used for terrain correction in GeoTIFF format (optional) S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_dem.tif _inc_map.tif Incidence angle file in GeoTIFF format (optional) S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_inc_map.tif _ls_map.tif Layover/shadow mask in GeoTIFF format S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_ls_map.tif Table 4: Image files in product package The RTC products (one for each available polarization) are generated as 32-bit floating-point single-band GeoTIFF files, as are the incidence angle and scattering area maps. The RGB Decomposition is a 3-band unsigned 8-bit GeoTIFF file, the layover/shadow mask is a single-band unsigned 8-bit GeoTIFF, and the DEM is a 16-bit unsigned integer GeoTIFF. The browse images (both grayscale and color) are generated in PNG format, and are each 2048 pixels wide. Finally, KMZ files suitable for viewing in Google Earth are included. Note that colorized products (RGB Decomposition GeoTIFF or color browse PNG) can only be created for dual-polarization (SDV and SDH) granules, not for single-polarization (SSV or SSH). Metadata Files \u00b6 The product package also includes a number of metadata files. Extension Description Example .README.md.txt README file S1A _IW _20180128T161201 _DVP _RTC30 _G _gpuned _FD6A.README.md.txt .log Log file of the processing steps S1A _IW _20180128T161201 _DVP _RTC30 _G _gpuned _FD6A.log .tif.xml ArcGIS compliant XML metadata for GeoTIFF files S1A _IW _20180128T161201 _DVP _RTC30 _G _gpuned _FD6A _VV.tif.xml .png.xml ArcGIS compliant XML metadata for PNG files S1A _IW _20180128T161201 _DVP _RTC30 _G _gpuned _FD6A.png.xml .png.aux.xml Geolocation metadata for PNG browse images S1A _IW _20180128T161201 _DVP _RTC30 _G _gpuned _FD6A.png.aux.xml Table 5: Metadata files and their extensions README File \u00b6 The text file with extension .README.md.txt explains the files included in the folder, and is customized to reflect that particular product. Users unfamiliar with RTC products should start by reading this README file, which will give some background on each of the files included in the product folder. ArcGIS-Compatible XML Files \u00b6 There is an ArcGIS-compatible xml file for each raster in the product folder. When ArcGIS Desktop users view any of the rasters in ArcCatalog or the Catalog window in ArcMap, they can open the Item Description to view the contents of the associated xml file. ArcGIS Pro users can access the information from the Metadata tab. These files will not appear as separate items in ArcCatalog, though if you use Windows Explorer to look at the contents of the folder you will see them listed individually. Because each one is named identically to the product it describes (with the addition of the .xml extension), ArcGIS recognizes the appropriate file as the raster\u2019s associated metadata, and integrates the metadata accordingly. ArcGIS users should take care not to change these xml files outside of the ArcGIS environment; changing the filename or content directly may render the files unreadable by ArcGIS. Those not using ArcGIS will still find the contents of these xml files useful, but will have to contend with the xml tagging when viewing the files as text or in a browser. Auxiliary Geolocation Files \u00b6 Geolocation XML files (aux files) are included for each of the PNG browse images to allow for proper display in GIS platforms. Log File \u00b6 A log file detailing the processing parameters and outputs is also included for reference. Shapefile \u00b6 A shapefile indicating the extent of the RTC data coverage is included in the package. Extension Description Example _shape.dbf _shape.prj _shape.shp _shape.shx Shapefile (.shp) and supporting files S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_shape.shp Table 6: Shapefile files and their extensions SAR Scales \u00b6 Power Scale \u00b6 Note that the default output of Sentinel-1 RTC products from HyP3 is in power scale. The values in this scale are generally very close to zero, so the dynamic range of the RTC image can be easily skewed by a few bright scatterers in the image. Power scale is appropriate for statistical analysis of the RTC dataset, but may not always be the best option for data visualization. When viewing an RTC image in power scale in a GIS environment, it may appear mostly or all black, and you may need to adjust the stretch to see features in the image. Often applying a stretch of 2 standard deviations, or setting the Min-Max stretch values to 0 and 0.3, will greatly improve the appearance of the image. You can adjust the stretch as desired to display your image to full advantage. Be aware that this does not change the actual pixel values. In some cases, it may be desirable to convert the actual pixel values to a different scale. Two other scales commonly used for SAR data are amplitude and dB. Amplitude Scale \u00b6 Amplitude scale is the square root of the power scale values. This brightens the darker pixels and darkens the brighter pixels, narrowing the dynamic range of the image. In many cases, amplitude scale presents a pleasing grayscale display of RTC images. Amplitude scale works well for calculating log difference ratios (see Change Detection Using RTC Data ). dB Scale \u00b6 The dB scale is calculated by multiplying 10 times the Log10 of the power scale values. This scale brightens the pixels, allowing for better differentiation among very dark pixels. When identifying water on the landscape, this is often a good scale to use; the water pixels generally remain very dark, while the terrestrial pixels are even brighter (see Identifying Surface Water ). This scale is not always the best choice for general visualization of RTC products, as it can give a washed-out appearance, and because it is in a log scale, it is not appropriate for all types of statistical analyses. RTC Use Examples \u00b6 The RTC products are presented as Cloud-Optimized GeoTIFFs (COGs), a user-friendly format that is GIS compatible. The products include pre-generated overviews, so users will not need to generate pyramids to display the images efficiently in a GIS environment. The following sections present examples of how one might use RTC datasets to identify areas of change and integrate RTC datasets into other datasets for enhanced results. We also present a bibliography of some of the scientific literature making use of Sentinel-1 RTC datasets. Change Detection Using RTC Data \u00b6 There are a number of ways that SAR data sets can be used to identify areas of change. Here are two examples of what you can do in a GIS environment. Seasonal Change \u00b6 Stacking RTC images into a multiband image (Figure 5) allows the user to display different times of year at the same time, using the color bands to highlight areas that differ in radar backscatter values from one month to the next. To generate this type of image, choose three images that capture different seasons or months of interest. These can either be individual RTC images from different times of the year, or rasters displaying the monthly median calculated from multiple RTC images collected in the same month. Combine the three images into a multiband raster and assign each to a different color band. The resulting RGB image highlights areas where there are distinctive differences among the three source image values. Figure 5: Monthly median VH gamma-0 power values for May, July and September, displayed as a multiband RGB (May, July, Sept) image. Contains modified Copernicus Sentinel data 2017, processed by ESA. Quantifying Change over Time \u00b6 A simple and informative approach to change detection is the calculation of the log difference between two RTC datasets from different dates. By calculating Log10(date2/date1) and applying a classified symbology, it is easy to identify areas where change occurred, as well as the direction of the change. Negative values indicate a decrease in radar backscatter over time, while positive values indicate an increase in backscatter. In the example below (Figure 6), RTC images from before and after heavy rains caused a dam breach. The area where the reservoir was located displays a significant increase in backscatter (symbolized in red). This positive change is driven by land that was once covered by standing water, which generally has very low backscatter, now being exposed saturated soil, which generally returns very high backscatter values. In surrounding areas, decreases in radar backscatter (symbolized by blue), are possibly the result of agricultural fields undergoing desiccation/hardening of the surface soil following the heavy rainfall and standing water. Areas with little change in backscatter are displayed in yellow. Figure 6: Log Difference Raster with Classified Symbology. Contains modified Copernicus Sentinel data 2020, processed by ESA. Identifying Surface Water \u00b6 Calm surface water has a very low radar cross section. Most of the signal is reflected off the smooth surface, due to the high dielectric constant of freshwater, so little to none of the signal is returned as backscatter. Because of this, it is often easy to delineate surface water using a simple threshold value, where all pixels below the threshold are assumed to be water. You can easily visualize the water extent using various thresholds by applying a classified symbology with two classes. It is often best to use dB scale datasets for identifying surface water. In many cases, there will be a bimodal distribution of values in an RTC image containing surface water, with the first peak comprised mostly of water values, and the second peak containing all the remaining values. A good first step is to select a break point between those two peaks, then adjust the value as needed to generate a good water mask (Figure 7). Figure 7: Setting the break point to fall between the two peaks of the histogram Once you have determined the appropriate threshold (Figure 8), you can reclassify the RTC image to include only those pixels that fall below the threshold value, providing a water mask that can be used for analysis or to overlay with other imagery to show the water extent. Figure 8: Water Mask. Contains modified Copernicus Sentinel data 2020, processed by ESA. Combination of RTC Image with other Remote Sensing Data \u00b6 One of the main advantages of using RTC imagery with its all weather and day/night capabilities is the combination with other remote sensing data such as optical data. In the example below, the backscatter information of the Sentinel-1 SAR image (Figure 9) is used to enhance the spectral information of the optical Landsat 8 image (Figure 10) in the urban area of Pavia, Italy. Figure 11 shows the image fusion result of an IHS transformation. In this transformation the color channels red, green and blue (RGB) are first converted into a different color representation: intensity, hue and saturation (IHS). In the second step the optical intensity is replaced by the SAR image, before IHS is transformed back to RGB. Figure 9: Sentinel-1 RTC image. Figure 10: False color composite (bands 5, 4, 3) of a Landsat 8 image The color values for the two rivers in the SAR image are far more similar to each other than in the optical image. The vegetated areas (highlighted in red) show up more uniformly in the data fusion result than in the optical false color composite image. Image fusion uses the complementary nature of the different sources to generate an enhanced product. Figure 11: Image fusion result of SAR and optical imagery ArcGIS Toolbox \u00b6 ASF has developed a custom ArcGIS Toolbox for working with RTC datasets in either ArcGIS Desktop or ArcGIS Pro. It includes tools for converting between different SAR scales, calculating the log difference between two images, generating RGB Decomposition (false-color) products, and reclassifying a raster to generate a water mask. For more information and to download the toolbox, visit our website: https://asf.alaska.edu/how-to/data-tools/gis-tools/ . Application Examples in the Literature \u00b6 The following journal articles represent some of the work being done using Radiometric Terrain Corrected Sentinel-1 data sets. Crop Monitoring \u00b6 Clauss, K., Ottinger M. and Kuenzer, C. 2018. Mapping rice areas with Sentinel-1 time series and superpixel segmentation. International Journal of Remote Sensing , 39 (5):1399-1420. DOI: 10.1080/01431161.2017.1404162 Nguyen, D.B., Gruber A. and Wagner, W. 2016. Mapping rice extent and cropping scheme in the Mekong Delta using Sentinel-1A data. Remote Sensing Letters , 7 (12):1209-1218. DOI: 10.1080/2150704X.2016.1225172 Disaster Response \u00b6 Markert, K.N., Chishtie, F., Anderson, E.R., Saah, D., Griffin, R.E. 2018. On the merging of optical and SAR satellite imagery for surface water mapping applications. Results In Physics , 9 :275-277. DOI: 10.1016/j.rinp.2018.02.054 Twele, A., Cao, W., Plank, S. and Martinis, S. 2016. Sentinel-1-based flood mapping: a fully automated processing chain. International Journal of Remote Sensing , 37 (13):2990-3004. DOI: 10.1080/01431161.2016.1192304 Land Classification and Change Detection \u00b6 Muro, J., Canty, M., Conradsen, K., H\u00fcttich, C., Nielsen, A.A., Skriver, H., Remy, F., Strauch, A., Thonfeld, F. and Menz, G. 2016. Short-Term change detection in wetlands using Sentinel-1 time series. Remote Sensing , 8 (10):795. DOI: 10.3390/rs8100795 R\u00fcetschi, M., Schaepman, M.E., Small, D. 2018. Using Multitemporal Sentinel-1 C-band backscatter to monitor phenology and classify deciduous and coniferous forests in Northern Switzerland. Remote Sensing , 10 (1):55. DOI: 10.3390/rs10010055 Data Access \u00b6 To view or download Sentinel-1 RTC products, please see the links below: Vertex: https://search.asf.alaska.edu/ API: https://asf.alaska.edu/api/ For details on accessing data, including other SAR datasets, see ASF\u2019s Get Started guide: https://asf.alaska.edu/how-to/get-started/ To access data recipes, which are step-by-step tutorials for processing and working with SAR data, see ASF\u2019s tutorials page: https://asf.alaska.edu/how-to/data-recipes/data-recipe-tutorials/","title":"Product Guide"},{"location":"guides/rtc_product_guide/#sentinel-1-rtc-product-guide","text":"This document is a guide for users of Radiometrically Terrain Corrected (RTC) Sentinel-1 products generated by the Alaska Satellite Facility (ASF). Users can request RTC products On Demand in ASF's Vertex data portal, or make use of our Python SDK or API . SAR datasets inherently contain geometric and radiometric distortions due to terrain being imaged by a side-looking instrument. Radiometric terrain correction corrects these distortions and creates analysis-ready data suitable for use in GIS applications or time-series analysis. RTC processing is a required first step for many amplitude-based SAR applications. ASF's Sentinel-1 On-Demand RTC products are generated using GAMMA Software . Products are distributed as GeoTIFFs (one for each available polarization) projected to the appropriate UTM Zone for the location of the scene. A Digital Elevation Model (DEM) is required for processing RTC. ASF uses the best publicly-available DEM with full coverage of the processing area. For a step-by-step tutorial on ordering On-Demand RTC Products using Vertex, visit our RTC On Demand story map , which also includes links to sample workflows using Sentinel-1 RTC products for GIS applications.","title":"Sentinel-1 RTC Product Guide"},{"location":"guides/rtc_product_guide/#introduction","text":"","title":"Introduction"},{"location":"guides/rtc_product_guide/#sentinel-1-mission","text":"The Sentinel-1 mission collects C-band band SAR from a pair of polar-orbiting satellites launched by the European Space Agency (ESA) as part of the Copernicus program . The Sentinel-1A satellite was launched April 3, 2014, and the Sentinel-1B satellite was launched April 25, 2016. The two Sentinel-1 satellites each have a 12-day repeat cycle, but their orbits are offset 180 degrees so that one or the other will pass over the same location on earth every 6 days. Most areas of the earth will still only have imagery collected every 12 days, but Europe and select areas of interest are imaged with a 6-day interval, as described in the mission observation scenario . Because this is a polar-orbiting satellite constellation, areas near the poles may have a number of overlapping paths, resulting in even more frequent acquisitions with similar footprints. The relatively short interval between acquisitions makes this SAR dataset a very useful tool for monitoring rapid or sudden landscape changes. In addition, SAR can image the earth's surface through cloud or smoke cover and does not require sunlight, so valid imagery can be collected on every pass. This is particularly useful for monitoring conditions during natural disasters such as hurricanes or wildfires, or in areas that are prone to frequent cloud cover.","title":"Sentinel-1 Mission"},{"location":"guides/rtc_product_guide/#sar-distortions","text":"There are a number of distortions inherent to SAR data due to the side-looking nature of the sensor, and these impacts will be more prevalent in areas with rugged terrain. The process of radiometric terrain correction addresses the geometric distortions that lead to geolocation errors in terrain features, and also normalizes the backscatter values based on the actual area contributing returns. This process generates an image that aligns well with other geospatial data and is suitable for GIS applications or time-series analysis. The key distortions present in SAR images are foreshortening, layover and shadow (Figure 1). Figure 1: Distortions induced by side-looking SAR. Ground points a, b, c are \u2018seen\u2019 by radar as points a\u2019, b\u2019, c\u2019 in the slant range. Credit: Franz J. Meyer In the case of foreshortening , the backscatter from the front side of the mountain is compressed, with returns from a large area arriving back to the sensor at about the same time. This results in the front slope being displayed as a narrow, bright band. When layover occurs, returns from the front slope (and potentially even some of the area before the slope starts) are received at the same time as returns from the back slope. Thus, area in the front of the slope is projected onto the back side in the slant range image. In this case, the data from the front slope cannot be extracted from the returns. Another condition that results in missing data is radar shadow . In this case, the angle of the back slope is such that the sensor can not image it at all. These areas with steep back slopes offer no information to the SAR sensor. When RTC is performed, foreshortened areas are corrected based on the DEM. Areas impacted by layover or shadow, however, do not actually have data returns to correct. In this case, the pixels in the resulting RTC image will have a value of No Data. We do not interpolate missing data; users who would like to fill holes with estimated values will need to do so as appropriate for their particular application. The RTC product package includes a Layover-Shadow mask (see Image Files section ) If you find that there are No Data pixels in your image, you can refer to that reference raster to see if the missing pixels are due to layover or shadow effects.","title":"SAR Distortions"},{"location":"guides/rtc_product_guide/#digital-elevation-models","text":"The quality of the terrain corrections are directly related to the quality of the digital elevation models (DEMs) used in the process of geometrically and radiometrically correcting the SAR imagery. We use DEMs that are publicly available and have wide-ranging coverage. In the past, ASF maintained a collection of DEMs that were pre-processed as appropriate for SAR workflows, and applied a preference hierarchy so that the best available DEM in any given area would be automatically selected for processing. With the public release of the GLO-30 Copernicus DEM , we have changed our default DEM strategy to leverage a cloud-hosted copy of the global Copernicus DEM. This is now the default DEM for processing RTC products. Copernicus DEM GLO-30 Updated We use the Copernicus DEM GLO-30 Public dataset as our default DEM for RTC and InSAR processing. We have now updated to the 2021 release of the Copernicus DEM GLO-30 Public dataset , which improves coverage over Norway, and includes 5 additional tiles. For more information, see the 'Releases' section of this article . Users still have the option to use the legacy DEMs when processing RTC jobs On Demand in Vertex and when using the API or SDK , but we recommend using the Copernicus DEM whenever possible. Table 1 summarizes ASF's DEM sources. Note that in each case, the DEM is resampled to RTC spacing and reprojected to a UTM Zone (WGS84), and a geoid correction is applied before being used for RTC processing. Resolution DEM Vertical Datum Area Posting Priority Medium GLO-30 EGM2008 Global 1 arc second Default High NED13 NAVD88 CONUS, Hawaii, parts of Alaska 1/3 arc seconds 1 Medium SRTMGL1 EGM96 60 N to 57 S latitude 1 arc second 2 Medium NED1 NAVD88 Canada 1 arc second 3 Low NED2 NAVD88 Parts of Alaska 2 arc seconds 4 Table 1: DEMs used for RTC processing. Note that the Copernicus 30 m DEM is the default, while the other four DEMs are only used if the legacy option is invoked. When ordering On-Demand RTC products, you can choose to include a copy of the DEM used for RTC processing in the RTC product package. This DEM copy is converted to 16-bit signed integer format, but is otherwise the same as the DEM used in the RTC process. Note that the height values will differ from the original source DEM in all cases, due to the geoid correction applied to prepare the DEM for use in RTC processing.","title":"Digital Elevation Models"},{"location":"guides/rtc_product_guide/#copernicus-dem","text":"The GLO-30 Copernicus DEM provides global coverage (with the current exception of an area covering Armenia and Azerbaijan, see Figure 3) at 30-m pixel spacing. When an RTC job is requested, we download the required DEM tiles from the Copernicus Digital Elevation Model (DEM) GLO-30 Public dataset available in the Registry of Open Data on AWS , managed by Sinergise . We mosaic the tiles and reproject them to the appropriate UTM Zone for the location of the SAR granule to be processed, resampling them to match the pixel spacing and alignment of the RTC product. A geoid correction is applied before it is used for RTC processing. Figure 2 shows the coverage of the Copernicus DEM GLO-30 Public dataset, and Figure 3 details the land area currently not covered. Figure 2: Copernicus DEM GLO-30 coverage map Figure 3: Detail of area currently not covered by Copernicus DEM GLO-30","title":"Copernicus DEM"},{"location":"guides/rtc_product_guide/#legacy-dems","text":"The legacy DEMs were pre-processed by ASF to a consistent raster format (GeoTIFF) from the original source formats: height (*.hgt), ESRI ArcGrid (*.adf), etc. Many of the NASA-provided DEMs were provided as orthometric heights with EGM96 vertical datum. These were converted by ASF to ellipsoid heights using the ASF MapReady tool named geoid_adjust . The pixel reference varied from the center (pixel as point) to a corner (pixel as area). The GAMMA software used to generate the RTC products uses pixel as area and adjusts DEM coordinates as needed. These processed DEM collections are stored by ASF in AWS. When an RTC job is requested, the best-available DEM covering the SAR granule is selected, and the necessary tiles are reprojected to a mosaic in the UTM Zone appropriate for the granule location. If legacy DEM processing is selected, one of the following DEMs will be used: The National Elevation Dataset (NED) \u2153 arc second (about 10 m resolution) DEM covers the continental U.S. (CONUS), Hawaii, and parts of Alaska. Shuttle Radar Topography Mission (SRTM) GL1 data at 30 m resolution is used where NED 13 is not available. 1 arc second NED gives coverage of Canada at about 30 m resolution. 2 arc second NED (about 60 m) covers the remaining parts of Alaska above 60 degrees northern latitude. Since more than one DEM may be available in legacy processing, DEMs are selected in priority order as listed in Table 1. DEM coverage of at least 20% from a single DEM source is required for legacy processing to proceed. In no case will the DEM selected be from more than one source; only the single best source of terrain height values is used for a given scene. Figure 4 shows the coverage of the various legacy DEM sources. Figure 4: Coverage of the various legacy DEM sources used for terrain correction","title":"Legacy DEMs"},{"location":"guides/rtc_product_guide/#radiometric-terrain-correction-workflow","text":"","title":"Radiometric Terrain Correction Workflow"},{"location":"guides/rtc_product_guide/#pre-processing","text":"The first step of pre-processing is the selection of the best DEM for the terrain correction. The DEM tiles are assembled to ensure sufficient coverage for the terrain correction of the Sentinel-1 granule. The application of the calibration parameters and multi-looking are the only pre-processing steps applied to the SAR image.","title":"Pre-processing"},{"location":"guides/rtc_product_guide/#terrain-correction","text":"The terrain correction is performed in slant range geometry. A mapping function is created, mapping from DEM space into SAR space. The actual mapping of the initial image into projected space is only applied once to mitigate the propagation of any resampling errors. All intermediate steps only update the look-up table used for the mapping. By default, images are not coregistered to the DEM. While RTC results can be improved by matching imagery to a high-quality DEM, different acquisitions over the same area may not always be matched to the DEM in the same way, due in part to the presence of speckle. This can introduce spatial inconsistencies to the dataset, especially when viewing a time-series of RTC images. For consistency, we use the geolocation from the Sentinel-1 state vectors rather than matching the geolocation based on DEM features. When custom-ordering imagery, however, the DEM Matching option is available for selection. In this case, the first step is the co-registration of the SAR image with a simulated SAR image derived from the DEM. An initial offset is first attempted as a single match; if it fails, a larger number of image chips are used to determine an average offset in azimuth and range direction. This initial offset is then refined using strict matching criteria. Matching may fail for three different reasons: (1) no match can be found, (2) the magnitude of the residual offset errors is greater than 2 pixels, or (3) the maximum calculated offset is greater than 50 m. In any of these cases, the dead reckoning approach is taken when matching fails. This approach solely relies on the geolocations calculated from state vectors (the same approach used when DEM matching is not selected as an option) - no geolocation refinement is applied.","title":"Terrain Correction"},{"location":"guides/rtc_product_guide/#radiometric-correction","text":"During processing, a surface scattering area image for the scene is calculated and saved. This projected area image is used to create the RTC product - the SAR image is multiplied by the ratio of an ellipsoidal scattering image (used during calibration) and this scattering area image. Note that this image is always projected to gamma-nought (\u03b3 0 ).","title":"Radiometric Correction"},{"location":"guides/rtc_product_guide/#geocoding","text":"In a final step, the RTC product is geocoded into map-projected space. Thus, radiometric terrain correction results in a geocoded radiometrically calibrated multi-looked image with gamma-nought (\u03b3 0 ) power scale values by default, though there are options to process to sigma-nought (\u03c3 0 ) radiometry and amplitude scale.","title":"Geocoding"},{"location":"guides/rtc_product_guide/#post-processing","text":"After the terrain correction is completed, the RTC products are exported to GeoTIFF format. If the scene being processed is dual polarization, users have the option to add a full-resolution RGB Decomposition GeoTIFF to the RTC product package. Side products including the DEM, layover shadow map, scattering area map, and incidence angle map are converted into GeoTIFF format. In addition, a README text file, browse images, item-specific ArcGIS-compatible XML metadata files, a log file, and a shapefile indicating the data extent are generated for the product.","title":"Post-Processing"},{"location":"guides/rtc_product_guide/#product-packaging","text":"","title":"Product Packaging"},{"location":"guides/rtc_product_guide/#naming-convention","text":"The naming convention for the RTC products follows this pattern for its base names: S1x_yy_aaaaaaaaTbbbbbb_ppo_RTCzz_u_defklm_ssss Example: S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A Element Definition Example x Mission: A or B A yy Beam Mode IW aaaaaaaa Start Year-Month-Day 20180128 bbbbbb Start Hour-Minute-Second 161201 pp Polarization: Dual-pol (D) vs. Single-pol (S), Primary Polarization (H or V) DV o Orbit Type: Precise (P), Restituted (R), or Original Predicted (O) P zz Terrain Correction Pixel Spacing (m) 30 u Software Package Used: GAMMA (G) G d Gamma-0 (g) or Sigma-0 (s) Output g e Power (p) or Amplitude (a) Output p f Unmasked (u) or Water Masked (w) u k Not Filtered (n) or Filtered (f) n l Entire Area (e) or Clipped Area (c) e m Dead Reckoning (d) or DEM Matching (m) d ssss Product ID FD6A Table 2: Naming convention for RTC products","title":"Naming Convention"},{"location":"guides/rtc_product_guide/#default-settings","text":"The default settings for RTC products are as follows: Setting Default Radiometry Gamma-0 (g) Scale Power (p) Water Mask No water mask applied (u) Speckle Filter Not filtered (n) Clipping Entire extent of input granule (e) DEM Matching No matching; dead reckoning is used (d) Table 3: Default settings for RTC products","title":"Default Settings"},{"location":"guides/rtc_product_guide/#image-files","text":"All files are stored in a folder named using the above convention, and the base name for each file matches the folder name. Multiple types of image files are present in this folder, and some of the files are optional. Users can choose to exclude the RGB Decomposition GeoTIFF, scattering area map, DEM, and incidence angle map rasters when ordering On-Demand RTC products. Extension Description Example _VV.tif, _VH.tif, _HH.tif, _HV.tif Terrain corrected product stored in separate files for each available polarization in GeoTIFF format S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_VV.tif .png Greyscale browse image S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A.png _rgb.png Color browse image S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_rgb.png .kmz Zipped Google Earth image S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A.kmz _rgb.kmz Zipped Google Earth color image S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_rgb.kmz _rgb.tif Color decomposition in GeoTIFF format (optional) S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_rgb.tif _area.tif Scattering area map in GeoTIFF format (optional) S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_area.tif _dem.tif DEM used for terrain correction in GeoTIFF format (optional) S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_dem.tif _inc_map.tif Incidence angle file in GeoTIFF format (optional) S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_inc_map.tif _ls_map.tif Layover/shadow mask in GeoTIFF format S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_ls_map.tif Table 4: Image files in product package The RTC products (one for each available polarization) are generated as 32-bit floating-point single-band GeoTIFF files, as are the incidence angle and scattering area maps. The RGB Decomposition is a 3-band unsigned 8-bit GeoTIFF file, the layover/shadow mask is a single-band unsigned 8-bit GeoTIFF, and the DEM is a 16-bit unsigned integer GeoTIFF. The browse images (both grayscale and color) are generated in PNG format, and are each 2048 pixels wide. Finally, KMZ files suitable for viewing in Google Earth are included. Note that colorized products (RGB Decomposition GeoTIFF or color browse PNG) can only be created for dual-polarization (SDV and SDH) granules, not for single-polarization (SSV or SSH).","title":"Image Files"},{"location":"guides/rtc_product_guide/#metadata-files","text":"The product package also includes a number of metadata files. Extension Description Example .README.md.txt README file S1A _IW _20180128T161201 _DVP _RTC30 _G _gpuned _FD6A.README.md.txt .log Log file of the processing steps S1A _IW _20180128T161201 _DVP _RTC30 _G _gpuned _FD6A.log .tif.xml ArcGIS compliant XML metadata for GeoTIFF files S1A _IW _20180128T161201 _DVP _RTC30 _G _gpuned _FD6A _VV.tif.xml .png.xml ArcGIS compliant XML metadata for PNG files S1A _IW _20180128T161201 _DVP _RTC30 _G _gpuned _FD6A.png.xml .png.aux.xml Geolocation metadata for PNG browse images S1A _IW _20180128T161201 _DVP _RTC30 _G _gpuned _FD6A.png.aux.xml Table 5: Metadata files and their extensions","title":"Metadata Files"},{"location":"guides/rtc_product_guide/#readme-file","text":"The text file with extension .README.md.txt explains the files included in the folder, and is customized to reflect that particular product. Users unfamiliar with RTC products should start by reading this README file, which will give some background on each of the files included in the product folder.","title":"README File"},{"location":"guides/rtc_product_guide/#arcgis-compatible-xml-files","text":"There is an ArcGIS-compatible xml file for each raster in the product folder. When ArcGIS Desktop users view any of the rasters in ArcCatalog or the Catalog window in ArcMap, they can open the Item Description to view the contents of the associated xml file. ArcGIS Pro users can access the information from the Metadata tab. These files will not appear as separate items in ArcCatalog, though if you use Windows Explorer to look at the contents of the folder you will see them listed individually. Because each one is named identically to the product it describes (with the addition of the .xml extension), ArcGIS recognizes the appropriate file as the raster\u2019s associated metadata, and integrates the metadata accordingly. ArcGIS users should take care not to change these xml files outside of the ArcGIS environment; changing the filename or content directly may render the files unreadable by ArcGIS. Those not using ArcGIS will still find the contents of these xml files useful, but will have to contend with the xml tagging when viewing the files as text or in a browser.","title":"ArcGIS-Compatible XML Files"},{"location":"guides/rtc_product_guide/#auxiliary-geolocation-files","text":"Geolocation XML files (aux files) are included for each of the PNG browse images to allow for proper display in GIS platforms.","title":"Auxiliary Geolocation Files"},{"location":"guides/rtc_product_guide/#log-file","text":"A log file detailing the processing parameters and outputs is also included for reference.","title":"Log File"},{"location":"guides/rtc_product_guide/#shapefile","text":"A shapefile indicating the extent of the RTC data coverage is included in the package. Extension Description Example _shape.dbf _shape.prj _shape.shp _shape.shx Shapefile (.shp) and supporting files S1A_IW_20180128T161201_DVP_RTC30_G_gpuned_FD6A_shape.shp Table 6: Shapefile files and their extensions","title":"Shapefile"},{"location":"guides/rtc_product_guide/#sar-scales","text":"","title":"SAR Scales"},{"location":"guides/rtc_product_guide/#power-scale","text":"Note that the default output of Sentinel-1 RTC products from HyP3 is in power scale. The values in this scale are generally very close to zero, so the dynamic range of the RTC image can be easily skewed by a few bright scatterers in the image. Power scale is appropriate for statistical analysis of the RTC dataset, but may not always be the best option for data visualization. When viewing an RTC image in power scale in a GIS environment, it may appear mostly or all black, and you may need to adjust the stretch to see features in the image. Often applying a stretch of 2 standard deviations, or setting the Min-Max stretch values to 0 and 0.3, will greatly improve the appearance of the image. You can adjust the stretch as desired to display your image to full advantage. Be aware that this does not change the actual pixel values. In some cases, it may be desirable to convert the actual pixel values to a different scale. Two other scales commonly used for SAR data are amplitude and dB.","title":"Power Scale"},{"location":"guides/rtc_product_guide/#amplitude-scale","text":"Amplitude scale is the square root of the power scale values. This brightens the darker pixels and darkens the brighter pixels, narrowing the dynamic range of the image. In many cases, amplitude scale presents a pleasing grayscale display of RTC images. Amplitude scale works well for calculating log difference ratios (see Change Detection Using RTC Data ).","title":"Amplitude Scale"},{"location":"guides/rtc_product_guide/#db-scale","text":"The dB scale is calculated by multiplying 10 times the Log10 of the power scale values. This scale brightens the pixels, allowing for better differentiation among very dark pixels. When identifying water on the landscape, this is often a good scale to use; the water pixels generally remain very dark, while the terrestrial pixels are even brighter (see Identifying Surface Water ). This scale is not always the best choice for general visualization of RTC products, as it can give a washed-out appearance, and because it is in a log scale, it is not appropriate for all types of statistical analyses.","title":"dB Scale"},{"location":"guides/rtc_product_guide/#rtc-use-examples","text":"The RTC products are presented as Cloud-Optimized GeoTIFFs (COGs), a user-friendly format that is GIS compatible. The products include pre-generated overviews, so users will not need to generate pyramids to display the images efficiently in a GIS environment. The following sections present examples of how one might use RTC datasets to identify areas of change and integrate RTC datasets into other datasets for enhanced results. We also present a bibliography of some of the scientific literature making use of Sentinel-1 RTC datasets.","title":"RTC Use Examples"},{"location":"guides/rtc_product_guide/#change-detection-using-rtc-data","text":"There are a number of ways that SAR data sets can be used to identify areas of change. Here are two examples of what you can do in a GIS environment.","title":"Change Detection Using RTC Data"},{"location":"guides/rtc_product_guide/#seasonal-change","text":"Stacking RTC images into a multiband image (Figure 5) allows the user to display different times of year at the same time, using the color bands to highlight areas that differ in radar backscatter values from one month to the next. To generate this type of image, choose three images that capture different seasons or months of interest. These can either be individual RTC images from different times of the year, or rasters displaying the monthly median calculated from multiple RTC images collected in the same month. Combine the three images into a multiband raster and assign each to a different color band. The resulting RGB image highlights areas where there are distinctive differences among the three source image values. Figure 5: Monthly median VH gamma-0 power values for May, July and September, displayed as a multiband RGB (May, July, Sept) image. Contains modified Copernicus Sentinel data 2017, processed by ESA.","title":"Seasonal Change"},{"location":"guides/rtc_product_guide/#quantifying-change-over-time","text":"A simple and informative approach to change detection is the calculation of the log difference between two RTC datasets from different dates. By calculating Log10(date2/date1) and applying a classified symbology, it is easy to identify areas where change occurred, as well as the direction of the change. Negative values indicate a decrease in radar backscatter over time, while positive values indicate an increase in backscatter. In the example below (Figure 6), RTC images from before and after heavy rains caused a dam breach. The area where the reservoir was located displays a significant increase in backscatter (symbolized in red). This positive change is driven by land that was once covered by standing water, which generally has very low backscatter, now being exposed saturated soil, which generally returns very high backscatter values. In surrounding areas, decreases in radar backscatter (symbolized by blue), are possibly the result of agricultural fields undergoing desiccation/hardening of the surface soil following the heavy rainfall and standing water. Areas with little change in backscatter are displayed in yellow. Figure 6: Log Difference Raster with Classified Symbology. Contains modified Copernicus Sentinel data 2020, processed by ESA.","title":"Quantifying Change over Time"},{"location":"guides/rtc_product_guide/#identifying-surface-water","text":"Calm surface water has a very low radar cross section. Most of the signal is reflected off the smooth surface, due to the high dielectric constant of freshwater, so little to none of the signal is returned as backscatter. Because of this, it is often easy to delineate surface water using a simple threshold value, where all pixels below the threshold are assumed to be water. You can easily visualize the water extent using various thresholds by applying a classified symbology with two classes. It is often best to use dB scale datasets for identifying surface water. In many cases, there will be a bimodal distribution of values in an RTC image containing surface water, with the first peak comprised mostly of water values, and the second peak containing all the remaining values. A good first step is to select a break point between those two peaks, then adjust the value as needed to generate a good water mask (Figure 7). Figure 7: Setting the break point to fall between the two peaks of the histogram Once you have determined the appropriate threshold (Figure 8), you can reclassify the RTC image to include only those pixels that fall below the threshold value, providing a water mask that can be used for analysis or to overlay with other imagery to show the water extent. Figure 8: Water Mask. Contains modified Copernicus Sentinel data 2020, processed by ESA.","title":"Identifying Surface Water"},{"location":"guides/rtc_product_guide/#combination-of-rtc-image-with-other-remote-sensing-data","text":"One of the main advantages of using RTC imagery with its all weather and day/night capabilities is the combination with other remote sensing data such as optical data. In the example below, the backscatter information of the Sentinel-1 SAR image (Figure 9) is used to enhance the spectral information of the optical Landsat 8 image (Figure 10) in the urban area of Pavia, Italy. Figure 11 shows the image fusion result of an IHS transformation. In this transformation the color channels red, green and blue (RGB) are first converted into a different color representation: intensity, hue and saturation (IHS). In the second step the optical intensity is replaced by the SAR image, before IHS is transformed back to RGB. Figure 9: Sentinel-1 RTC image. Figure 10: False color composite (bands 5, 4, 3) of a Landsat 8 image The color values for the two rivers in the SAR image are far more similar to each other than in the optical image. The vegetated areas (highlighted in red) show up more uniformly in the data fusion result than in the optical false color composite image. Image fusion uses the complementary nature of the different sources to generate an enhanced product. Figure 11: Image fusion result of SAR and optical imagery","title":"Combination of RTC Image with other Remote Sensing Data"},{"location":"guides/rtc_product_guide/#arcgis-toolbox","text":"ASF has developed a custom ArcGIS Toolbox for working with RTC datasets in either ArcGIS Desktop or ArcGIS Pro. It includes tools for converting between different SAR scales, calculating the log difference between two images, generating RGB Decomposition (false-color) products, and reclassifying a raster to generate a water mask. For more information and to download the toolbox, visit our website: https://asf.alaska.edu/how-to/data-tools/gis-tools/ .","title":"ArcGIS Toolbox"},{"location":"guides/rtc_product_guide/#application-examples-in-the-literature","text":"The following journal articles represent some of the work being done using Radiometric Terrain Corrected Sentinel-1 data sets.","title":"Application Examples in the Literature"},{"location":"guides/rtc_product_guide/#crop-monitoring","text":"Clauss, K., Ottinger M. and Kuenzer, C. 2018. Mapping rice areas with Sentinel-1 time series and superpixel segmentation. International Journal of Remote Sensing , 39 (5):1399-1420. DOI: 10.1080/01431161.2017.1404162 Nguyen, D.B., Gruber A. and Wagner, W. 2016. Mapping rice extent and cropping scheme in the Mekong Delta using Sentinel-1A data. Remote Sensing Letters , 7 (12):1209-1218. DOI: 10.1080/2150704X.2016.1225172","title":"Crop Monitoring"},{"location":"guides/rtc_product_guide/#disaster-response","text":"Markert, K.N., Chishtie, F., Anderson, E.R., Saah, D., Griffin, R.E. 2018. On the merging of optical and SAR satellite imagery for surface water mapping applications. Results In Physics , 9 :275-277. DOI: 10.1016/j.rinp.2018.02.054 Twele, A., Cao, W., Plank, S. and Martinis, S. 2016. Sentinel-1-based flood mapping: a fully automated processing chain. International Journal of Remote Sensing , 37 (13):2990-3004. DOI: 10.1080/01431161.2016.1192304","title":"Disaster Response"},{"location":"guides/rtc_product_guide/#land-classification-and-change-detection","text":"Muro, J., Canty, M., Conradsen, K., H\u00fcttich, C., Nielsen, A.A., Skriver, H., Remy, F., Strauch, A., Thonfeld, F. and Menz, G. 2016. Short-Term change detection in wetlands using Sentinel-1 time series. Remote Sensing , 8 (10):795. DOI: 10.3390/rs8100795 R\u00fcetschi, M., Schaepman, M.E., Small, D. 2018. Using Multitemporal Sentinel-1 C-band backscatter to monitor phenology and classify deciduous and coniferous forests in Northern Switzerland. Remote Sensing , 10 (1):55. DOI: 10.3390/rs10010055","title":"Land Classification and Change Detection"},{"location":"guides/rtc_product_guide/#data-access","text":"To view or download Sentinel-1 RTC products, please see the links below: Vertex: https://search.asf.alaska.edu/ API: https://asf.alaska.edu/api/ For details on accessing data, including other SAR datasets, see ASF\u2019s Get Started guide: https://asf.alaska.edu/how-to/get-started/ To access data recipes, which are step-by-step tutorials for processing and working with SAR data, see ASF\u2019s tutorials page: https://asf.alaska.edu/how-to/data-recipes/data-recipe-tutorials/","title":"Data Access"},{"location":"tools/arcgis_toolbox/","text":"ArcGIS Toolbox \u00b6 The ASF_Tools ArcGIS Python Toolbox can be used with either ArcGIS Desktop or ArcGIS Pro, and contains tools that perform geoprocessing tasks useful for working with Synthetic Aperture Radar (SAR) data. The tools were designed to be used with Sentinel-1 Radiometric Terrain Corrected (RTC) SAR datasets , such as those available on-demand using ASF's Data Search - Vertex portal, but several of the tools have the potential to be used with a variety of rasters, including non-SAR datasets. The Toolbox is distributed as a zipped archive including the .pyt Toolbox script and associated .xml files. There is an XML file for the toolbox itself and one for each of the tools it contains. These XML files contain the metadata displayed in the item descriptions and tool help windows in ArcGIS, and must be kept in the same directory as the Python Toolbox (.pyt) file, or the information they contain will no longer be accessible. Toolbox Contents \u00b6 Unzip Files Tool \u00b6 This tool assists in file management when downloading .zip files from ASF. It could be used to extract to a specified location any zip files with an additional internal directory containing the individual files. The tool deletes the original zip files once they are extracted, and is especially helpful when dealing with file paths that are so long that they are beyond the maximum allowed in default Windows unzip utilities. Scale Conversion Tool \u00b6 This tool converts pixel values in calibrated SAR datasets (such as RTC rasters) from power or amplitude scale into power, amplitude or dB scale. This is an application specific to SAR data values/scales. Reclassify RTC Tool \u00b6 This tool generates a raster that includes only those pixels below a user-defined threshold value, and is designed for isolating water pixels. While intended for RTC files in dB scale, this tool could be used for any application where the user is interested in generating a spatial mask for values below a given threshold in a single-band raster. Log Difference Tool \u00b6 This tool compares two rasters by calculating the log difference on a pixel-by-pixel basis to identify areas where backscatter values have changed over time. While intended for RTC files in amplitude scale, this tool could be used to compare the pixel values of any two single-band rasters, as long as there are no negative values (NoData values will be returned for pixels with a negative number in either of the datasets). RGB Decomposition Tool \u00b6 This tool generates an RGB image using the co- and cross-polarized datasets from an RTC product. Input datasets can be in either amplitude or power scale, and the primary polarization can be either vertical (VV/VH) or horizontal (HH/HV). Additional documentation is available regarding the calculations used and the interpretation of these false-color images. Prerequisites \u00b6 Users must have either ArcGIS Desktop (ArcMap) or ArcGIS Pro installed and licensed on their computer. The Toolbox has been tested with Desktop versions 10.6.1 and 10.7.1 and Pro versions 2.4.2, 2.5.x and 2.6.1, but it may work with earlier versions as well. Note that several of the tools require the Spatial Analyst extension. Users who do not have licensing for this extension in ArcGIS will not be able to use many of the included tools. To install the Toolbox \u00b6 Download the zip file and extract the contents to any directory accessible by the computer running ArcGIS. Ensure that the Spatial Analyst extension is licensed and enabled. ArcGIS Desktop (ArcMap) \u00b6 Click on the Customize menu in ArcMap and select Extensions\u2026 Check the box next to Spatial Analyst and click the Close button at the bottom of the Extensions window. If you are unable to check this box, you do not have access to the Spatial Analyst extension and will not be able to make use of tools requiring this extension. ArcGIS Pro \u00b6 Click on the Project tab and select the Licensing tab. In the list of Esri Extensions, scroll down to verify that the Spatial Analyst is licensed and enabled. If it is not, an organization administrator will need to enable the extension in your user account. If your organization does not have a license available for you to use, you will not be able to make use of tools requiring this extension. Using the Toolbox \u00b6 In the ArcMap Catalog window or the ArcGIS Pro Catalog pane/view, navigate to the directory containing the toolbox (create a new folder connection if necessary). - To open the Catalog window in ArcMap, click on the Windows menu and select Catalog. - To open the Catalog pane or view in ArcGIS Pro, click the View tab and click on either the Catalog Pane or Catalog View button. Note that if you explore the extracted contents of the zip file outside of the ArcGIS environment, the directory will contain one .pyt file and a number of .xml files. In the ArcGIS Catalog window/pane/view, only the Toolbox is displayed, and when it is expanded, all of the Tools contained in the Toolbox script are displayed. The XML files are automatically referenced when ArcGIS requires the information they contain, and do not appear as additional files in the ArcGIS Catalog environment. The XML files must remain in the same directory as the .pyt file, and their filenames should not be changed. Double-click the ASF_Tools.pyt file to display the Tools (Scripts) included in the toolbox. Double-click on a Tool (displayed with a Script icon) to launch the dialog box or geoprocessing pane, as you would for any other ArcGIS Tool/Script. Enter the parameters as prompted and click the OK button to execute the tool. Note that output products are not automatically added to a project by default. You must navigate to them in the Catalog window/pane/view (or using the Add Data dialog) and add them to your project if desired. Tool Help \u00b6 The XML files included in the zip file are accessed when a user views the metadata for the toolbox, individual tools, or even different fields within the tool dialog. Accessing Help from within the Tool Dialog Box \u00b6 ArcGIS Desktop \u00b6 Click on the Show Help button at the bottom of the tool window to open the help panel. This panel will display information about the tool in general if no field is activated. If the user clicks on any of the parameter fields, information specific to that parameter will be displayed. Click on the Tool Help button at the bottom of the Help pane to open another window that displays most of the information that would be displayed in the tool\u2019s Item Description. ArcGIS Pro \u00b6 When you hover over any of the parameter fields in the tool dialog, a blue i appears. Hover over or click the blue i icon to view helpful tips specific to that parameter. Hover over the blue question mark at the top of the geoprocessing pane to display information about the tool. Click on it to open the full tool description in a browser window. Accessing Help from the Catalog Interface \u00b6 ArcGIS Desktop \u00b6 ArcCatalog displays the information contained in the xml metadata files in the Description tab for the toolbox and each tool. In the ArcMap Catalog window, the Item Description for the toolbox or any of its constituent tools displays the xml content. - Right-click the toolbox or tool in the Catalog window and select Item Description to view the information. ArcGIS Pro \u00b6 The xml metadata is displayed in the Metadata tab in the Catalog view. - Right-click a tool in the Catalog pane and select View Metadata to open the Metadata tab for the item in the Catalog view. OR - Open the Catalog View directly to navigate to the tool and select the Metadata tab.","title":"ArcGIS Toolbox"},{"location":"tools/arcgis_toolbox/#arcgis-toolbox","text":"The ASF_Tools ArcGIS Python Toolbox can be used with either ArcGIS Desktop or ArcGIS Pro, and contains tools that perform geoprocessing tasks useful for working with Synthetic Aperture Radar (SAR) data. The tools were designed to be used with Sentinel-1 Radiometric Terrain Corrected (RTC) SAR datasets , such as those available on-demand using ASF's Data Search - Vertex portal, but several of the tools have the potential to be used with a variety of rasters, including non-SAR datasets. The Toolbox is distributed as a zipped archive including the .pyt Toolbox script and associated .xml files. There is an XML file for the toolbox itself and one for each of the tools it contains. These XML files contain the metadata displayed in the item descriptions and tool help windows in ArcGIS, and must be kept in the same directory as the Python Toolbox (.pyt) file, or the information they contain will no longer be accessible.","title":"ArcGIS Toolbox"},{"location":"tools/arcgis_toolbox/#toolbox-contents","text":"","title":"Toolbox Contents"},{"location":"tools/arcgis_toolbox/#unzip-files-tool","text":"This tool assists in file management when downloading .zip files from ASF. It could be used to extract to a specified location any zip files with an additional internal directory containing the individual files. The tool deletes the original zip files once they are extracted, and is especially helpful when dealing with file paths that are so long that they are beyond the maximum allowed in default Windows unzip utilities.","title":"Unzip Files Tool"},{"location":"tools/arcgis_toolbox/#scale-conversion-tool","text":"This tool converts pixel values in calibrated SAR datasets (such as RTC rasters) from power or amplitude scale into power, amplitude or dB scale. This is an application specific to SAR data values/scales.","title":"Scale Conversion Tool"},{"location":"tools/arcgis_toolbox/#reclassify-rtc-tool","text":"This tool generates a raster that includes only those pixels below a user-defined threshold value, and is designed for isolating water pixels. While intended for RTC files in dB scale, this tool could be used for any application where the user is interested in generating a spatial mask for values below a given threshold in a single-band raster.","title":"Reclassify RTC Tool"},{"location":"tools/arcgis_toolbox/#log-difference-tool","text":"This tool compares two rasters by calculating the log difference on a pixel-by-pixel basis to identify areas where backscatter values have changed over time. While intended for RTC files in amplitude scale, this tool could be used to compare the pixel values of any two single-band rasters, as long as there are no negative values (NoData values will be returned for pixels with a negative number in either of the datasets).","title":"Log Difference Tool"},{"location":"tools/arcgis_toolbox/#rgb-decomposition-tool","text":"This tool generates an RGB image using the co- and cross-polarized datasets from an RTC product. Input datasets can be in either amplitude or power scale, and the primary polarization can be either vertical (VV/VH) or horizontal (HH/HV). Additional documentation is available regarding the calculations used and the interpretation of these false-color images.","title":"RGB Decomposition Tool"},{"location":"tools/arcgis_toolbox/#prerequisites","text":"Users must have either ArcGIS Desktop (ArcMap) or ArcGIS Pro installed and licensed on their computer. The Toolbox has been tested with Desktop versions 10.6.1 and 10.7.1 and Pro versions 2.4.2, 2.5.x and 2.6.1, but it may work with earlier versions as well. Note that several of the tools require the Spatial Analyst extension. Users who do not have licensing for this extension in ArcGIS will not be able to use many of the included tools.","title":"Prerequisites"},{"location":"tools/arcgis_toolbox/#to-install-the-toolbox","text":"Download the zip file and extract the contents to any directory accessible by the computer running ArcGIS. Ensure that the Spatial Analyst extension is licensed and enabled.","title":"To install the Toolbox"},{"location":"tools/arcgis_toolbox/#using-the-toolbox","text":"In the ArcMap Catalog window or the ArcGIS Pro Catalog pane/view, navigate to the directory containing the toolbox (create a new folder connection if necessary). - To open the Catalog window in ArcMap, click on the Windows menu and select Catalog. - To open the Catalog pane or view in ArcGIS Pro, click the View tab and click on either the Catalog Pane or Catalog View button. Note that if you explore the extracted contents of the zip file outside of the ArcGIS environment, the directory will contain one .pyt file and a number of .xml files. In the ArcGIS Catalog window/pane/view, only the Toolbox is displayed, and when it is expanded, all of the Tools contained in the Toolbox script are displayed. The XML files are automatically referenced when ArcGIS requires the information they contain, and do not appear as additional files in the ArcGIS Catalog environment. The XML files must remain in the same directory as the .pyt file, and their filenames should not be changed. Double-click the ASF_Tools.pyt file to display the Tools (Scripts) included in the toolbox. Double-click on a Tool (displayed with a Script icon) to launch the dialog box or geoprocessing pane, as you would for any other ArcGIS Tool/Script. Enter the parameters as prompted and click the OK button to execute the tool. Note that output products are not automatically added to a project by default. You must navigate to them in the Catalog window/pane/view (or using the Add Data dialog) and add them to your project if desired.","title":"Using the Toolbox"},{"location":"tools/arcgis_toolbox/#tool-help","text":"The XML files included in the zip file are accessed when a user views the metadata for the toolbox, individual tools, or even different fields within the tool dialog.","title":"Tool Help"},{"location":"tools/arcgis_toolbox/#accessing-help-from-within-the-tool-dialog-box","text":"","title":"Accessing Help from within the Tool Dialog Box"},{"location":"tools/arcgis_toolbox/#accessing-help-from-the-catalog-interface","text":"","title":"Accessing Help from the Catalog Interface"},{"location":"tools/asf_tools/","text":"ASF Tools for Python \u00b6 asf_tools is a Python package for working with Synthetic Aperture Radar (SAR) data. It was designed for working with datasets generated by HyP3 , but several of the tools have the potential to be used with a variety of rasters, including non-SAR datasets. Install \u00b6 In order to easily manage dependencies, we recommend using dedicated project environments via Anaconda/Miniconda . It is also possible to use Python virtual environments , but installation of non-python dependencies (e.g., gdal ) can be challenging. asf_tools can be installed into a conda environment with: conda install -c conda-forge asf_tools or into a virtual environment with: python -m pip install asf_tools Running as a Docker container \u00b6 We also publish a Docker image for asf_tools , with all the dependencies pre-installed, to the GitHub Container Registry: https://github.com/ASFHyP3/asf-tools/pkgs/container/asf-tools . You can pull an image with the latest released version of asf_tools with the command: docker pull ghcr.io/asfhyp3/asf-tools:latest Or, the development version with: docker pull ghcr.io/asfhyp3/asf-tools:test And then run the container with: docker run --rm -it ghcr.io/asfhyp3/asf-tools:latest which will drop you into a bash shell inside the container with an active asf-tools conda environment. To move data between your local (host) machine and the container, you can mount a volume with: docker run --rm -it -v /path/to/data:/home/conda/data ghcr.io/asfhyp3/asf-tools:latest Quick Usage \u00b6 Local Resolution Weighted Composite \u00b6 The make_composite tool allows you to create a local-resolution-weighted composite from a set of Sentinel-1 RTC products ( D. Small, 2012 ). It is intended to be used with RTC products generated by ASF HyP3 . You will need to request RTC products using the Include Scattering Area option, then download and unzip them into an empty directory. To generate a composite of the co-polarization images, navigate to the directory containing the unzipped RTC products and run: make_composite VV-composite */*VV.tif To generate a composite of the cross-polarization images, navigate to the directory containing the unzipped RTC products and run: make_composite VH-composite */*VH.tif Usage Tip \u00b6 Because the imagery has been radiometrically terrain corrected (RTC), geometric and radiometric distortions have been removed from the files to be composited. One the strong points of LRW composites is that you combine both ascending and descending datatakes into a single product. In this manner no layover or shadow masks are required - what is shadowed on an ascending pass is visible in a descending pass and vice-versa. Thus, not only is it possible to combine ascending and descending, but it is highly encouraged. Using many datatakes from both the ascending and descending satellite passes will make the best composites possible. About Local Resolution Weighting (LRW) \u00b6 In an LRW composite, each satellite pass contributes to creating the output pixels. The amount of this contribution is scaled by the inverse of the scattering area used during terrain correction (thus the need for requesting the area map option of HyP3 RTC). The inverse of the surface scattering area, also referred to as local resolution, is multiplied by each pixel's backscatter value. The results of all of the images covering any single pixel are then summed. This total is then divided by the sum of the weights used to get the output average backscatter. Water extent mapping \u00b6 The water_map tool allows you to create a surface water extent map from a Sentinel-1 dual-pol (VV+VH) RTC product. It is intended to be used with RTC products generated by ASF HyP3 . Additionally, a HAND (height above nearest drainage) GeoTIFF that is pixel aligned to the RTC images is required, and preferably derived from the same DEM used to correct the RTC images -- the quality of the HAND used is directly tied to the quality of the output water extent map. To make a water extent map, run: water_map [OUT_RASTER] [VV_RASTER] [VH_RASTER] [HAND_RASTER] For more information and to see the options available, see: water_map --help For details on the algorithm see the asf_tools.water_map.make_water_map docstring. Flood depth mapping \u00b6 Warning: The flood depth tool is still under active development and the products created using this tool are likely to change in the future. The flood_map tool allows you to create an estimated flood depth map from the surface water extent map created by the water_map tool. Additionally, a HAND (height above nearest drainage) GeoTIFF that is pixel aligned to the surface water extent map is required. An ideal candidate is the HAND image created by the water_map tool. To make a flood depth map, run: flood_map [OUT_RASTER] [SURFACE_WATER_MAP] [HAND_RASTER] For more information and to see the options available, see: flood_map --help For details on the algorithm see the asf_tools.flood_map.make_flood_map docstring.","title":"ASF Tools for Python"},{"location":"tools/asf_tools/#asf-tools-for-python","text":"asf_tools is a Python package for working with Synthetic Aperture Radar (SAR) data. It was designed for working with datasets generated by HyP3 , but several of the tools have the potential to be used with a variety of rasters, including non-SAR datasets.","title":"ASF Tools for Python"},{"location":"tools/asf_tools/#install","text":"In order to easily manage dependencies, we recommend using dedicated project environments via Anaconda/Miniconda . It is also possible to use Python virtual environments , but installation of non-python dependencies (e.g., gdal ) can be challenging. asf_tools can be installed into a conda environment with: conda install -c conda-forge asf_tools or into a virtual environment with: python -m pip install asf_tools","title":"Install"},{"location":"tools/asf_tools/#running-as-a-docker-container","text":"We also publish a Docker image for asf_tools , with all the dependencies pre-installed, to the GitHub Container Registry: https://github.com/ASFHyP3/asf-tools/pkgs/container/asf-tools . You can pull an image with the latest released version of asf_tools with the command: docker pull ghcr.io/asfhyp3/asf-tools:latest Or, the development version with: docker pull ghcr.io/asfhyp3/asf-tools:test And then run the container with: docker run --rm -it ghcr.io/asfhyp3/asf-tools:latest which will drop you into a bash shell inside the container with an active asf-tools conda environment. To move data between your local (host) machine and the container, you can mount a volume with: docker run --rm -it -v /path/to/data:/home/conda/data ghcr.io/asfhyp3/asf-tools:latest","title":"Running as a Docker container"},{"location":"tools/asf_tools/#quick-usage","text":"","title":"Quick Usage"},{"location":"tools/asf_tools/#local-resolution-weighted-composite","text":"The make_composite tool allows you to create a local-resolution-weighted composite from a set of Sentinel-1 RTC products ( D. Small, 2012 ). It is intended to be used with RTC products generated by ASF HyP3 . You will need to request RTC products using the Include Scattering Area option, then download and unzip them into an empty directory. To generate a composite of the co-polarization images, navigate to the directory containing the unzipped RTC products and run: make_composite VV-composite */*VV.tif To generate a composite of the cross-polarization images, navigate to the directory containing the unzipped RTC products and run: make_composite VH-composite */*VH.tif","title":"Local Resolution Weighted Composite"},{"location":"tools/asf_tools/#usage-tip","text":"Because the imagery has been radiometrically terrain corrected (RTC), geometric and radiometric distortions have been removed from the files to be composited. One the strong points of LRW composites is that you combine both ascending and descending datatakes into a single product. In this manner no layover or shadow masks are required - what is shadowed on an ascending pass is visible in a descending pass and vice-versa. Thus, not only is it possible to combine ascending and descending, but it is highly encouraged. Using many datatakes from both the ascending and descending satellite passes will make the best composites possible.","title":"Usage Tip"},{"location":"tools/asf_tools/#about-local-resolution-weighting-lrw","text":"In an LRW composite, each satellite pass contributes to creating the output pixels. The amount of this contribution is scaled by the inverse of the scattering area used during terrain correction (thus the need for requesting the area map option of HyP3 RTC). The inverse of the surface scattering area, also referred to as local resolution, is multiplied by each pixel's backscatter value. The results of all of the images covering any single pixel are then summed. This total is then divided by the sum of the weights used to get the output average backscatter.","title":"About Local Resolution Weighting (LRW)"},{"location":"tools/asf_tools/#water-extent-mapping","text":"The water_map tool allows you to create a surface water extent map from a Sentinel-1 dual-pol (VV+VH) RTC product. It is intended to be used with RTC products generated by ASF HyP3 . Additionally, a HAND (height above nearest drainage) GeoTIFF that is pixel aligned to the RTC images is required, and preferably derived from the same DEM used to correct the RTC images -- the quality of the HAND used is directly tied to the quality of the output water extent map. To make a water extent map, run: water_map [OUT_RASTER] [VV_RASTER] [VH_RASTER] [HAND_RASTER] For more information and to see the options available, see: water_map --help For details on the algorithm see the asf_tools.water_map.make_water_map docstring.","title":"Water extent mapping"},{"location":"tools/asf_tools/#flood-depth-mapping","text":"Warning: The flood depth tool is still under active development and the products created using this tool are likely to change in the future. The flood_map tool allows you to create an estimated flood depth map from the surface water extent map created by the water_map tool. Additionally, a HAND (height above nearest drainage) GeoTIFF that is pixel aligned to the surface water extent map is required. An ideal candidate is the HAND image created by the water_map tool. To make a flood depth map, run: flood_map [OUT_RASTER] [SURFACE_WATER_MAP] [HAND_RASTER] For more information and to see the options available, see: flood_map --help For details on the algorithm see the asf_tools.flood_map.make_flood_map docstring.","title":"Flood depth mapping"},{"location":"tools/asf_tools_api/","text":"asf_tools v0.4.0 API Reference \u00b6 Tools developed by ASF for working with SAR data","title":"API Reference"},{"location":"tools/asf_tools_api/#asf_tools-v040-api-reference","text":"Tools developed by ASF for working with SAR data","title":"asf_tools v0.4.0 API Reference"},{"location":"using/api/","text":"Using the HyP3 API \u00b6 The HyP3 API is built on OpenAPI and Swagger . A friendly interface for exploring the API is available at: https://hyp3-api.asf.alaska.edu/ui/ \u00b6 In order to use the API, you'll need a asf-urs session cookie, which you can get by signing in to Vertex Confirm you are authenticated \u00b6 To confirm you are authenticated, you can run a GET request to our /user endpoint. Select the blue GET button next to /user and click the Try it out button Then, execute the request and look at the response If you get a Code 200 you should see a JSON dictionary of your user information. Authentication Required If you get a 401 response back you need to sign in to Vertex to get the asf-urs session cookie. { \"detail\" : \"No authorization token provided\" , \"status\" : 401 , \"title\" : \"Unauthorized\" , \"type\" : \"about:blank\" } Submitting Sentinel-1 RTC jobs \u00b6 Jobs are submitted through the API by providing a JSON payload with a list of job definitions. Sentinel-1 jobs are submitted using ESA granule IDs . A minimal job list for a single Sentinel-1 RTC job would look like: { \"jobs\" : [ { \"name\" : \"minimal-rtc-example\" , \"job_type\" : \"RTC_GAMMA\" , \"job_parameters\" : { \"granules\" : [ \"S1A_IW_GRDH_1SDV_20210214T154837_20210214T154901_036588_044C54_032E\" ] } } ] } The job list may contain up to 200 job definitions. You can also provide custom RTC options: { \"jobs\" : [ { \"name\" : \"custom-rtc-example\" , \"job_type\" : \"RTC_GAMMA\" , \"job_parameters\" : { \"granules\" : [ \"S1B_IW_GRDH_1SDV_20210210T153157_20210210T153222_025546_030B48_2901\" ], \"radiometry\" : \"gamma0\" , \"scale\" : \"power\" , \"dem_matching\" : false , \"include_dem\" : true , \"include_inc_map\" : true , \"include_scattering_area\" : false , \"speckle_filter\" : false } }, { \"name\" : \"custom-rtc-example\" , \"job_type\" : \"RTC_GAMMA\" , \"job_parameters\" : { \"granules\" : [ \"S1B_IW_GRDH_1SDV_20210210T153132_20210210T153157_025546_030B48_4E31\" ], \"radiometry\" : \"sigma0\" , \"scale\" : \"amplitude\" , \"dem_matching\" : false , \"include_dem\" : false , \"include_inc_map\" : false , \"include_scattering_area\" : true , \"speckle_filter\" : true } } ] } Submitting Sentinel-1 InSAR jobs \u00b6 You can also submit InSAR jobs for scene pairs using ESA granule IDs . { \"jobs\" : [ { \"name\" : \"minimal-insar-example\" , \"job_type\" : \"INSAR_GAMMA\" , \"job_parameters\" : { \"granules\" : [ \"S1A_IW_SLC__1SDV_20200203T172103_20200203T172122_031091_03929B_3048\" , \"S1A_IW_SLC__1SDV_20200110T172104_20200110T172123_030741_03864E_A996\" ] } }, { \"name\" : \"custom-insar-example\" , \"job_type\" : \"INSAR_GAMMA\" , \"job_parameters\" : { \"granules\" : [ \"S1A_IW_SLC__1SDV_20200527T195012_20200527T195028_032755_03CB56_3D96\" , \"S1A_IW_SLC__1SDV_20200515T195012_20200515T195027_032580_03C609_4EBA\" ], \"looks\" : \"10x2\" , \"include_look_vectors\" : true , \"include_los_displacement\" : true } } ] } Submitting autoRIFT jobs \u00b6 AutoRIFT supports processing Sentinel-1, Sentinel-2, or Landsat-8 Collection 2 pairs. Sentinel-1 jobs are submitted using ESA granule IDs Sentinel-2 jobs can be submitted using ESA granule IDs or Element 84 Earth Search IDs Landsat-8 Collection 2 jobs are submitted using USGS scene IDs To submit an example set of jobs including all supported missions, you could write a job list like: { \"jobs\" : [ { \"name\" : \"autorift-example\" , \"job_type\" : \"AUTORIFT\" , \"job_parameters\" : { \"granules\" : [ \"S1A_IW_SLC__1SSH_20170221T204710_20170221T204737_015387_0193F6_AB07\" , \"S1B_IW_SLC__1SSH_20170227T204628_20170227T204655_004491_007D11_6654\" ] } }, { \"name\" : \"autorift-example\" , \"job_type\" : \"AUTORIFT\" , \"job_parameters\" : { \"granules\" : [ \"S2B_MSIL1C_20200612T150759_N0209_R025_T22WEB_20200612T184700\" , \"S2A_MSIL1C_20200627T150921_N0209_R025_T22WEB_20200627T170912\" ] } }, { \"name\" : \"autorift-example\" , \"job_type\" : \"AUTORIFT\" , \"job_parameters\" : { \"granules\" : [ \"S2B_22WEB_20200612_0_L1C\" , \"S2A_22WEB_20200627_0_L1C\" ] } } { \"name\" : \"autorift-example\" , \"job_type\" : \"AUTORIFT\" , \"job_parameters\" : { \"granules\" : [ \"LC08_L1TP_009011_20200703_20200913_02_T1\" , \"LC08_L1TP_009011_20200820_20200905_02_T1\" ] } } ] } With your JSON jobs definition, you can POST to the /jobs endpoint to submit the jobs. click the green POST button next to /jobs click Try it out on the right paste your jobs definition into the Request body click execute If your jobs were submitted successfully you should see a Code 200 and a JSON response of your job list, with some additional job attributes filled in. Querying jobs \u00b6 You can GET job information from the /jobs endpoint. You may provide query parameters to filter which jobs are returned: For our above examples, you can get the RTC job that was submitted with the default options by searching for name=minimal-rtc-example . If you provide no query parameters, you'll get a JSON response with a jobs list for every job you've submitted. Within the jobs list, a complete job dictionary will look like: { \"jobs\" : [ { \"name\" : \"minimal-rtc-example\" , \"job_type\" : \"RTC_GAMMA\" , \"job_parameters\" : { \"granules\" : [ \"S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8\" ] }, \"job_id\" : \"20c377be-2511-46a8-b908-e015abd3c24e\" , \"user_id\" : \"MY_EDL_USERNAME\" , \"status_code\" : \"SUCCEEDED\" , \"request_time\" : \"2021-02-24T21:30:45+00:00\" , \"expiration_time\" : \"2021-03-11T00:00:00+00:00\" , \"files\" : [ { \"filename\" : \"S1A_IW_20150621T120220_SVP_RTC30_G_gpuned_0AEA.zip\" , \"s3\" : { \"bucket\" : \"hyp3-contentbucket-fo259f6r6dn6\" , \"key\" : \"20c377be-2511-46a8-b908-e015abd3c24e/S1A_IW_20150621T120220_SVP_RTC30_G_gpuned_0AEA.zip\" }, \"size\" : 28676279 , \"url\" : \"https://hyp3-contentbucket-fo259f6r6dn6.s3.us-west-2.amazonaws.com/20c377be-2511-46a8-b908-e015abd3c24e/S1A_IW_20150621T120220_SVP_RTC30_G_gpuned_0AEA.zip\" } ], \"browse_images\" : [ \"https://hyp3-contentbucket-fo259f6r6dn6.s3.us-west-2.amazonaws.com/20c377be-2511-46a8-b908-e015abd3c24e/S1A_IW_20150621T120220_SVP_RTC30_G_gpuned_0AEA.png\" ], \"thumbnail_images\" : [ \"https://hyp3-contentbucket-fo259f6r6dn6.s3.us-west-2.amazonaws.com/20c377be-2511-46a8-b908-e015abd3c24e/S1A_IW_20150621T120220_SVP_RTC30_G_gpuned_0AEA_thumb.png\" ], \"logs\" : [ \"https://hyp3-contentbucket-fo259f6r6dn6.s3.us-west-2.amazonaws.com/20c377be-2511-46a8-b908-e015abd3c24e/20c377be-2511-46a8-b908-e015abd3c24e.log\" ] } ] } Importantly, the files block provides download links for the product files. For large queries results may be truncated. In this case there will be a next key in the response that will contain a url to continue the query (this response may be similarly truncated and include a next key). { \"jobs\" : [ ... ], \"next\" : \"https://hyp3-api.asf.alaska.edu/jobs?start_token=eyJqb2JfaWQiOiAiYzk1MDUzY2ItYWQzNy00ZGFhLTgxZDItYzA0YmQ4NWZiNDhiIiwgInVzZXJfaWQiOiAiamxyaW5lMiIsICJyZXF1ZXN0X3RpbWUiOiAiMjAyMC0xMC0yOVQxOTo0Mzo0NCswMDowMCJ9\" }","title":"API"},{"location":"using/api/#using-the-hyp3-api","text":"The HyP3 API is built on OpenAPI and Swagger . A friendly interface for exploring the API is available at:","title":"Using the HyP3 API"},{"location":"using/api/#httpshyp3-apiasfalaskaeduui","text":"In order to use the API, you'll need a asf-urs session cookie, which you can get by signing in to Vertex","title":"https://hyp3-api.asf.alaska.edu/ui/"},{"location":"using/api/#confirm-you-are-authenticated","text":"To confirm you are authenticated, you can run a GET request to our /user endpoint. Select the blue GET button next to /user and click the Try it out button Then, execute the request and look at the response If you get a Code 200 you should see a JSON dictionary of your user information. Authentication Required If you get a 401 response back you need to sign in to Vertex to get the asf-urs session cookie. { \"detail\" : \"No authorization token provided\" , \"status\" : 401 , \"title\" : \"Unauthorized\" , \"type\" : \"about:blank\" }","title":"Confirm you are authenticated"},{"location":"using/api/#submitting-sentinel-1-rtc-jobs","text":"Jobs are submitted through the API by providing a JSON payload with a list of job definitions. Sentinel-1 jobs are submitted using ESA granule IDs . A minimal job list for a single Sentinel-1 RTC job would look like: { \"jobs\" : [ { \"name\" : \"minimal-rtc-example\" , \"job_type\" : \"RTC_GAMMA\" , \"job_parameters\" : { \"granules\" : [ \"S1A_IW_GRDH_1SDV_20210214T154837_20210214T154901_036588_044C54_032E\" ] } } ] } The job list may contain up to 200 job definitions. You can also provide custom RTC options: { \"jobs\" : [ { \"name\" : \"custom-rtc-example\" , \"job_type\" : \"RTC_GAMMA\" , \"job_parameters\" : { \"granules\" : [ \"S1B_IW_GRDH_1SDV_20210210T153157_20210210T153222_025546_030B48_2901\" ], \"radiometry\" : \"gamma0\" , \"scale\" : \"power\" , \"dem_matching\" : false , \"include_dem\" : true , \"include_inc_map\" : true , \"include_scattering_area\" : false , \"speckle_filter\" : false } }, { \"name\" : \"custom-rtc-example\" , \"job_type\" : \"RTC_GAMMA\" , \"job_parameters\" : { \"granules\" : [ \"S1B_IW_GRDH_1SDV_20210210T153132_20210210T153157_025546_030B48_4E31\" ], \"radiometry\" : \"sigma0\" , \"scale\" : \"amplitude\" , \"dem_matching\" : false , \"include_dem\" : false , \"include_inc_map\" : false , \"include_scattering_area\" : true , \"speckle_filter\" : true } } ] }","title":"Submitting Sentinel-1 RTC jobs"},{"location":"using/api/#submitting-sentinel-1-insar-jobs","text":"You can also submit InSAR jobs for scene pairs using ESA granule IDs . { \"jobs\" : [ { \"name\" : \"minimal-insar-example\" , \"job_type\" : \"INSAR_GAMMA\" , \"job_parameters\" : { \"granules\" : [ \"S1A_IW_SLC__1SDV_20200203T172103_20200203T172122_031091_03929B_3048\" , \"S1A_IW_SLC__1SDV_20200110T172104_20200110T172123_030741_03864E_A996\" ] } }, { \"name\" : \"custom-insar-example\" , \"job_type\" : \"INSAR_GAMMA\" , \"job_parameters\" : { \"granules\" : [ \"S1A_IW_SLC__1SDV_20200527T195012_20200527T195028_032755_03CB56_3D96\" , \"S1A_IW_SLC__1SDV_20200515T195012_20200515T195027_032580_03C609_4EBA\" ], \"looks\" : \"10x2\" , \"include_look_vectors\" : true , \"include_los_displacement\" : true } } ] }","title":"Submitting Sentinel-1 InSAR jobs"},{"location":"using/api/#submitting-autorift-jobs","text":"AutoRIFT supports processing Sentinel-1, Sentinel-2, or Landsat-8 Collection 2 pairs. Sentinel-1 jobs are submitted using ESA granule IDs Sentinel-2 jobs can be submitted using ESA granule IDs or Element 84 Earth Search IDs Landsat-8 Collection 2 jobs are submitted using USGS scene IDs To submit an example set of jobs including all supported missions, you could write a job list like: { \"jobs\" : [ { \"name\" : \"autorift-example\" , \"job_type\" : \"AUTORIFT\" , \"job_parameters\" : { \"granules\" : [ \"S1A_IW_SLC__1SSH_20170221T204710_20170221T204737_015387_0193F6_AB07\" , \"S1B_IW_SLC__1SSH_20170227T204628_20170227T204655_004491_007D11_6654\" ] } }, { \"name\" : \"autorift-example\" , \"job_type\" : \"AUTORIFT\" , \"job_parameters\" : { \"granules\" : [ \"S2B_MSIL1C_20200612T150759_N0209_R025_T22WEB_20200612T184700\" , \"S2A_MSIL1C_20200627T150921_N0209_R025_T22WEB_20200627T170912\" ] } }, { \"name\" : \"autorift-example\" , \"job_type\" : \"AUTORIFT\" , \"job_parameters\" : { \"granules\" : [ \"S2B_22WEB_20200612_0_L1C\" , \"S2A_22WEB_20200627_0_L1C\" ] } } { \"name\" : \"autorift-example\" , \"job_type\" : \"AUTORIFT\" , \"job_parameters\" : { \"granules\" : [ \"LC08_L1TP_009011_20200703_20200913_02_T1\" , \"LC08_L1TP_009011_20200820_20200905_02_T1\" ] } } ] } With your JSON jobs definition, you can POST to the /jobs endpoint to submit the jobs. click the green POST button next to /jobs click Try it out on the right paste your jobs definition into the Request body click execute If your jobs were submitted successfully you should see a Code 200 and a JSON response of your job list, with some additional job attributes filled in.","title":"Submitting autoRIFT jobs"},{"location":"using/api/#querying-jobs","text":"You can GET job information from the /jobs endpoint. You may provide query parameters to filter which jobs are returned: For our above examples, you can get the RTC job that was submitted with the default options by searching for name=minimal-rtc-example . If you provide no query parameters, you'll get a JSON response with a jobs list for every job you've submitted. Within the jobs list, a complete job dictionary will look like: { \"jobs\" : [ { \"name\" : \"minimal-rtc-example\" , \"job_type\" : \"RTC_GAMMA\" , \"job_parameters\" : { \"granules\" : [ \"S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8\" ] }, \"job_id\" : \"20c377be-2511-46a8-b908-e015abd3c24e\" , \"user_id\" : \"MY_EDL_USERNAME\" , \"status_code\" : \"SUCCEEDED\" , \"request_time\" : \"2021-02-24T21:30:45+00:00\" , \"expiration_time\" : \"2021-03-11T00:00:00+00:00\" , \"files\" : [ { \"filename\" : \"S1A_IW_20150621T120220_SVP_RTC30_G_gpuned_0AEA.zip\" , \"s3\" : { \"bucket\" : \"hyp3-contentbucket-fo259f6r6dn6\" , \"key\" : \"20c377be-2511-46a8-b908-e015abd3c24e/S1A_IW_20150621T120220_SVP_RTC30_G_gpuned_0AEA.zip\" }, \"size\" : 28676279 , \"url\" : \"https://hyp3-contentbucket-fo259f6r6dn6.s3.us-west-2.amazonaws.com/20c377be-2511-46a8-b908-e015abd3c24e/S1A_IW_20150621T120220_SVP_RTC30_G_gpuned_0AEA.zip\" } ], \"browse_images\" : [ \"https://hyp3-contentbucket-fo259f6r6dn6.s3.us-west-2.amazonaws.com/20c377be-2511-46a8-b908-e015abd3c24e/S1A_IW_20150621T120220_SVP_RTC30_G_gpuned_0AEA.png\" ], \"thumbnail_images\" : [ \"https://hyp3-contentbucket-fo259f6r6dn6.s3.us-west-2.amazonaws.com/20c377be-2511-46a8-b908-e015abd3c24e/S1A_IW_20150621T120220_SVP_RTC30_G_gpuned_0AEA_thumb.png\" ], \"logs\" : [ \"https://hyp3-contentbucket-fo259f6r6dn6.s3.us-west-2.amazonaws.com/20c377be-2511-46a8-b908-e015abd3c24e/20c377be-2511-46a8-b908-e015abd3c24e.log\" ] } ] } Importantly, the files block provides download links for the product files. For large queries results may be truncated. In this case there will be a next key in the response that will contain a url to continue the query (this response may be similarly truncated and include a next key). { \"jobs\" : [ ... ], \"next\" : \"https://hyp3-api.asf.alaska.edu/jobs?start_token=eyJqb2JfaWQiOiAiYzk1MDUzY2ItYWQzNy00ZGFhLTgxZDItYzA0YmQ4NWZiNDhiIiwgInVzZXJfaWQiOiAiamxyaW5lMiIsICJyZXF1ZXN0X3RpbWUiOiAiMjAyMC0xMC0yOVQxOTo0Mzo0NCswMDowMCJ9\" }","title":"Querying jobs"},{"location":"using/quota/","text":"Monthly Processing Quota \u00b6 Attention Due to the increasing popularity of On Demand processing, and in order to continue providing this service at no cost to users, beginning February 2022, processing quotas will be set to 1000 jobs per user per month. If this change impacts your current workflows, or doesn't meet your needs, please let us know! In order to provide On Demand products across the community, and to support our mission of making remote-sensing data accessible , we've implemented a monthly processing quota. Anyone with an Earthdata Login can freely request 1000 jobs each calendar month. We may periodically adjust this quota based on usage, with the goal of providing valuable products to the widest breadth of users possible. If the quota doesn't meet your needs, please contact us and let us know how you would like to use the service. We have several options available for increased processing via HyP3. All requests will be balanced against our mission -- to make remote-sensing data accessible to the community. Contact Us \u00b6 Want to talk about HyP3? We would love to hear from you! Found a bug? Want to request a feature? open an issue General questions? Suggestions? Or just want to talk to the team? chat with us on gitter You can also reach us by email through ASF User Services: Email ASF User Services","title":"Processing Quota"},{"location":"using/quota/#monthly-processing-quota","text":"Attention Due to the increasing popularity of On Demand processing, and in order to continue providing this service at no cost to users, beginning February 2022, processing quotas will be set to 1000 jobs per user per month. If this change impacts your current workflows, or doesn't meet your needs, please let us know! In order to provide On Demand products across the community, and to support our mission of making remote-sensing data accessible , we've implemented a monthly processing quota. Anyone with an Earthdata Login can freely request 1000 jobs each calendar month. We may periodically adjust this quota based on usage, with the goal of providing valuable products to the widest breadth of users possible. If the quota doesn't meet your needs, please contact us and let us know how you would like to use the service. We have several options available for increased processing via HyP3. All requests will be balanced against our mission -- to make remote-sensing data accessible to the community.","title":"Monthly Processing Quota"},{"location":"using/quota/#contact-us","text":"Want to talk about HyP3? We would love to hear from you! Found a bug? Want to request a feature? open an issue General questions? Suggestions? Or just want to talk to the team? chat with us on gitter You can also reach us by email through ASF User Services: Email ASF User Services","title":"Contact Us"},{"location":"using/sdk/","text":"HyP3 SDK \u00b6 A python wrapper around the HyP3 API >>> from hyp3_sdk import HyP3 >>> hyp3 = HyP3 ( username = 'MyUsername' , password = 'MyPassword' ) >>> granule = 'S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8' >>> job = hyp3 . submit_rtc_job ( granule = granule , name = 'MyNewJob' ) >>> job = hyp3 . watch ( job ) >>> job . download_files () Install \u00b6 In order to easily manage dependencies, we recommend using dedicated project environments via Anaconda/Miniconda . or Python virtual environments . The HyP3 SDK can be installed into a conda environment with conda install -c conda-forge hyp3_sdk or into a virtual environment with python -m pip install hyp3_sdk Quick Usage \u00b6 There are 3 main classes that the SDK exposes: HyP3 to perform HyP3 operations (find jobs, refresh job information, submitting new jobs) Job to perform operations on single jobs (downloading products, check status) Batch to perform operations on multiple jobs at once (downloading products, check status) An instance of the HyP3 class will be needed to interact with the external HyP3 API. >>> from hyp3_sdk import HyP3 >>> hyp3 = HyP3 ( username = 'MyUsername' , password = 'MyPassword' ) >>> granule = 'S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8' >>> job = hyp3 . submit_rtc_job ( granule = granule , name = 'MyNewJob' ) >>> job = hyp3 . watch ( job ) >>> job . download_files () Submitting Jobs \u00b6 hyp3 has member functions for submitting new jobs: rtc_job = hyp3 . submit_rtc_job ( 'granule_id' , 'job_name' ) insar_job = hyp3 . submit_insar_job ( 'reference_granule_id' , 'secondary_granule_id' , 'job_name' ) autorift_job = hyp3 . submit_autorift_job ( 'reference_granule_id' , 'secondary_granule_id' , 'job_name' ) Each of these functions will return an instance of the Job class that represents a new HyP3 job request. Finding Existing Jobs \u00b6 To find HyP3 jobs that were run previously, you can use the hyp3.find_jobs() batch = hyp3 . find_jobs () This will return a Batch instance representing all jobs owned by you. You can also pass parameters to query to a specific set of jobs Operations on Job and Batch \u00b6 If your jobs are not complete you can use the HyP3 instance to update them, and wait from completion batch = hyp3 . find_jobs () if not batch . complete (): # to get updated information batch = hyp3 . refresh ( batch ) # or to wait until completion and get updated information (which will take a fair bit) batch = hyp3 . watch ( batch ) Once you have complete jobs you can download the products to your machine batch . download_files () These operations also work on Job objects job = hyp3 . submit_rtc_job ( 'S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8' , 'MyJobName' ) job = hyp3 . watch ( job ) job . download_files () Documentation \u00b6 For the full SDK API Reference, see the HyP3 documentation Contact Us \u00b6 Want to talk about the HyP3 SDK? We would love to hear from you! Found a bug? Want to request a feature? open an issue General questions? Suggestions? Or just want to talk to the team? chat with us on gitter","title":"SDK"},{"location":"using/sdk/#hyp3-sdk","text":"A python wrapper around the HyP3 API >>> from hyp3_sdk import HyP3 >>> hyp3 = HyP3 ( username = 'MyUsername' , password = 'MyPassword' ) >>> granule = 'S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8' >>> job = hyp3 . submit_rtc_job ( granule = granule , name = 'MyNewJob' ) >>> job = hyp3 . watch ( job ) >>> job . download_files ()","title":"HyP3 SDK"},{"location":"using/sdk/#install","text":"In order to easily manage dependencies, we recommend using dedicated project environments via Anaconda/Miniconda . or Python virtual environments . The HyP3 SDK can be installed into a conda environment with conda install -c conda-forge hyp3_sdk or into a virtual environment with python -m pip install hyp3_sdk","title":"Install"},{"location":"using/sdk/#quick-usage","text":"There are 3 main classes that the SDK exposes: HyP3 to perform HyP3 operations (find jobs, refresh job information, submitting new jobs) Job to perform operations on single jobs (downloading products, check status) Batch to perform operations on multiple jobs at once (downloading products, check status) An instance of the HyP3 class will be needed to interact with the external HyP3 API. >>> from hyp3_sdk import HyP3 >>> hyp3 = HyP3 ( username = 'MyUsername' , password = 'MyPassword' ) >>> granule = 'S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8' >>> job = hyp3 . submit_rtc_job ( granule = granule , name = 'MyNewJob' ) >>> job = hyp3 . watch ( job ) >>> job . download_files ()","title":"Quick Usage"},{"location":"using/sdk/#submitting-jobs","text":"hyp3 has member functions for submitting new jobs: rtc_job = hyp3 . submit_rtc_job ( 'granule_id' , 'job_name' ) insar_job = hyp3 . submit_insar_job ( 'reference_granule_id' , 'secondary_granule_id' , 'job_name' ) autorift_job = hyp3 . submit_autorift_job ( 'reference_granule_id' , 'secondary_granule_id' , 'job_name' ) Each of these functions will return an instance of the Job class that represents a new HyP3 job request.","title":"Submitting Jobs"},{"location":"using/sdk/#finding-existing-jobs","text":"To find HyP3 jobs that were run previously, you can use the hyp3.find_jobs() batch = hyp3 . find_jobs () This will return a Batch instance representing all jobs owned by you. You can also pass parameters to query to a specific set of jobs","title":"Finding Existing Jobs"},{"location":"using/sdk/#operations-on-job-and-batch","text":"If your jobs are not complete you can use the HyP3 instance to update them, and wait from completion batch = hyp3 . find_jobs () if not batch . complete (): # to get updated information batch = hyp3 . refresh ( batch ) # or to wait until completion and get updated information (which will take a fair bit) batch = hyp3 . watch ( batch ) Once you have complete jobs you can download the products to your machine batch . download_files () These operations also work on Job objects job = hyp3 . submit_rtc_job ( 'S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8' , 'MyJobName' ) job = hyp3 . watch ( job ) job . download_files ()","title":"Operations on Job and Batch"},{"location":"using/sdk/#documentation","text":"For the full SDK API Reference, see the HyP3 documentation","title":"Documentation"},{"location":"using/sdk/#contact-us","text":"Want to talk about the HyP3 SDK? We would love to hear from you! Found a bug? Want to request a feature? open an issue General questions? Suggestions? Or just want to talk to the team? chat with us on gitter","title":"Contact Us"},{"location":"using/sdk_api/","text":"hyp3_sdk v1.6.0 API Reference \u00b6 A python wrapper around the HyP3 API Batch \u00b6 Source code in hyp3_sdk/jobs.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 class Batch : def __init__ ( self , jobs : Optional [ List [ Job ]] = None ): if jobs is None : jobs = [] self . jobs = jobs def __add__ ( self , other : Union [ Job , 'Batch' ]): if isinstance ( other , Batch ): return Batch ( self . jobs + other . jobs ) elif isinstance ( other , Job ): return Batch ( self . jobs + [ other ]) else : raise TypeError ( f \"unsupported operand type(s) for +: ' { type ( self ) } ' and ' { type ( other ) } '\" ) def __iadd__ ( self , other : Union [ Job , 'Batch' ]): if isinstance ( other , Batch ): self . jobs += other . jobs elif isinstance ( other , Job ): self . jobs += [ other ] else : raise TypeError ( f \"unsupported operand type(s) for +=: ' { type ( self ) } ' and ' { type ( other ) } '\" ) return self def __iter__ ( self ): return iter ( self . jobs ) def __len__ ( self ): return len ( self . jobs ) def __contains__ ( self , job : Job ): return job in self . jobs def __eq__ ( self , other : 'Batch' ): return self . jobs == other . jobs def __delitem__ ( self , job : int ): self . jobs . pop ( job ) return self def __getitem__ ( self , index : int ): if isinstance ( index , slice ): return Batch ( self . jobs [ index ]) return self . jobs [ index ] def __setitem__ ( self , index : int , job : Job ): self . jobs [ index ] = job return self def __repr__ ( self ): reprs = \", \" . join ([ job . __repr__ () for job in self . jobs ]) return f 'Batch([ { reprs } ])' def __str__ ( self ): count = self . _count_statuses () return f ' { len ( self ) } HyP3 Jobs: ' \\ f ' { count [ \"SUCCEEDED\" ] } succeeded, ' \\ f ' { count [ \"FAILED\" ] } failed, ' \\ f ' { count [ \"RUNNING\" ] } running, ' \\ f ' { count [ \"PENDING\" ] } pending.' def _count_statuses ( self ): return Counter ([ job . status_code for job in self . jobs ]) def complete ( self ) -> bool : \"\"\" Returns: True if all jobs are complete, otherwise returns False \"\"\" for job in self . jobs : if not job . complete (): return False return True def succeeded ( self ) -> bool : \"\"\" Returns: True if all jobs have succeeded, otherwise returns False \"\"\" for job in self . jobs : if not job . succeeded (): return False return True def download_files ( self , location : Union [ Path , str ] = '.' , create : bool = True ) -> List [ Path ]: \"\"\" Args: location: Directory location to put files into create: Create `location` if it does not point to an existing directory Returns: list of Path objects to downloaded files \"\"\" downloaded_files = [] tqdm = get_tqdm_progress_bar () for job in tqdm ( self . jobs ): try : downloaded_files . extend ( job . download_files ( location , create )) except HyP3SDKError as e : print ( f 'Warning: { e } . Skipping download for { job } .' ) return downloaded_files def any_expired ( self ) -> bool : \"\"\"Check succeeded jobs for expiration\"\"\" for job in self . jobs : try : if job . expired (): return True except HyP3SDKError : continue return False def filter_jobs ( self , succeeded : bool = True , running : bool = True , failed : bool = False , include_expired : bool = True , ) -> 'Batch' : \"\"\"Filter jobs by status. By default, only succeeded and still running jobs will be in the returned batch. Args: succeeded: Include all succeeded jobs running: Include all running jobs failed: Include all failed jobs include_expired: Include expired jobs in the result Returns: batch: A batch object containing jobs matching all the selected statuses \"\"\" filtered_jobs = [] for job in self . jobs : if job . succeeded () and succeeded : if include_expired or not job . expired (): filtered_jobs . append ( job ) elif job . running () and running : filtered_jobs . append ( job ) elif job . failed () and failed : filtered_jobs . append ( job ) return Batch ( filtered_jobs ) any_expired () \u00b6 Check succeeded jobs for expiration Source code in hyp3_sdk/jobs.py 241 242 243 244 245 246 247 248 249 def any_expired ( self ) -> bool : \"\"\"Check succeeded jobs for expiration\"\"\" for job in self . jobs : try : if job . expired (): return True except HyP3SDKError : continue return False complete () \u00b6 Source code in hyp3_sdk/jobs.py 206 207 208 209 210 211 212 213 def complete ( self ) -> bool : \"\"\" Returns: True if all jobs are complete, otherwise returns False \"\"\" for job in self . jobs : if not job . complete (): return False return True download_files ( location = '.' , create = True ) \u00b6 Parameters: Name Type Description Default location Union [ Path , str ] Directory location to put files into '.' create bool Create location if it does not point to an existing directory True Source code in hyp3_sdk/jobs.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def download_files ( self , location : Union [ Path , str ] = '.' , create : bool = True ) -> List [ Path ]: \"\"\" Args: location: Directory location to put files into create: Create `location` if it does not point to an existing directory Returns: list of Path objects to downloaded files \"\"\" downloaded_files = [] tqdm = get_tqdm_progress_bar () for job in tqdm ( self . jobs ): try : downloaded_files . extend ( job . download_files ( location , create )) except HyP3SDKError as e : print ( f 'Warning: { e } . Skipping download for { job } .' ) return downloaded_files filter_jobs ( succeeded = True , running = True , failed = False , include_expired = True ) \u00b6 Filter jobs by status. By default, only succeeded and still running jobs will be in the returned batch. Parameters: Name Type Description Default succeeded bool Include all succeeded jobs True running bool Include all running jobs True failed bool Include all failed jobs False include_expired bool Include expired jobs in the result True Returns: Name Type Description batch 'Batch' A batch object containing jobs matching all the selected statuses Source code in hyp3_sdk/jobs.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def filter_jobs ( self , succeeded : bool = True , running : bool = True , failed : bool = False , include_expired : bool = True , ) -> 'Batch' : \"\"\"Filter jobs by status. By default, only succeeded and still running jobs will be in the returned batch. Args: succeeded: Include all succeeded jobs running: Include all running jobs failed: Include all failed jobs include_expired: Include expired jobs in the result Returns: batch: A batch object containing jobs matching all the selected statuses \"\"\" filtered_jobs = [] for job in self . jobs : if job . succeeded () and succeeded : if include_expired or not job . expired (): filtered_jobs . append ( job ) elif job . running () and running : filtered_jobs . append ( job ) elif job . failed () and failed : filtered_jobs . append ( job ) return Batch ( filtered_jobs ) succeeded () \u00b6 Source code in hyp3_sdk/jobs.py 215 216 217 218 219 220 221 222 def succeeded ( self ) -> bool : \"\"\" Returns: True if all jobs have succeeded, otherwise returns False \"\"\" for job in self . jobs : if not job . succeeded (): return False return True HyP3 \u00b6 A python wrapper around the HyP3 API Source code in hyp3_sdk/hyp3.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 class HyP3 : \"\"\"A python wrapper around the HyP3 API\"\"\" def __init__ ( self , api_url : str = PROD_API , username : Optional [ str ] = None , password : Optional [ str ] = None , prompt : bool = False ): \"\"\" Args: api_url: Address of the HyP3 API username: Username for authenticating to `urs.earthdata.nasa.gov`. Both username and password must be provided if either is provided. password: Password for authenticating to `urs.earthdata.nasa.gov`. Both username and password must be provided if either is provided. prompt: Prompt for username and/or password interactively when they are not provided as keyword parameters \"\"\" self . url = api_url if username is None and prompt : username = input ( 'NASA Earthdata Login username: ' ) if password is None and prompt : password = getpass ( 'NASA Earthdata Login password: ' ) self . session = get_authenticated_session ( username , password ) self . session . headers . update ({ 'User-Agent' : f ' { hyp3_sdk . __name__ } / { hyp3_sdk . __version__ } ' }) def find_jobs ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None , status_code : Optional [ str ] = None , name : Optional [ str ] = None , job_type : Optional [ str ] = None ) -> Batch : \"\"\"Gets a Batch of jobs from HyP3 matching the provided search criteria Args: start: only jobs submitted after given time end: only jobs submitted before given time status_code: only jobs matching this status (SUCCEEDED, FAILED, RUNNING, PENDING) name: only jobs with this name job_type: only jobs with this job_type Returns: A Batch object containing the found jobs \"\"\" params = {} for param_name in ( 'start' , 'end' , 'status_code' , 'name' , 'job_type' ): param_value = locals () . get ( param_name ) if param_value is not None : if isinstance ( param_value , datetime ): if param_value . tzinfo is None : param_value = param_value . replace ( tzinfo = timezone . utc ) param_value = param_value . isoformat ( timespec = 'seconds' ) params [ param_name ] = param_value response = self . session . get ( urljoin ( self . url , '/jobs' ), params = params ) _raise_for_hyp3_status ( response ) jobs = [ Job . from_dict ( job ) for job in response . json ()[ 'jobs' ]] while 'next' in response . json (): next_url = response . json ()[ 'next' ] response = self . session . get ( next_url ) _raise_for_hyp3_status ( response ) jobs . extend ([ Job . from_dict ( job ) for job in response . json ()[ 'jobs' ]]) return Batch ( jobs ) def get_job_by_id ( self , job_id : str ) -> Job : \"\"\"Get job by job ID Args: job_id: A job ID Returns: A Job object \"\"\" response = self . session . get ( urljoin ( self . url , f '/jobs/ { job_id } ' )) _raise_for_hyp3_status ( response ) return Job . from_dict ( response . json ()) @singledispatchmethod def watch ( self , job_or_batch : Union [ Batch , Job ], timeout : int = 10800 , interval : Union [ int , float ] = 60 ) -> Union [ Batch , Job ]: \"\"\"Watch jobs until they complete Args: job_or_batch: A Batch or Job object of jobs to watch timeout: How long to wait until exiting in seconds interval: How often to check for updates in seconds Returns: A Batch or Job object with refreshed watched jobs \"\"\" raise NotImplementedError ( f 'Cannot watch { type ( job_or_batch ) } type object' ) @watch . register def _watch_batch ( self , batch : Batch , timeout : int = 10800 , interval : Union [ int , float ] = 60 ) -> Batch : tqdm = get_tqdm_progress_bar () iterations_until_timeout = math . ceil ( timeout / interval ) bar_format = ' {l_bar}{bar} | {n_fmt} / {total_fmt} [ {postfix[0]} ]' with tqdm ( total = len ( batch ), bar_format = bar_format , postfix = [ f 'timeout in { timeout } s' ]) as progress_bar : for ii in range ( iterations_until_timeout ): batch = self . refresh ( batch ) counts = batch . _count_statuses () complete = counts [ 'SUCCEEDED' ] + counts [ 'FAILED' ] progress_bar . postfix = [ f 'timeout in { timeout - ii * interval } s' ] # to control n/total manually; update is n += value progress_bar . n = complete progress_bar . update ( 0 ) if batch . complete (): return batch time . sleep ( interval ) raise HyP3Error ( f 'Timeout occurred while waiting for { batch } ' ) @watch . register def _watch_job ( self , job : Job , timeout : int = 10800 , interval : Union [ int , float ] = 60 ) -> Job : tqdm = get_tqdm_progress_bar () iterations_until_timeout = math . ceil ( timeout / interval ) bar_format = ' {n_fmt} / {total_fmt} [ {postfix[0]} ]' with tqdm ( total = 1 , bar_format = bar_format , postfix = [ f 'timeout in { timeout } s' ]) as progress_bar : for ii in range ( iterations_until_timeout ): job = self . refresh ( job ) progress_bar . postfix = [ f 'timeout in { timeout - ii * interval } s' ] progress_bar . update ( int ( job . complete ())) if job . complete (): return job time . sleep ( interval ) raise HyP3Error ( f 'Timeout occurred while waiting for { job } ' ) @singledispatchmethod def refresh ( self , job_or_batch : Union [ Batch , Job ]) -> Union [ Batch , Job ]: \"\"\"Refresh each jobs' information Args: job_or_batch: A Batch of Job object to refresh Returns: A Batch or Job object with refreshed information \"\"\" raise NotImplementedError ( f 'Cannot refresh { type ( job_or_batch ) } type object' ) @refresh . register def _refresh_batch ( self , batch : Batch ): jobs = [] for job in batch . jobs : jobs . append ( self . refresh ( job )) return Batch ( jobs ) @refresh . register def _refresh_job ( self , job : Job ): return self . get_job_by_id ( job . job_id ) def submit_prepared_jobs ( self , prepared_jobs : Union [ dict , List [ dict ]]) -> Batch : \"\"\"Submit a prepared job dictionary, or list of prepared job dictionaries Args: prepared_jobs: A prepared job dictionary, or list of prepared job dictionaries Returns: A Batch object containing the submitted job(s) \"\"\" if isinstance ( prepared_jobs , dict ): payload = { 'jobs' : [ prepared_jobs ]} else : payload = { 'jobs' : prepared_jobs } response = self . session . post ( urljoin ( self . url , '/jobs' ), json = payload ) _raise_for_hyp3_status ( response ) batch = Batch () for job in response . json ()[ 'jobs' ]: batch += Job . from_dict ( job ) return batch def submit_autorift_job ( self , granule1 : str , granule2 : str , name : Optional [ str ] = None ) -> Batch : \"\"\"Submit an autoRIFT job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job Returns: A Batch object containing the autoRIFT job \"\"\" job_dict = self . prepare_autorift_job ( granule1 , granule2 , name = name ) return self . submit_prepared_jobs ( prepared_jobs = job_dict ) @classmethod def prepare_autorift_job ( cls , granule1 : str , granule2 : str , name : Optional [ str ] = None ) -> dict : \"\"\"Submit an autoRIFT job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job Returns: A dictionary containing the prepared autoRIFT job \"\"\" job_dict = { 'job_parameters' : { 'granules' : [ granule1 , granule2 ]}, 'job_type' : 'AUTORIFT' , } if name is not None : job_dict [ 'name' ] = name return job_dict def submit_rtc_job ( self , granule : str , name : Optional [ str ] = None , dem_matching : bool = False , include_dem : bool = False , include_inc_map : bool = False , include_rgb : bool = False , include_scattering_area : bool = False , radiometry : Literal [ 'sigma0' , 'gamma0' ] = 'gamma0' , resolution : Literal [ 30 ] = 30 , scale : Literal [ 'amplitude' , 'power' ] = 'power' , speckle_filter : bool = False , dem_name : Literal [ 'copernicus' , 'legacy' ] = 'copernicus' ) -> Batch : \"\"\"Submit an RTC job Args: granule: The granule (scene) to use name: A name for the job dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files include_dem: Include the DEM file in the product package include_inc_map: Include the local incidence angle map in the product package include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules) include_scattering_area: Include the scattering area in the product package radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0) resolution: Desired output pixel spacing in meters scale: Scale of output image; either power or amplitude speckle_filter: Apply an Enhanced Lee speckle filter dem_name: Name of the DEM to use for processing. `copernicus` will use the Copernicus GLO-30 Public DEM, while `legacy` will use the DEM with the best coverage from ASF's legacy SRTM/NED datasets. Returns: A Batch object containing the RTC job \"\"\" arguments = locals () arguments . pop ( 'self' ) job_dict = self . prepare_rtc_job ( ** arguments ) return self . submit_prepared_jobs ( prepared_jobs = job_dict ) @classmethod def prepare_rtc_job ( cls , granule : str , name : Optional [ str ] = None , dem_matching : bool = False , include_dem : bool = False , include_inc_map : bool = False , include_rgb : bool = False , include_scattering_area : bool = False , radiometry : Literal [ 'sigma0' , 'gamma0' ] = 'gamma0' , resolution : Literal [ 30 ] = 30 , scale : Literal [ 'amplitude' , 'power' ] = 'power' , speckle_filter : bool = False , dem_name : Literal [ 'copernicus' , 'legacy' ] = 'copernicus' ) -> dict : \"\"\"Submit an RTC job Args: granule: The granule (scene) to use name: A name for the job dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files include_dem: Include the DEM file in the product package include_inc_map: Include the local incidence angle map in the product package include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules) include_scattering_area: Include the scattering area in the product package radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0) resolution: Desired output pixel spacing in meters scale: Scale of output image; either power or amplitude speckle_filter: Apply an Enhanced Lee speckle filter dem_name: Name of the DEM to use for processing. `copernicus` will use the Copernicus GLO-30 Public DEM, while `legacy` will use the DEM with the best coverage from ASF's legacy SRTM/NED datasets. Returns: A dictionary containing the prepared RTC job \"\"\" job_parameters = locals () . copy () for key in [ 'granule' , 'name' , 'cls' ]: job_parameters . pop ( key , None ) job_dict = { 'job_parameters' : { 'granules' : [ granule ], ** job_parameters }, 'job_type' : 'RTC_GAMMA' , } if name is not None : job_dict [ 'name' ] = name return job_dict def submit_insar_job ( self , granule1 : str , granule2 : str , name : Optional [ str ] = None , include_look_vectors : bool = False , include_los_displacement : bool = False , include_inc_map : bool = False , looks : Literal [ '20x4' , '10x2' ] = '20x4' , include_dem : bool = False , include_wrapped_phase : bool = False , apply_water_mask : bool = False , include_displacement_maps : bool = False ) -> Batch : \"\"\"Submit an InSAR job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job include_look_vectors: Include the look vector theta and phi files in the product package include_los_displacement: Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of `include_displacement_maps`, and will be removed in a future release. include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package looks: Number of looks to take in range and azimuth include_dem: Include the digital elevation model GeoTIFF in the product package include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package apply_water_mask: Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package Returns: A Batch object containing the InSAR job \"\"\" arguments = locals () . copy () arguments . pop ( 'self' ) job_dict = self . prepare_insar_job ( ** arguments ) return self . submit_prepared_jobs ( prepared_jobs = job_dict ) @classmethod def prepare_insar_job ( cls , granule1 : str , granule2 : str , name : Optional [ str ] = None , include_look_vectors : bool = False , include_los_displacement : bool = False , include_inc_map : bool = False , looks : Literal [ '20x4' , '10x2' ] = '20x4' , include_dem : bool = False , include_wrapped_phase : bool = False , apply_water_mask : bool = False , include_displacement_maps : bool = False ) -> dict : \"\"\"Submit an InSAR job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job include_look_vectors: Include the look vector theta and phi files in the product package include_los_displacement: Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of `include_displacement_maps`, and will be removed in a future release. include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package looks: Number of looks to take in range and azimuth include_dem: Include the digital elevation model GeoTIFF in the product package include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package apply_water_mask: Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package Returns: A dictionary containing the prepared InSAR job \"\"\" if include_los_displacement : warnings . warn ( 'The include_los_displacement parameter has been deprecated in favor of ' 'include_displacement_maps, and will be removed in a future release.' , FutureWarning ) job_parameters = locals () . copy () for key in [ 'cls' , 'granule1' , 'granule2' , 'name' ]: job_parameters . pop ( key ) job_dict = { 'job_parameters' : { 'granules' : [ granule1 , granule2 ], ** job_parameters }, 'job_type' : 'INSAR_GAMMA' , } if name is not None : job_dict [ 'name' ] = name return job_dict def my_info ( self ) -> dict : \"\"\" Returns: Your user information \"\"\" response = self . session . get ( urljoin ( self . url , '/user' )) _raise_for_hyp3_status ( response ) return response . json () def check_quota ( self ) -> Optional [ int ]: \"\"\" Returns: The number of jobs left in your quota, or None if you have no quota \"\"\" info = self . my_info () return info [ 'quota' ][ 'remaining' ] __init__ ( api_url = PROD_API , username = None , password = None , prompt = False ) \u00b6 Parameters: Name Type Description Default api_url str Address of the HyP3 API PROD_API username Optional [ str ] Username for authenticating to urs.earthdata.nasa.gov . Both username and password must be provided if either is provided. None password Optional [ str ] Password for authenticating to urs.earthdata.nasa.gov . Both username and password must be provided if either is provided. None prompt bool Prompt for username and/or password interactively when they are not provided as keyword parameters False Source code in hyp3_sdk/hyp3.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , api_url : str = PROD_API , username : Optional [ str ] = None , password : Optional [ str ] = None , prompt : bool = False ): \"\"\" Args: api_url: Address of the HyP3 API username: Username for authenticating to `urs.earthdata.nasa.gov`. Both username and password must be provided if either is provided. password: Password for authenticating to `urs.earthdata.nasa.gov`. Both username and password must be provided if either is provided. prompt: Prompt for username and/or password interactively when they are not provided as keyword parameters \"\"\" self . url = api_url if username is None and prompt : username = input ( 'NASA Earthdata Login username: ' ) if password is None and prompt : password = getpass ( 'NASA Earthdata Login password: ' ) self . session = get_authenticated_session ( username , password ) self . session . headers . update ({ 'User-Agent' : f ' { hyp3_sdk . __name__ } / { hyp3_sdk . __version__ } ' }) check_quota () \u00b6 Returns: Type Description Optional [ int ] The number of jobs left in your quota, or None if you have no quota Source code in hyp3_sdk/hyp3.py 413 414 415 416 417 418 419 def check_quota ( self ) -> Optional [ int ]: \"\"\" Returns: The number of jobs left in your quota, or None if you have no quota \"\"\" info = self . my_info () return info [ 'quota' ][ 'remaining' ] find_jobs ( start = None , end = None , status_code = None , name = None , job_type = None ) \u00b6 Gets a Batch of jobs from HyP3 matching the provided search criteria Parameters: Name Type Description Default start Optional [ datetime ] only jobs submitted after given time None end Optional [ datetime ] only jobs submitted before given time None status_code Optional [ str ] only jobs matching this status (SUCCEEDED, FAILED, RUNNING, PENDING) None name Optional [ str ] only jobs with this name None job_type Optional [ str ] only jobs with this job_type None Returns: Type Description Batch A Batch object containing the found jobs Source code in hyp3_sdk/hyp3.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def find_jobs ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None , status_code : Optional [ str ] = None , name : Optional [ str ] = None , job_type : Optional [ str ] = None ) -> Batch : \"\"\"Gets a Batch of jobs from HyP3 matching the provided search criteria Args: start: only jobs submitted after given time end: only jobs submitted before given time status_code: only jobs matching this status (SUCCEEDED, FAILED, RUNNING, PENDING) name: only jobs with this name job_type: only jobs with this job_type Returns: A Batch object containing the found jobs \"\"\" params = {} for param_name in ( 'start' , 'end' , 'status_code' , 'name' , 'job_type' ): param_value = locals () . get ( param_name ) if param_value is not None : if isinstance ( param_value , datetime ): if param_value . tzinfo is None : param_value = param_value . replace ( tzinfo = timezone . utc ) param_value = param_value . isoformat ( timespec = 'seconds' ) params [ param_name ] = param_value response = self . session . get ( urljoin ( self . url , '/jobs' ), params = params ) _raise_for_hyp3_status ( response ) jobs = [ Job . from_dict ( job ) for job in response . json ()[ 'jobs' ]] while 'next' in response . json (): next_url = response . json ()[ 'next' ] response = self . session . get ( next_url ) _raise_for_hyp3_status ( response ) jobs . extend ([ Job . from_dict ( job ) for job in response . json ()[ 'jobs' ]]) return Batch ( jobs ) get_job_by_id ( job_id ) \u00b6 Get job by job ID Parameters: Name Type Description Default job_id str A job ID required Returns: Type Description Job A Job object Source code in hyp3_sdk/hyp3.py 82 83 84 85 86 87 88 89 90 91 92 93 94 def get_job_by_id ( self , job_id : str ) -> Job : \"\"\"Get job by job ID Args: job_id: A job ID Returns: A Job object \"\"\" response = self . session . get ( urljoin ( self . url , f '/jobs/ { job_id } ' )) _raise_for_hyp3_status ( response ) return Job . from_dict ( response . json ()) my_info () \u00b6 Returns: Type Description dict Your user information Source code in hyp3_sdk/hyp3.py 404 405 406 407 408 409 410 411 def my_info ( self ) -> dict : \"\"\" Returns: Your user information \"\"\" response = self . session . get ( urljoin ( self . url , '/user' )) _raise_for_hyp3_status ( response ) return response . json () prepare_autorift_job ( granule1 , granule2 , name = None ) classmethod \u00b6 Submit an autoRIFT job Parameters: Name Type Description Default granule1 str The first granule (scene) to use required granule2 str The second granule (scene) to use required name Optional [ str ] A name for the job None Returns: Type Description dict A dictionary containing the prepared autoRIFT job Source code in hyp3_sdk/hyp3.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 @classmethod def prepare_autorift_job ( cls , granule1 : str , granule2 : str , name : Optional [ str ] = None ) -> dict : \"\"\"Submit an autoRIFT job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job Returns: A dictionary containing the prepared autoRIFT job \"\"\" job_dict = { 'job_parameters' : { 'granules' : [ granule1 , granule2 ]}, 'job_type' : 'AUTORIFT' , } if name is not None : job_dict [ 'name' ] = name return job_dict prepare_insar_job ( granule1 , granule2 , name = None , include_look_vectors = False , include_los_displacement = False , include_inc_map = False , looks = '20x4' , include_dem = False , include_wrapped_phase = False , apply_water_mask = False , include_displacement_maps = False ) classmethod \u00b6 Submit an InSAR job Parameters: Name Type Description Default granule1 str The first granule (scene) to use required granule2 str The second granule (scene) to use required name Optional [ str ] A name for the job None include_look_vectors bool Include the look vector theta and phi files in the product package False include_los_displacement bool Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of include_displacement_maps , and will be removed in a future release. False include_inc_map bool Include the local and ellipsoidal incidence angle maps in the product package False looks Literal ['20x4', '10x2'] Number of looks to take in range and azimuth '20x4' include_dem bool Include the digital elevation model GeoTIFF in the product package False include_wrapped_phase bool Include the wrapped phase GeoTIFF in the product package False apply_water_mask bool Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping False include_displacement_maps bool Include displacement maps (line-of-sight and vertical) in the product package False Returns: Type Description dict A dictionary containing the prepared InSAR job Source code in hyp3_sdk/hyp3.py 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 @classmethod def prepare_insar_job ( cls , granule1 : str , granule2 : str , name : Optional [ str ] = None , include_look_vectors : bool = False , include_los_displacement : bool = False , include_inc_map : bool = False , looks : Literal [ '20x4' , '10x2' ] = '20x4' , include_dem : bool = False , include_wrapped_phase : bool = False , apply_water_mask : bool = False , include_displacement_maps : bool = False ) -> dict : \"\"\"Submit an InSAR job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job include_look_vectors: Include the look vector theta and phi files in the product package include_los_displacement: Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of `include_displacement_maps`, and will be removed in a future release. include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package looks: Number of looks to take in range and azimuth include_dem: Include the digital elevation model GeoTIFF in the product package include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package apply_water_mask: Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package Returns: A dictionary containing the prepared InSAR job \"\"\" if include_los_displacement : warnings . warn ( 'The include_los_displacement parameter has been deprecated in favor of ' 'include_displacement_maps, and will be removed in a future release.' , FutureWarning ) job_parameters = locals () . copy () for key in [ 'cls' , 'granule1' , 'granule2' , 'name' ]: job_parameters . pop ( key ) job_dict = { 'job_parameters' : { 'granules' : [ granule1 , granule2 ], ** job_parameters }, 'job_type' : 'INSAR_GAMMA' , } if name is not None : job_dict [ 'name' ] = name return job_dict prepare_rtc_job ( granule , name = None , dem_matching = False , include_dem = False , include_inc_map = False , include_rgb = False , include_scattering_area = False , radiometry = 'gamma0' , resolution = 30 , scale = 'power' , speckle_filter = False , dem_name = 'copernicus' ) classmethod \u00b6 Submit an RTC job Parameters: Name Type Description Default granule str The granule (scene) to use required name Optional [ str ] A name for the job None dem_matching bool Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files False include_dem bool Include the DEM file in the product package False include_inc_map bool Include the local incidence angle map in the product package False include_rgb bool Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules) False include_scattering_area bool Include the scattering area in the product package False radiometry Literal ['sigma0', 'gamma0'] Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0) 'gamma0' resolution Literal [30] Desired output pixel spacing in meters 30 scale Literal ['amplitude', 'power'] Scale of output image; either power or amplitude 'power' speckle_filter bool Apply an Enhanced Lee speckle filter False dem_name Literal ['copernicus', 'legacy'] Name of the DEM to use for processing. copernicus will use the Copernicus GLO-30 Public DEM, while legacy will use the DEM with the best coverage from ASF's legacy SRTM/NED datasets. 'copernicus' Returns: Type Description dict A dictionary containing the prepared RTC job Source code in hyp3_sdk/hyp3.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 @classmethod def prepare_rtc_job ( cls , granule : str , name : Optional [ str ] = None , dem_matching : bool = False , include_dem : bool = False , include_inc_map : bool = False , include_rgb : bool = False , include_scattering_area : bool = False , radiometry : Literal [ 'sigma0' , 'gamma0' ] = 'gamma0' , resolution : Literal [ 30 ] = 30 , scale : Literal [ 'amplitude' , 'power' ] = 'power' , speckle_filter : bool = False , dem_name : Literal [ 'copernicus' , 'legacy' ] = 'copernicus' ) -> dict : \"\"\"Submit an RTC job Args: granule: The granule (scene) to use name: A name for the job dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files include_dem: Include the DEM file in the product package include_inc_map: Include the local incidence angle map in the product package include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules) include_scattering_area: Include the scattering area in the product package radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0) resolution: Desired output pixel spacing in meters scale: Scale of output image; either power or amplitude speckle_filter: Apply an Enhanced Lee speckle filter dem_name: Name of the DEM to use for processing. `copernicus` will use the Copernicus GLO-30 Public DEM, while `legacy` will use the DEM with the best coverage from ASF's legacy SRTM/NED datasets. Returns: A dictionary containing the prepared RTC job \"\"\" job_parameters = locals () . copy () for key in [ 'granule' , 'name' , 'cls' ]: job_parameters . pop ( key , None ) job_dict = { 'job_parameters' : { 'granules' : [ granule ], ** job_parameters }, 'job_type' : 'RTC_GAMMA' , } if name is not None : job_dict [ 'name' ] = name return job_dict refresh ( job_or_batch ) \u00b6 Refresh each jobs' information Parameters: Name Type Description Default job_or_batch Union [ Batch , Job ] A Batch of Job object to refresh required Returns: Type Description Union [ Batch , Job ] A Batch or Job object with refreshed information Source code in hyp3_sdk/hyp3.py 149 150 151 152 153 154 155 156 157 158 159 @singledispatchmethod def refresh ( self , job_or_batch : Union [ Batch , Job ]) -> Union [ Batch , Job ]: \"\"\"Refresh each jobs' information Args: job_or_batch: A Batch of Job object to refresh Returns: A Batch or Job object with refreshed information \"\"\" raise NotImplementedError ( f 'Cannot refresh { type ( job_or_batch ) } type object' ) submit_autorift_job ( granule1 , granule2 , name = None ) \u00b6 Submit an autoRIFT job Parameters: Name Type Description Default granule1 str The first granule (scene) to use required granule2 str The second granule (scene) to use required name Optional [ str ] A name for the job None Returns: Type Description Batch A Batch object containing the autoRIFT job Source code in hyp3_sdk/hyp3.py 194 195 196 197 198 199 200 201 202 203 204 205 206 def submit_autorift_job ( self , granule1 : str , granule2 : str , name : Optional [ str ] = None ) -> Batch : \"\"\"Submit an autoRIFT job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job Returns: A Batch object containing the autoRIFT job \"\"\" job_dict = self . prepare_autorift_job ( granule1 , granule2 , name = name ) return self . submit_prepared_jobs ( prepared_jobs = job_dict ) submit_insar_job ( granule1 , granule2 , name = None , include_look_vectors = False , include_los_displacement = False , include_inc_map = False , looks = '20x4' , include_dem = False , include_wrapped_phase = False , apply_water_mask = False , include_displacement_maps = False ) \u00b6 Submit an InSAR job Parameters: Name Type Description Default granule1 str The first granule (scene) to use required granule2 str The second granule (scene) to use required name Optional [ str ] A name for the job None include_look_vectors bool Include the look vector theta and phi files in the product package False include_los_displacement bool Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of include_displacement_maps , and will be removed in a future release. False include_inc_map bool Include the local and ellipsoidal incidence angle maps in the product package False looks Literal ['20x4', '10x2'] Number of looks to take in range and azimuth '20x4' include_dem bool Include the digital elevation model GeoTIFF in the product package False include_wrapped_phase bool Include the wrapped phase GeoTIFF in the product package False apply_water_mask bool Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping False include_displacement_maps bool Include displacement maps (line-of-sight and vertical) in the product package False Returns: Type Description Batch A Batch object containing the InSAR job Source code in hyp3_sdk/hyp3.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 def submit_insar_job ( self , granule1 : str , granule2 : str , name : Optional [ str ] = None , include_look_vectors : bool = False , include_los_displacement : bool = False , include_inc_map : bool = False , looks : Literal [ '20x4' , '10x2' ] = '20x4' , include_dem : bool = False , include_wrapped_phase : bool = False , apply_water_mask : bool = False , include_displacement_maps : bool = False ) -> Batch : \"\"\"Submit an InSAR job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job include_look_vectors: Include the look vector theta and phi files in the product package include_los_displacement: Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of `include_displacement_maps`, and will be removed in a future release. include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package looks: Number of looks to take in range and azimuth include_dem: Include the digital elevation model GeoTIFF in the product package include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package apply_water_mask: Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package Returns: A Batch object containing the InSAR job \"\"\" arguments = locals () . copy () arguments . pop ( 'self' ) job_dict = self . prepare_insar_job ( ** arguments ) return self . submit_prepared_jobs ( prepared_jobs = job_dict ) submit_prepared_jobs ( prepared_jobs ) \u00b6 Submit a prepared job dictionary, or list of prepared job dictionaries Parameters: Name Type Description Default prepared_jobs Union [ dict , List [ dict ]] A prepared job dictionary, or list of prepared job dictionaries required Returns: Type Description Batch A Batch object containing the submitted job(s) Source code in hyp3_sdk/hyp3.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def submit_prepared_jobs ( self , prepared_jobs : Union [ dict , List [ dict ]]) -> Batch : \"\"\"Submit a prepared job dictionary, or list of prepared job dictionaries Args: prepared_jobs: A prepared job dictionary, or list of prepared job dictionaries Returns: A Batch object containing the submitted job(s) \"\"\" if isinstance ( prepared_jobs , dict ): payload = { 'jobs' : [ prepared_jobs ]} else : payload = { 'jobs' : prepared_jobs } response = self . session . post ( urljoin ( self . url , '/jobs' ), json = payload ) _raise_for_hyp3_status ( response ) batch = Batch () for job in response . json ()[ 'jobs' ]: batch += Job . from_dict ( job ) return batch submit_rtc_job ( granule , name = None , dem_matching = False , include_dem = False , include_inc_map = False , include_rgb = False , include_scattering_area = False , radiometry = 'gamma0' , resolution = 30 , scale = 'power' , speckle_filter = False , dem_name = 'copernicus' ) \u00b6 Submit an RTC job Parameters: Name Type Description Default granule str The granule (scene) to use required name Optional [ str ] A name for the job None dem_matching bool Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files False include_dem bool Include the DEM file in the product package False include_inc_map bool Include the local incidence angle map in the product package False include_rgb bool Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules) False include_scattering_area bool Include the scattering area in the product package False radiometry Literal ['sigma0', 'gamma0'] Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0) 'gamma0' resolution Literal [30] Desired output pixel spacing in meters 30 scale Literal ['amplitude', 'power'] Scale of output image; either power or amplitude 'power' speckle_filter bool Apply an Enhanced Lee speckle filter False dem_name Literal ['copernicus', 'legacy'] Name of the DEM to use for processing. copernicus will use the Copernicus GLO-30 Public DEM, while legacy will use the DEM with the best coverage from ASF's legacy SRTM/NED datasets. 'copernicus' Returns: Type Description Batch A Batch object containing the RTC job Source code in hyp3_sdk/hyp3.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def submit_rtc_job ( self , granule : str , name : Optional [ str ] = None , dem_matching : bool = False , include_dem : bool = False , include_inc_map : bool = False , include_rgb : bool = False , include_scattering_area : bool = False , radiometry : Literal [ 'sigma0' , 'gamma0' ] = 'gamma0' , resolution : Literal [ 30 ] = 30 , scale : Literal [ 'amplitude' , 'power' ] = 'power' , speckle_filter : bool = False , dem_name : Literal [ 'copernicus' , 'legacy' ] = 'copernicus' ) -> Batch : \"\"\"Submit an RTC job Args: granule: The granule (scene) to use name: A name for the job dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files include_dem: Include the DEM file in the product package include_inc_map: Include the local incidence angle map in the product package include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules) include_scattering_area: Include the scattering area in the product package radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0) resolution: Desired output pixel spacing in meters scale: Scale of output image; either power or amplitude speckle_filter: Apply an Enhanced Lee speckle filter dem_name: Name of the DEM to use for processing. `copernicus` will use the Copernicus GLO-30 Public DEM, while `legacy` will use the DEM with the best coverage from ASF's legacy SRTM/NED datasets. Returns: A Batch object containing the RTC job \"\"\" arguments = locals () arguments . pop ( 'self' ) job_dict = self . prepare_rtc_job ( ** arguments ) return self . submit_prepared_jobs ( prepared_jobs = job_dict ) watch ( job_or_batch , timeout = 10800 , interval = 60 ) \u00b6 Watch jobs until they complete Parameters: Name Type Description Default job_or_batch Union [ Batch , Job ] A Batch or Job object of jobs to watch required timeout int How long to wait until exiting in seconds 10800 interval Union [ int , float ] How often to check for updates in seconds 60 Returns: Type Description Union [ Batch , Job ] A Batch or Job object with refreshed watched jobs Source code in hyp3_sdk/hyp3.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 @singledispatchmethod def watch ( self , job_or_batch : Union [ Batch , Job ], timeout : int = 10800 , interval : Union [ int , float ] = 60 ) -> Union [ Batch , Job ]: \"\"\"Watch jobs until they complete Args: job_or_batch: A Batch or Job object of jobs to watch timeout: How long to wait until exiting in seconds interval: How often to check for updates in seconds Returns: A Batch or Job object with refreshed watched jobs \"\"\" raise NotImplementedError ( f 'Cannot watch { type ( job_or_batch ) } type object' ) Job \u00b6 Source code in hyp3_sdk/jobs.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class Job : _attributes_for_resubmit = { 'name' , 'job_parameters' , 'job_type' } def __init__ ( self , job_type : str , job_id : str , request_time : datetime , status_code : str , user_id : str , name : Optional [ str ] = None , job_parameters : Optional [ dict ] = None , files : Optional [ List ] = None , logs : Optional [ List ] = None , browse_images : Optional [ List ] = None , thumbnail_images : Optional [ List ] = None , expiration_time : Optional [ datetime ] = None , processing_time_in_seconds : Optional [ int ] = None , ): self . job_id = job_id self . job_type = job_type self . request_time = request_time self . status_code = status_code self . user_id = user_id self . name = name self . job_parameters = job_parameters self . files = files self . logs = logs self . browse_images = browse_images self . thumbnail_images = thumbnail_images self . expiration_time = expiration_time self . processing_time_in_seconds = processing_time_in_seconds def __repr__ ( self ): return f 'Job.from_dict( { self . to_dict () } )' def __str__ ( self ): return f 'HyP3 { self . job_type } job { self . job_id } ' def __eq__ ( self , other ): return self . __dict__ == other . __dict__ @staticmethod def from_dict ( input_dict : dict ): expiration_time = parse_date ( input_dict [ 'expiration_time' ]) if input_dict . get ( 'expiration_time' ) else None return Job ( job_type = input_dict [ 'job_type' ], job_id = input_dict [ 'job_id' ], request_time = parse_date ( input_dict [ 'request_time' ]), status_code = input_dict [ 'status_code' ], user_id = input_dict [ 'user_id' ], name = input_dict . get ( 'name' ), job_parameters = input_dict . get ( 'job_parameters' ), files = input_dict . get ( 'files' ), logs = input_dict . get ( 'logs' ), browse_images = input_dict . get ( 'browse_images' ), thumbnail_images = input_dict . get ( 'thumbnail_images' ), expiration_time = expiration_time , processing_time_in_seconds = input_dict . get ( 'processing_time_in_seconds' ), ) def to_dict ( self , for_resubmit : bool = False ): job_dict = {} if for_resubmit : keys_to_process = Job . _attributes_for_resubmit else : keys_to_process = vars ( self ) . keys () for key in keys_to_process : value = self . __getattribute__ ( key ) if value is not None : if isinstance ( value , datetime ): job_dict [ key ] = value . isoformat ( timespec = 'seconds' ) else : job_dict [ key ] = value return job_dict def succeeded ( self ) -> bool : return self . status_code == 'SUCCEEDED' def failed ( self ) -> bool : return self . status_code == 'FAILED' def complete ( self ) -> bool : return self . succeeded () or self . failed () # TODO may want to update this to check if status code is actually RUNNING, because currently this also returns # true if status is PENDING def running ( self ) -> bool : return not self . complete () def expired ( self ) -> bool : return self . expiration_time is not None and datetime . now ( tz . UTC ) >= self . expiration_time def download_files ( self , location : Union [ Path , str ] = '.' , create : bool = True ) -> List [ Path ]: \"\"\" Args: location: Directory location to put files into create: Create `location` if it does not point to an existing directory Returns: list of Path objects to downloaded files \"\"\" location = Path ( location ) if not self . succeeded (): raise HyP3SDKError ( f 'Only succeeded jobs can be downloaded; job is { self . status_code } .' ) if self . expired (): raise HyP3SDKError ( f 'Expired jobs cannot be downloaded; ' f 'job expired { self . expiration_time . isoformat ( timespec = \"seconds\" ) } .' ) if create : location . mkdir ( parents = True , exist_ok = True ) elif not location . is_dir (): raise NotADirectoryError ( str ( location )) downloaded_files = [] for file in self . files : download_url = file [ 'url' ] filename = location / file [ 'filename' ] try : downloaded_files . append ( download_file ( download_url , filename , chunk_size = 10485760 )) except HTTPError : raise HyP3SDKError ( f 'Unable to download file: { download_url } ' ) return downloaded_files download_files ( location = '.' , create = True ) \u00b6 Parameters: Name Type Description Default location Union [ Path , str ] Directory location to put files into '.' create bool Create location if it does not point to an existing directory True Source code in hyp3_sdk/jobs.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def download_files ( self , location : Union [ Path , str ] = '.' , create : bool = True ) -> List [ Path ]: \"\"\" Args: location: Directory location to put files into create: Create `location` if it does not point to an existing directory Returns: list of Path objects to downloaded files \"\"\" location = Path ( location ) if not self . succeeded (): raise HyP3SDKError ( f 'Only succeeded jobs can be downloaded; job is { self . status_code } .' ) if self . expired (): raise HyP3SDKError ( f 'Expired jobs cannot be downloaded; ' f 'job expired { self . expiration_time . isoformat ( timespec = \"seconds\" ) } .' ) if create : location . mkdir ( parents = True , exist_ok = True ) elif not location . is_dir (): raise NotADirectoryError ( str ( location )) downloaded_files = [] for file in self . files : download_url = file [ 'url' ] filename = location / file [ 'filename' ] try : downloaded_files . append ( download_file ( download_url , filename , chunk_size = 10485760 )) except HTTPError : raise HyP3SDKError ( f 'Unable to download file: { download_url } ' ) return downloaded_files","title":"API Reference"},{"location":"using/sdk_api/#hyp3_sdk-v160-api-reference","text":"A python wrapper around the HyP3 API","title":"hyp3_sdk v1.6.0 API Reference"},{"location":"using/sdk_api/#hyp3_sdk.Batch","text":"Source code in hyp3_sdk/jobs.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 class Batch : def __init__ ( self , jobs : Optional [ List [ Job ]] = None ): if jobs is None : jobs = [] self . jobs = jobs def __add__ ( self , other : Union [ Job , 'Batch' ]): if isinstance ( other , Batch ): return Batch ( self . jobs + other . jobs ) elif isinstance ( other , Job ): return Batch ( self . jobs + [ other ]) else : raise TypeError ( f \"unsupported operand type(s) for +: ' { type ( self ) } ' and ' { type ( other ) } '\" ) def __iadd__ ( self , other : Union [ Job , 'Batch' ]): if isinstance ( other , Batch ): self . jobs += other . jobs elif isinstance ( other , Job ): self . jobs += [ other ] else : raise TypeError ( f \"unsupported operand type(s) for +=: ' { type ( self ) } ' and ' { type ( other ) } '\" ) return self def __iter__ ( self ): return iter ( self . jobs ) def __len__ ( self ): return len ( self . jobs ) def __contains__ ( self , job : Job ): return job in self . jobs def __eq__ ( self , other : 'Batch' ): return self . jobs == other . jobs def __delitem__ ( self , job : int ): self . jobs . pop ( job ) return self def __getitem__ ( self , index : int ): if isinstance ( index , slice ): return Batch ( self . jobs [ index ]) return self . jobs [ index ] def __setitem__ ( self , index : int , job : Job ): self . jobs [ index ] = job return self def __repr__ ( self ): reprs = \", \" . join ([ job . __repr__ () for job in self . jobs ]) return f 'Batch([ { reprs } ])' def __str__ ( self ): count = self . _count_statuses () return f ' { len ( self ) } HyP3 Jobs: ' \\ f ' { count [ \"SUCCEEDED\" ] } succeeded, ' \\ f ' { count [ \"FAILED\" ] } failed, ' \\ f ' { count [ \"RUNNING\" ] } running, ' \\ f ' { count [ \"PENDING\" ] } pending.' def _count_statuses ( self ): return Counter ([ job . status_code for job in self . jobs ]) def complete ( self ) -> bool : \"\"\" Returns: True if all jobs are complete, otherwise returns False \"\"\" for job in self . jobs : if not job . complete (): return False return True def succeeded ( self ) -> bool : \"\"\" Returns: True if all jobs have succeeded, otherwise returns False \"\"\" for job in self . jobs : if not job . succeeded (): return False return True def download_files ( self , location : Union [ Path , str ] = '.' , create : bool = True ) -> List [ Path ]: \"\"\" Args: location: Directory location to put files into create: Create `location` if it does not point to an existing directory Returns: list of Path objects to downloaded files \"\"\" downloaded_files = [] tqdm = get_tqdm_progress_bar () for job in tqdm ( self . jobs ): try : downloaded_files . extend ( job . download_files ( location , create )) except HyP3SDKError as e : print ( f 'Warning: { e } . Skipping download for { job } .' ) return downloaded_files def any_expired ( self ) -> bool : \"\"\"Check succeeded jobs for expiration\"\"\" for job in self . jobs : try : if job . expired (): return True except HyP3SDKError : continue return False def filter_jobs ( self , succeeded : bool = True , running : bool = True , failed : bool = False , include_expired : bool = True , ) -> 'Batch' : \"\"\"Filter jobs by status. By default, only succeeded and still running jobs will be in the returned batch. Args: succeeded: Include all succeeded jobs running: Include all running jobs failed: Include all failed jobs include_expired: Include expired jobs in the result Returns: batch: A batch object containing jobs matching all the selected statuses \"\"\" filtered_jobs = [] for job in self . jobs : if job . succeeded () and succeeded : if include_expired or not job . expired (): filtered_jobs . append ( job ) elif job . running () and running : filtered_jobs . append ( job ) elif job . failed () and failed : filtered_jobs . append ( job ) return Batch ( filtered_jobs )","title":"Batch"},{"location":"using/sdk_api/#hyp3_sdk.jobs.Batch.any_expired","text":"Check succeeded jobs for expiration Source code in hyp3_sdk/jobs.py 241 242 243 244 245 246 247 248 249 def any_expired ( self ) -> bool : \"\"\"Check succeeded jobs for expiration\"\"\" for job in self . jobs : try : if job . expired (): return True except HyP3SDKError : continue return False","title":"any_expired()"},{"location":"using/sdk_api/#hyp3_sdk.jobs.Batch.complete","text":"Source code in hyp3_sdk/jobs.py 206 207 208 209 210 211 212 213 def complete ( self ) -> bool : \"\"\" Returns: True if all jobs are complete, otherwise returns False \"\"\" for job in self . jobs : if not job . complete (): return False return True","title":"complete()"},{"location":"using/sdk_api/#hyp3_sdk.jobs.Batch.download_files","text":"Parameters: Name Type Description Default location Union [ Path , str ] Directory location to put files into '.' create bool Create location if it does not point to an existing directory True Source code in hyp3_sdk/jobs.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def download_files ( self , location : Union [ Path , str ] = '.' , create : bool = True ) -> List [ Path ]: \"\"\" Args: location: Directory location to put files into create: Create `location` if it does not point to an existing directory Returns: list of Path objects to downloaded files \"\"\" downloaded_files = [] tqdm = get_tqdm_progress_bar () for job in tqdm ( self . jobs ): try : downloaded_files . extend ( job . download_files ( location , create )) except HyP3SDKError as e : print ( f 'Warning: { e } . Skipping download for { job } .' ) return downloaded_files","title":"download_files()"},{"location":"using/sdk_api/#hyp3_sdk.jobs.Batch.filter_jobs","text":"Filter jobs by status. By default, only succeeded and still running jobs will be in the returned batch. Parameters: Name Type Description Default succeeded bool Include all succeeded jobs True running bool Include all running jobs True failed bool Include all failed jobs False include_expired bool Include expired jobs in the result True Returns: Name Type Description batch 'Batch' A batch object containing jobs matching all the selected statuses Source code in hyp3_sdk/jobs.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def filter_jobs ( self , succeeded : bool = True , running : bool = True , failed : bool = False , include_expired : bool = True , ) -> 'Batch' : \"\"\"Filter jobs by status. By default, only succeeded and still running jobs will be in the returned batch. Args: succeeded: Include all succeeded jobs running: Include all running jobs failed: Include all failed jobs include_expired: Include expired jobs in the result Returns: batch: A batch object containing jobs matching all the selected statuses \"\"\" filtered_jobs = [] for job in self . jobs : if job . succeeded () and succeeded : if include_expired or not job . expired (): filtered_jobs . append ( job ) elif job . running () and running : filtered_jobs . append ( job ) elif job . failed () and failed : filtered_jobs . append ( job ) return Batch ( filtered_jobs )","title":"filter_jobs()"},{"location":"using/sdk_api/#hyp3_sdk.jobs.Batch.succeeded","text":"Source code in hyp3_sdk/jobs.py 215 216 217 218 219 220 221 222 def succeeded ( self ) -> bool : \"\"\" Returns: True if all jobs have succeeded, otherwise returns False \"\"\" for job in self . jobs : if not job . succeeded (): return False return True","title":"succeeded()"},{"location":"using/sdk_api/#hyp3_sdk.HyP3","text":"A python wrapper around the HyP3 API Source code in hyp3_sdk/hyp3.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 class HyP3 : \"\"\"A python wrapper around the HyP3 API\"\"\" def __init__ ( self , api_url : str = PROD_API , username : Optional [ str ] = None , password : Optional [ str ] = None , prompt : bool = False ): \"\"\" Args: api_url: Address of the HyP3 API username: Username for authenticating to `urs.earthdata.nasa.gov`. Both username and password must be provided if either is provided. password: Password for authenticating to `urs.earthdata.nasa.gov`. Both username and password must be provided if either is provided. prompt: Prompt for username and/or password interactively when they are not provided as keyword parameters \"\"\" self . url = api_url if username is None and prompt : username = input ( 'NASA Earthdata Login username: ' ) if password is None and prompt : password = getpass ( 'NASA Earthdata Login password: ' ) self . session = get_authenticated_session ( username , password ) self . session . headers . update ({ 'User-Agent' : f ' { hyp3_sdk . __name__ } / { hyp3_sdk . __version__ } ' }) def find_jobs ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None , status_code : Optional [ str ] = None , name : Optional [ str ] = None , job_type : Optional [ str ] = None ) -> Batch : \"\"\"Gets a Batch of jobs from HyP3 matching the provided search criteria Args: start: only jobs submitted after given time end: only jobs submitted before given time status_code: only jobs matching this status (SUCCEEDED, FAILED, RUNNING, PENDING) name: only jobs with this name job_type: only jobs with this job_type Returns: A Batch object containing the found jobs \"\"\" params = {} for param_name in ( 'start' , 'end' , 'status_code' , 'name' , 'job_type' ): param_value = locals () . get ( param_name ) if param_value is not None : if isinstance ( param_value , datetime ): if param_value . tzinfo is None : param_value = param_value . replace ( tzinfo = timezone . utc ) param_value = param_value . isoformat ( timespec = 'seconds' ) params [ param_name ] = param_value response = self . session . get ( urljoin ( self . url , '/jobs' ), params = params ) _raise_for_hyp3_status ( response ) jobs = [ Job . from_dict ( job ) for job in response . json ()[ 'jobs' ]] while 'next' in response . json (): next_url = response . json ()[ 'next' ] response = self . session . get ( next_url ) _raise_for_hyp3_status ( response ) jobs . extend ([ Job . from_dict ( job ) for job in response . json ()[ 'jobs' ]]) return Batch ( jobs ) def get_job_by_id ( self , job_id : str ) -> Job : \"\"\"Get job by job ID Args: job_id: A job ID Returns: A Job object \"\"\" response = self . session . get ( urljoin ( self . url , f '/jobs/ { job_id } ' )) _raise_for_hyp3_status ( response ) return Job . from_dict ( response . json ()) @singledispatchmethod def watch ( self , job_or_batch : Union [ Batch , Job ], timeout : int = 10800 , interval : Union [ int , float ] = 60 ) -> Union [ Batch , Job ]: \"\"\"Watch jobs until they complete Args: job_or_batch: A Batch or Job object of jobs to watch timeout: How long to wait until exiting in seconds interval: How often to check for updates in seconds Returns: A Batch or Job object with refreshed watched jobs \"\"\" raise NotImplementedError ( f 'Cannot watch { type ( job_or_batch ) } type object' ) @watch . register def _watch_batch ( self , batch : Batch , timeout : int = 10800 , interval : Union [ int , float ] = 60 ) -> Batch : tqdm = get_tqdm_progress_bar () iterations_until_timeout = math . ceil ( timeout / interval ) bar_format = ' {l_bar}{bar} | {n_fmt} / {total_fmt} [ {postfix[0]} ]' with tqdm ( total = len ( batch ), bar_format = bar_format , postfix = [ f 'timeout in { timeout } s' ]) as progress_bar : for ii in range ( iterations_until_timeout ): batch = self . refresh ( batch ) counts = batch . _count_statuses () complete = counts [ 'SUCCEEDED' ] + counts [ 'FAILED' ] progress_bar . postfix = [ f 'timeout in { timeout - ii * interval } s' ] # to control n/total manually; update is n += value progress_bar . n = complete progress_bar . update ( 0 ) if batch . complete (): return batch time . sleep ( interval ) raise HyP3Error ( f 'Timeout occurred while waiting for { batch } ' ) @watch . register def _watch_job ( self , job : Job , timeout : int = 10800 , interval : Union [ int , float ] = 60 ) -> Job : tqdm = get_tqdm_progress_bar () iterations_until_timeout = math . ceil ( timeout / interval ) bar_format = ' {n_fmt} / {total_fmt} [ {postfix[0]} ]' with tqdm ( total = 1 , bar_format = bar_format , postfix = [ f 'timeout in { timeout } s' ]) as progress_bar : for ii in range ( iterations_until_timeout ): job = self . refresh ( job ) progress_bar . postfix = [ f 'timeout in { timeout - ii * interval } s' ] progress_bar . update ( int ( job . complete ())) if job . complete (): return job time . sleep ( interval ) raise HyP3Error ( f 'Timeout occurred while waiting for { job } ' ) @singledispatchmethod def refresh ( self , job_or_batch : Union [ Batch , Job ]) -> Union [ Batch , Job ]: \"\"\"Refresh each jobs' information Args: job_or_batch: A Batch of Job object to refresh Returns: A Batch or Job object with refreshed information \"\"\" raise NotImplementedError ( f 'Cannot refresh { type ( job_or_batch ) } type object' ) @refresh . register def _refresh_batch ( self , batch : Batch ): jobs = [] for job in batch . jobs : jobs . append ( self . refresh ( job )) return Batch ( jobs ) @refresh . register def _refresh_job ( self , job : Job ): return self . get_job_by_id ( job . job_id ) def submit_prepared_jobs ( self , prepared_jobs : Union [ dict , List [ dict ]]) -> Batch : \"\"\"Submit a prepared job dictionary, or list of prepared job dictionaries Args: prepared_jobs: A prepared job dictionary, or list of prepared job dictionaries Returns: A Batch object containing the submitted job(s) \"\"\" if isinstance ( prepared_jobs , dict ): payload = { 'jobs' : [ prepared_jobs ]} else : payload = { 'jobs' : prepared_jobs } response = self . session . post ( urljoin ( self . url , '/jobs' ), json = payload ) _raise_for_hyp3_status ( response ) batch = Batch () for job in response . json ()[ 'jobs' ]: batch += Job . from_dict ( job ) return batch def submit_autorift_job ( self , granule1 : str , granule2 : str , name : Optional [ str ] = None ) -> Batch : \"\"\"Submit an autoRIFT job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job Returns: A Batch object containing the autoRIFT job \"\"\" job_dict = self . prepare_autorift_job ( granule1 , granule2 , name = name ) return self . submit_prepared_jobs ( prepared_jobs = job_dict ) @classmethod def prepare_autorift_job ( cls , granule1 : str , granule2 : str , name : Optional [ str ] = None ) -> dict : \"\"\"Submit an autoRIFT job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job Returns: A dictionary containing the prepared autoRIFT job \"\"\" job_dict = { 'job_parameters' : { 'granules' : [ granule1 , granule2 ]}, 'job_type' : 'AUTORIFT' , } if name is not None : job_dict [ 'name' ] = name return job_dict def submit_rtc_job ( self , granule : str , name : Optional [ str ] = None , dem_matching : bool = False , include_dem : bool = False , include_inc_map : bool = False , include_rgb : bool = False , include_scattering_area : bool = False , radiometry : Literal [ 'sigma0' , 'gamma0' ] = 'gamma0' , resolution : Literal [ 30 ] = 30 , scale : Literal [ 'amplitude' , 'power' ] = 'power' , speckle_filter : bool = False , dem_name : Literal [ 'copernicus' , 'legacy' ] = 'copernicus' ) -> Batch : \"\"\"Submit an RTC job Args: granule: The granule (scene) to use name: A name for the job dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files include_dem: Include the DEM file in the product package include_inc_map: Include the local incidence angle map in the product package include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules) include_scattering_area: Include the scattering area in the product package radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0) resolution: Desired output pixel spacing in meters scale: Scale of output image; either power or amplitude speckle_filter: Apply an Enhanced Lee speckle filter dem_name: Name of the DEM to use for processing. `copernicus` will use the Copernicus GLO-30 Public DEM, while `legacy` will use the DEM with the best coverage from ASF's legacy SRTM/NED datasets. Returns: A Batch object containing the RTC job \"\"\" arguments = locals () arguments . pop ( 'self' ) job_dict = self . prepare_rtc_job ( ** arguments ) return self . submit_prepared_jobs ( prepared_jobs = job_dict ) @classmethod def prepare_rtc_job ( cls , granule : str , name : Optional [ str ] = None , dem_matching : bool = False , include_dem : bool = False , include_inc_map : bool = False , include_rgb : bool = False , include_scattering_area : bool = False , radiometry : Literal [ 'sigma0' , 'gamma0' ] = 'gamma0' , resolution : Literal [ 30 ] = 30 , scale : Literal [ 'amplitude' , 'power' ] = 'power' , speckle_filter : bool = False , dem_name : Literal [ 'copernicus' , 'legacy' ] = 'copernicus' ) -> dict : \"\"\"Submit an RTC job Args: granule: The granule (scene) to use name: A name for the job dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files include_dem: Include the DEM file in the product package include_inc_map: Include the local incidence angle map in the product package include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules) include_scattering_area: Include the scattering area in the product package radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0) resolution: Desired output pixel spacing in meters scale: Scale of output image; either power or amplitude speckle_filter: Apply an Enhanced Lee speckle filter dem_name: Name of the DEM to use for processing. `copernicus` will use the Copernicus GLO-30 Public DEM, while `legacy` will use the DEM with the best coverage from ASF's legacy SRTM/NED datasets. Returns: A dictionary containing the prepared RTC job \"\"\" job_parameters = locals () . copy () for key in [ 'granule' , 'name' , 'cls' ]: job_parameters . pop ( key , None ) job_dict = { 'job_parameters' : { 'granules' : [ granule ], ** job_parameters }, 'job_type' : 'RTC_GAMMA' , } if name is not None : job_dict [ 'name' ] = name return job_dict def submit_insar_job ( self , granule1 : str , granule2 : str , name : Optional [ str ] = None , include_look_vectors : bool = False , include_los_displacement : bool = False , include_inc_map : bool = False , looks : Literal [ '20x4' , '10x2' ] = '20x4' , include_dem : bool = False , include_wrapped_phase : bool = False , apply_water_mask : bool = False , include_displacement_maps : bool = False ) -> Batch : \"\"\"Submit an InSAR job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job include_look_vectors: Include the look vector theta and phi files in the product package include_los_displacement: Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of `include_displacement_maps`, and will be removed in a future release. include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package looks: Number of looks to take in range and azimuth include_dem: Include the digital elevation model GeoTIFF in the product package include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package apply_water_mask: Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package Returns: A Batch object containing the InSAR job \"\"\" arguments = locals () . copy () arguments . pop ( 'self' ) job_dict = self . prepare_insar_job ( ** arguments ) return self . submit_prepared_jobs ( prepared_jobs = job_dict ) @classmethod def prepare_insar_job ( cls , granule1 : str , granule2 : str , name : Optional [ str ] = None , include_look_vectors : bool = False , include_los_displacement : bool = False , include_inc_map : bool = False , looks : Literal [ '20x4' , '10x2' ] = '20x4' , include_dem : bool = False , include_wrapped_phase : bool = False , apply_water_mask : bool = False , include_displacement_maps : bool = False ) -> dict : \"\"\"Submit an InSAR job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job include_look_vectors: Include the look vector theta and phi files in the product package include_los_displacement: Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of `include_displacement_maps`, and will be removed in a future release. include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package looks: Number of looks to take in range and azimuth include_dem: Include the digital elevation model GeoTIFF in the product package include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package apply_water_mask: Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package Returns: A dictionary containing the prepared InSAR job \"\"\" if include_los_displacement : warnings . warn ( 'The include_los_displacement parameter has been deprecated in favor of ' 'include_displacement_maps, and will be removed in a future release.' , FutureWarning ) job_parameters = locals () . copy () for key in [ 'cls' , 'granule1' , 'granule2' , 'name' ]: job_parameters . pop ( key ) job_dict = { 'job_parameters' : { 'granules' : [ granule1 , granule2 ], ** job_parameters }, 'job_type' : 'INSAR_GAMMA' , } if name is not None : job_dict [ 'name' ] = name return job_dict def my_info ( self ) -> dict : \"\"\" Returns: Your user information \"\"\" response = self . session . get ( urljoin ( self . url , '/user' )) _raise_for_hyp3_status ( response ) return response . json () def check_quota ( self ) -> Optional [ int ]: \"\"\" Returns: The number of jobs left in your quota, or None if you have no quota \"\"\" info = self . my_info () return info [ 'quota' ][ 'remaining' ]","title":"HyP3"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.__init__","text":"Parameters: Name Type Description Default api_url str Address of the HyP3 API PROD_API username Optional [ str ] Username for authenticating to urs.earthdata.nasa.gov . Both username and password must be provided if either is provided. None password Optional [ str ] Password for authenticating to urs.earthdata.nasa.gov . Both username and password must be provided if either is provided. None prompt bool Prompt for username and/or password interactively when they are not provided as keyword parameters False Source code in hyp3_sdk/hyp3.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , api_url : str = PROD_API , username : Optional [ str ] = None , password : Optional [ str ] = None , prompt : bool = False ): \"\"\" Args: api_url: Address of the HyP3 API username: Username for authenticating to `urs.earthdata.nasa.gov`. Both username and password must be provided if either is provided. password: Password for authenticating to `urs.earthdata.nasa.gov`. Both username and password must be provided if either is provided. prompt: Prompt for username and/or password interactively when they are not provided as keyword parameters \"\"\" self . url = api_url if username is None and prompt : username = input ( 'NASA Earthdata Login username: ' ) if password is None and prompt : password = getpass ( 'NASA Earthdata Login password: ' ) self . session = get_authenticated_session ( username , password ) self . session . headers . update ({ 'User-Agent' : f ' { hyp3_sdk . __name__ } / { hyp3_sdk . __version__ } ' })","title":"__init__()"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.check_quota","text":"Returns: Type Description Optional [ int ] The number of jobs left in your quota, or None if you have no quota Source code in hyp3_sdk/hyp3.py 413 414 415 416 417 418 419 def check_quota ( self ) -> Optional [ int ]: \"\"\" Returns: The number of jobs left in your quota, or None if you have no quota \"\"\" info = self . my_info () return info [ 'quota' ][ 'remaining' ]","title":"check_quota()"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.find_jobs","text":"Gets a Batch of jobs from HyP3 matching the provided search criteria Parameters: Name Type Description Default start Optional [ datetime ] only jobs submitted after given time None end Optional [ datetime ] only jobs submitted before given time None status_code Optional [ str ] only jobs matching this status (SUCCEEDED, FAILED, RUNNING, PENDING) None name Optional [ str ] only jobs with this name None job_type Optional [ str ] only jobs with this job_type None Returns: Type Description Batch A Batch object containing the found jobs Source code in hyp3_sdk/hyp3.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def find_jobs ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None , status_code : Optional [ str ] = None , name : Optional [ str ] = None , job_type : Optional [ str ] = None ) -> Batch : \"\"\"Gets a Batch of jobs from HyP3 matching the provided search criteria Args: start: only jobs submitted after given time end: only jobs submitted before given time status_code: only jobs matching this status (SUCCEEDED, FAILED, RUNNING, PENDING) name: only jobs with this name job_type: only jobs with this job_type Returns: A Batch object containing the found jobs \"\"\" params = {} for param_name in ( 'start' , 'end' , 'status_code' , 'name' , 'job_type' ): param_value = locals () . get ( param_name ) if param_value is not None : if isinstance ( param_value , datetime ): if param_value . tzinfo is None : param_value = param_value . replace ( tzinfo = timezone . utc ) param_value = param_value . isoformat ( timespec = 'seconds' ) params [ param_name ] = param_value response = self . session . get ( urljoin ( self . url , '/jobs' ), params = params ) _raise_for_hyp3_status ( response ) jobs = [ Job . from_dict ( job ) for job in response . json ()[ 'jobs' ]] while 'next' in response . json (): next_url = response . json ()[ 'next' ] response = self . session . get ( next_url ) _raise_for_hyp3_status ( response ) jobs . extend ([ Job . from_dict ( job ) for job in response . json ()[ 'jobs' ]]) return Batch ( jobs )","title":"find_jobs()"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.get_job_by_id","text":"Get job by job ID Parameters: Name Type Description Default job_id str A job ID required Returns: Type Description Job A Job object Source code in hyp3_sdk/hyp3.py 82 83 84 85 86 87 88 89 90 91 92 93 94 def get_job_by_id ( self , job_id : str ) -> Job : \"\"\"Get job by job ID Args: job_id: A job ID Returns: A Job object \"\"\" response = self . session . get ( urljoin ( self . url , f '/jobs/ { job_id } ' )) _raise_for_hyp3_status ( response ) return Job . from_dict ( response . json ())","title":"get_job_by_id()"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.my_info","text":"Returns: Type Description dict Your user information Source code in hyp3_sdk/hyp3.py 404 405 406 407 408 409 410 411 def my_info ( self ) -> dict : \"\"\" Returns: Your user information \"\"\" response = self . session . get ( urljoin ( self . url , '/user' )) _raise_for_hyp3_status ( response ) return response . json ()","title":"my_info()"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.prepare_autorift_job","text":"Submit an autoRIFT job Parameters: Name Type Description Default granule1 str The first granule (scene) to use required granule2 str The second granule (scene) to use required name Optional [ str ] A name for the job None Returns: Type Description dict A dictionary containing the prepared autoRIFT job Source code in hyp3_sdk/hyp3.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 @classmethod def prepare_autorift_job ( cls , granule1 : str , granule2 : str , name : Optional [ str ] = None ) -> dict : \"\"\"Submit an autoRIFT job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job Returns: A dictionary containing the prepared autoRIFT job \"\"\" job_dict = { 'job_parameters' : { 'granules' : [ granule1 , granule2 ]}, 'job_type' : 'AUTORIFT' , } if name is not None : job_dict [ 'name' ] = name return job_dict","title":"prepare_autorift_job()"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.prepare_insar_job","text":"Submit an InSAR job Parameters: Name Type Description Default granule1 str The first granule (scene) to use required granule2 str The second granule (scene) to use required name Optional [ str ] A name for the job None include_look_vectors bool Include the look vector theta and phi files in the product package False include_los_displacement bool Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of include_displacement_maps , and will be removed in a future release. False include_inc_map bool Include the local and ellipsoidal incidence angle maps in the product package False looks Literal ['20x4', '10x2'] Number of looks to take in range and azimuth '20x4' include_dem bool Include the digital elevation model GeoTIFF in the product package False include_wrapped_phase bool Include the wrapped phase GeoTIFF in the product package False apply_water_mask bool Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping False include_displacement_maps bool Include displacement maps (line-of-sight and vertical) in the product package False Returns: Type Description dict A dictionary containing the prepared InSAR job Source code in hyp3_sdk/hyp3.py 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 @classmethod def prepare_insar_job ( cls , granule1 : str , granule2 : str , name : Optional [ str ] = None , include_look_vectors : bool = False , include_los_displacement : bool = False , include_inc_map : bool = False , looks : Literal [ '20x4' , '10x2' ] = '20x4' , include_dem : bool = False , include_wrapped_phase : bool = False , apply_water_mask : bool = False , include_displacement_maps : bool = False ) -> dict : \"\"\"Submit an InSAR job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job include_look_vectors: Include the look vector theta and phi files in the product package include_los_displacement: Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of `include_displacement_maps`, and will be removed in a future release. include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package looks: Number of looks to take in range and azimuth include_dem: Include the digital elevation model GeoTIFF in the product package include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package apply_water_mask: Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package Returns: A dictionary containing the prepared InSAR job \"\"\" if include_los_displacement : warnings . warn ( 'The include_los_displacement parameter has been deprecated in favor of ' 'include_displacement_maps, and will be removed in a future release.' , FutureWarning ) job_parameters = locals () . copy () for key in [ 'cls' , 'granule1' , 'granule2' , 'name' ]: job_parameters . pop ( key ) job_dict = { 'job_parameters' : { 'granules' : [ granule1 , granule2 ], ** job_parameters }, 'job_type' : 'INSAR_GAMMA' , } if name is not None : job_dict [ 'name' ] = name return job_dict","title":"prepare_insar_job()"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.prepare_rtc_job","text":"Submit an RTC job Parameters: Name Type Description Default granule str The granule (scene) to use required name Optional [ str ] A name for the job None dem_matching bool Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files False include_dem bool Include the DEM file in the product package False include_inc_map bool Include the local incidence angle map in the product package False include_rgb bool Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules) False include_scattering_area bool Include the scattering area in the product package False radiometry Literal ['sigma0', 'gamma0'] Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0) 'gamma0' resolution Literal [30] Desired output pixel spacing in meters 30 scale Literal ['amplitude', 'power'] Scale of output image; either power or amplitude 'power' speckle_filter bool Apply an Enhanced Lee speckle filter False dem_name Literal ['copernicus', 'legacy'] Name of the DEM to use for processing. copernicus will use the Copernicus GLO-30 Public DEM, while legacy will use the DEM with the best coverage from ASF's legacy SRTM/NED datasets. 'copernicus' Returns: Type Description dict A dictionary containing the prepared RTC job Source code in hyp3_sdk/hyp3.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 @classmethod def prepare_rtc_job ( cls , granule : str , name : Optional [ str ] = None , dem_matching : bool = False , include_dem : bool = False , include_inc_map : bool = False , include_rgb : bool = False , include_scattering_area : bool = False , radiometry : Literal [ 'sigma0' , 'gamma0' ] = 'gamma0' , resolution : Literal [ 30 ] = 30 , scale : Literal [ 'amplitude' , 'power' ] = 'power' , speckle_filter : bool = False , dem_name : Literal [ 'copernicus' , 'legacy' ] = 'copernicus' ) -> dict : \"\"\"Submit an RTC job Args: granule: The granule (scene) to use name: A name for the job dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files include_dem: Include the DEM file in the product package include_inc_map: Include the local incidence angle map in the product package include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules) include_scattering_area: Include the scattering area in the product package radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0) resolution: Desired output pixel spacing in meters scale: Scale of output image; either power or amplitude speckle_filter: Apply an Enhanced Lee speckle filter dem_name: Name of the DEM to use for processing. `copernicus` will use the Copernicus GLO-30 Public DEM, while `legacy` will use the DEM with the best coverage from ASF's legacy SRTM/NED datasets. Returns: A dictionary containing the prepared RTC job \"\"\" job_parameters = locals () . copy () for key in [ 'granule' , 'name' , 'cls' ]: job_parameters . pop ( key , None ) job_dict = { 'job_parameters' : { 'granules' : [ granule ], ** job_parameters }, 'job_type' : 'RTC_GAMMA' , } if name is not None : job_dict [ 'name' ] = name return job_dict","title":"prepare_rtc_job()"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.refresh","text":"Refresh each jobs' information Parameters: Name Type Description Default job_or_batch Union [ Batch , Job ] A Batch of Job object to refresh required Returns: Type Description Union [ Batch , Job ] A Batch or Job object with refreshed information Source code in hyp3_sdk/hyp3.py 149 150 151 152 153 154 155 156 157 158 159 @singledispatchmethod def refresh ( self , job_or_batch : Union [ Batch , Job ]) -> Union [ Batch , Job ]: \"\"\"Refresh each jobs' information Args: job_or_batch: A Batch of Job object to refresh Returns: A Batch or Job object with refreshed information \"\"\" raise NotImplementedError ( f 'Cannot refresh { type ( job_or_batch ) } type object' )","title":"refresh()"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_autorift_job","text":"Submit an autoRIFT job Parameters: Name Type Description Default granule1 str The first granule (scene) to use required granule2 str The second granule (scene) to use required name Optional [ str ] A name for the job None Returns: Type Description Batch A Batch object containing the autoRIFT job Source code in hyp3_sdk/hyp3.py 194 195 196 197 198 199 200 201 202 203 204 205 206 def submit_autorift_job ( self , granule1 : str , granule2 : str , name : Optional [ str ] = None ) -> Batch : \"\"\"Submit an autoRIFT job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job Returns: A Batch object containing the autoRIFT job \"\"\" job_dict = self . prepare_autorift_job ( granule1 , granule2 , name = name ) return self . submit_prepared_jobs ( prepared_jobs = job_dict )","title":"submit_autorift_job()"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_insar_job","text":"Submit an InSAR job Parameters: Name Type Description Default granule1 str The first granule (scene) to use required granule2 str The second granule (scene) to use required name Optional [ str ] A name for the job None include_look_vectors bool Include the look vector theta and phi files in the product package False include_los_displacement bool Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of include_displacement_maps , and will be removed in a future release. False include_inc_map bool Include the local and ellipsoidal incidence angle maps in the product package False looks Literal ['20x4', '10x2'] Number of looks to take in range and azimuth '20x4' include_dem bool Include the digital elevation model GeoTIFF in the product package False include_wrapped_phase bool Include the wrapped phase GeoTIFF in the product package False apply_water_mask bool Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping False include_displacement_maps bool Include displacement maps (line-of-sight and vertical) in the product package False Returns: Type Description Batch A Batch object containing the InSAR job Source code in hyp3_sdk/hyp3.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 def submit_insar_job ( self , granule1 : str , granule2 : str , name : Optional [ str ] = None , include_look_vectors : bool = False , include_los_displacement : bool = False , include_inc_map : bool = False , looks : Literal [ '20x4' , '10x2' ] = '20x4' , include_dem : bool = False , include_wrapped_phase : bool = False , apply_water_mask : bool = False , include_displacement_maps : bool = False ) -> Batch : \"\"\"Submit an InSAR job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job include_look_vectors: Include the look vector theta and phi files in the product package include_los_displacement: Include a GeoTIFF in the product package containing displacement values along the Line-Of-Sight (LOS). This parameter has been deprecated in favor of `include_displacement_maps`, and will be removed in a future release. include_inc_map: Include the local and ellipsoidal incidence angle maps in the product package looks: Number of looks to take in range and azimuth include_dem: Include the digital elevation model GeoTIFF in the product package include_wrapped_phase: Include the wrapped phase GeoTIFF in the product package apply_water_mask: Sets pixels over coastal waters and large inland waterbodies as invalid for phase unwrapping include_displacement_maps: Include displacement maps (line-of-sight and vertical) in the product package Returns: A Batch object containing the InSAR job \"\"\" arguments = locals () . copy () arguments . pop ( 'self' ) job_dict = self . prepare_insar_job ( ** arguments ) return self . submit_prepared_jobs ( prepared_jobs = job_dict )","title":"submit_insar_job()"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_prepared_jobs","text":"Submit a prepared job dictionary, or list of prepared job dictionaries Parameters: Name Type Description Default prepared_jobs Union [ dict , List [ dict ]] A prepared job dictionary, or list of prepared job dictionaries required Returns: Type Description Batch A Batch object containing the submitted job(s) Source code in hyp3_sdk/hyp3.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def submit_prepared_jobs ( self , prepared_jobs : Union [ dict , List [ dict ]]) -> Batch : \"\"\"Submit a prepared job dictionary, or list of prepared job dictionaries Args: prepared_jobs: A prepared job dictionary, or list of prepared job dictionaries Returns: A Batch object containing the submitted job(s) \"\"\" if isinstance ( prepared_jobs , dict ): payload = { 'jobs' : [ prepared_jobs ]} else : payload = { 'jobs' : prepared_jobs } response = self . session . post ( urljoin ( self . url , '/jobs' ), json = payload ) _raise_for_hyp3_status ( response ) batch = Batch () for job in response . json ()[ 'jobs' ]: batch += Job . from_dict ( job ) return batch","title":"submit_prepared_jobs()"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.submit_rtc_job","text":"Submit an RTC job Parameters: Name Type Description Default granule str The granule (scene) to use required name Optional [ str ] A name for the job None dem_matching bool Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files False include_dem bool Include the DEM file in the product package False include_inc_map bool Include the local incidence angle map in the product package False include_rgb bool Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules) False include_scattering_area bool Include the scattering area in the product package False radiometry Literal ['sigma0', 'gamma0'] Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0) 'gamma0' resolution Literal [30] Desired output pixel spacing in meters 30 scale Literal ['amplitude', 'power'] Scale of output image; either power or amplitude 'power' speckle_filter bool Apply an Enhanced Lee speckle filter False dem_name Literal ['copernicus', 'legacy'] Name of the DEM to use for processing. copernicus will use the Copernicus GLO-30 Public DEM, while legacy will use the DEM with the best coverage from ASF's legacy SRTM/NED datasets. 'copernicus' Returns: Type Description Batch A Batch object containing the RTC job Source code in hyp3_sdk/hyp3.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 def submit_rtc_job ( self , granule : str , name : Optional [ str ] = None , dem_matching : bool = False , include_dem : bool = False , include_inc_map : bool = False , include_rgb : bool = False , include_scattering_area : bool = False , radiometry : Literal [ 'sigma0' , 'gamma0' ] = 'gamma0' , resolution : Literal [ 30 ] = 30 , scale : Literal [ 'amplitude' , 'power' ] = 'power' , speckle_filter : bool = False , dem_name : Literal [ 'copernicus' , 'legacy' ] = 'copernicus' ) -> Batch : \"\"\"Submit an RTC job Args: granule: The granule (scene) to use name: A name for the job dem_matching: Coregisters SAR data to the DEM, rather than using dead reckoning based on orbit files include_dem: Include the DEM file in the product package include_inc_map: Include the local incidence angle map in the product package include_rgb: Include a false-color RGB decomposition in the product package for dual-pol granules (ignored for single-pol granules) include_scattering_area: Include the scattering area in the product package radiometry: Backscatter coefficient normalization, either by ground area (sigma0) or illuminated area projected into the look direction (gamma0) resolution: Desired output pixel spacing in meters scale: Scale of output image; either power or amplitude speckle_filter: Apply an Enhanced Lee speckle filter dem_name: Name of the DEM to use for processing. `copernicus` will use the Copernicus GLO-30 Public DEM, while `legacy` will use the DEM with the best coverage from ASF's legacy SRTM/NED datasets. Returns: A Batch object containing the RTC job \"\"\" arguments = locals () arguments . pop ( 'self' ) job_dict = self . prepare_rtc_job ( ** arguments ) return self . submit_prepared_jobs ( prepared_jobs = job_dict )","title":"submit_rtc_job()"},{"location":"using/sdk_api/#hyp3_sdk.hyp3.HyP3.watch","text":"Watch jobs until they complete Parameters: Name Type Description Default job_or_batch Union [ Batch , Job ] A Batch or Job object of jobs to watch required timeout int How long to wait until exiting in seconds 10800 interval Union [ int , float ] How often to check for updates in seconds 60 Returns: Type Description Union [ Batch , Job ] A Batch or Job object with refreshed watched jobs Source code in hyp3_sdk/hyp3.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 @singledispatchmethod def watch ( self , job_or_batch : Union [ Batch , Job ], timeout : int = 10800 , interval : Union [ int , float ] = 60 ) -> Union [ Batch , Job ]: \"\"\"Watch jobs until they complete Args: job_or_batch: A Batch or Job object of jobs to watch timeout: How long to wait until exiting in seconds interval: How often to check for updates in seconds Returns: A Batch or Job object with refreshed watched jobs \"\"\" raise NotImplementedError ( f 'Cannot watch { type ( job_or_batch ) } type object' )","title":"watch()"},{"location":"using/sdk_api/#hyp3_sdk.Job","text":"Source code in hyp3_sdk/jobs.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 class Job : _attributes_for_resubmit = { 'name' , 'job_parameters' , 'job_type' } def __init__ ( self , job_type : str , job_id : str , request_time : datetime , status_code : str , user_id : str , name : Optional [ str ] = None , job_parameters : Optional [ dict ] = None , files : Optional [ List ] = None , logs : Optional [ List ] = None , browse_images : Optional [ List ] = None , thumbnail_images : Optional [ List ] = None , expiration_time : Optional [ datetime ] = None , processing_time_in_seconds : Optional [ int ] = None , ): self . job_id = job_id self . job_type = job_type self . request_time = request_time self . status_code = status_code self . user_id = user_id self . name = name self . job_parameters = job_parameters self . files = files self . logs = logs self . browse_images = browse_images self . thumbnail_images = thumbnail_images self . expiration_time = expiration_time self . processing_time_in_seconds = processing_time_in_seconds def __repr__ ( self ): return f 'Job.from_dict( { self . to_dict () } )' def __str__ ( self ): return f 'HyP3 { self . job_type } job { self . job_id } ' def __eq__ ( self , other ): return self . __dict__ == other . __dict__ @staticmethod def from_dict ( input_dict : dict ): expiration_time = parse_date ( input_dict [ 'expiration_time' ]) if input_dict . get ( 'expiration_time' ) else None return Job ( job_type = input_dict [ 'job_type' ], job_id = input_dict [ 'job_id' ], request_time = parse_date ( input_dict [ 'request_time' ]), status_code = input_dict [ 'status_code' ], user_id = input_dict [ 'user_id' ], name = input_dict . get ( 'name' ), job_parameters = input_dict . get ( 'job_parameters' ), files = input_dict . get ( 'files' ), logs = input_dict . get ( 'logs' ), browse_images = input_dict . get ( 'browse_images' ), thumbnail_images = input_dict . get ( 'thumbnail_images' ), expiration_time = expiration_time , processing_time_in_seconds = input_dict . get ( 'processing_time_in_seconds' ), ) def to_dict ( self , for_resubmit : bool = False ): job_dict = {} if for_resubmit : keys_to_process = Job . _attributes_for_resubmit else : keys_to_process = vars ( self ) . keys () for key in keys_to_process : value = self . __getattribute__ ( key ) if value is not None : if isinstance ( value , datetime ): job_dict [ key ] = value . isoformat ( timespec = 'seconds' ) else : job_dict [ key ] = value return job_dict def succeeded ( self ) -> bool : return self . status_code == 'SUCCEEDED' def failed ( self ) -> bool : return self . status_code == 'FAILED' def complete ( self ) -> bool : return self . succeeded () or self . failed () # TODO may want to update this to check if status code is actually RUNNING, because currently this also returns # true if status is PENDING def running ( self ) -> bool : return not self . complete () def expired ( self ) -> bool : return self . expiration_time is not None and datetime . now ( tz . UTC ) >= self . expiration_time def download_files ( self , location : Union [ Path , str ] = '.' , create : bool = True ) -> List [ Path ]: \"\"\" Args: location: Directory location to put files into create: Create `location` if it does not point to an existing directory Returns: list of Path objects to downloaded files \"\"\" location = Path ( location ) if not self . succeeded (): raise HyP3SDKError ( f 'Only succeeded jobs can be downloaded; job is { self . status_code } .' ) if self . expired (): raise HyP3SDKError ( f 'Expired jobs cannot be downloaded; ' f 'job expired { self . expiration_time . isoformat ( timespec = \"seconds\" ) } .' ) if create : location . mkdir ( parents = True , exist_ok = True ) elif not location . is_dir (): raise NotADirectoryError ( str ( location )) downloaded_files = [] for file in self . files : download_url = file [ 'url' ] filename = location / file [ 'filename' ] try : downloaded_files . append ( download_file ( download_url , filename , chunk_size = 10485760 )) except HTTPError : raise HyP3SDKError ( f 'Unable to download file: { download_url } ' ) return downloaded_files","title":"Job"},{"location":"using/sdk_api/#hyp3_sdk.jobs.Job.download_files","text":"Parameters: Name Type Description Default location Union [ Path , str ] Directory location to put files into '.' create bool Create location if it does not point to an existing directory True Source code in hyp3_sdk/jobs.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def download_files ( self , location : Union [ Path , str ] = '.' , create : bool = True ) -> List [ Path ]: \"\"\" Args: location: Directory location to put files into create: Create `location` if it does not point to an existing directory Returns: list of Path objects to downloaded files \"\"\" location = Path ( location ) if not self . succeeded (): raise HyP3SDKError ( f 'Only succeeded jobs can be downloaded; job is { self . status_code } .' ) if self . expired (): raise HyP3SDKError ( f 'Expired jobs cannot be downloaded; ' f 'job expired { self . expiration_time . isoformat ( timespec = \"seconds\" ) } .' ) if create : location . mkdir ( parents = True , exist_ok = True ) elif not location . is_dir (): raise NotADirectoryError ( str ( location )) downloaded_files = [] for file in self . files : download_url = file [ 'url' ] filename = location / file [ 'filename' ] try : downloaded_files . append ( download_file ( download_url , filename , chunk_size = 10485760 )) except HTTPError : raise HyP3SDKError ( f 'Unable to download file: { download_url } ' ) return downloaded_files","title":"download_files()"},{"location":"using/subscriptions/","text":"Subscriptions in HyP3 \u00b6 For monitoring applications, we provide a subscription service for On Demand processing, which automatically processes new Sentinel-1 acquisitions as they appear in ASF's data holdings. To create a subscription, users define the following parameters: Geographic Area of Interest Search Filters (file type, direction, path, platform, etc.) On Demand processing type (InSAR, RTC) On Demand processing options Project Name Date range extending into the future As ASF receives new acquisitions that meet the search criteria for the subscription, they will automatically be submitted to HyP3 for On Demand processing using the defined parameters. This service is particularly useful for ongoing monitoring efforts where users need regular updates over an area of interest, and want to make sure that they process all available images for that area. Without a subscription, a user would need to periodically search to find new images acquired through time and manually submit them for processing. Subscriptions can be created and managed in the Vertex interface. Refer to the Vertex On Demand Subscription documentation for more information. Guidelines for Using Subscriptions \u00b6 Subscriptions are only necessary for applications that require imagery to be processed beyond the date of the initial search. If all of the images required for a project were acquired before the date of the search, they can be submitted directly for On Demand processing . Set an end date that is appropriate for your use case. You must define an end date; you cannot set an open-ended subscription. If your project extends beyond your original end date, you can change it, so there is no need to set it far into the future. Set your AOI to be as small and targeted as possible to minimize the number of granules submitted for processing. Consider applying additional filters to ensure that you are only processing the images you really want; it can be easy to exceed your processing quota if your parameters are set too broadly. On Demand products are deleted from storage 14 days after they are generated. Make sure to download the products generated by your subscription on a regular basis so that they don't expire before you access them. Deactivate your subscription when you no longer need it. If you want to restart your monitoring effort later, you can reactivate your subscription or create a new one. Accessing Subscriptions \u00b6 Subscriptions can be created and managed in Vertex and the HyP3 API . Stay tuned for subscription support in the HyP3 Python SDK!","title":"Subscriptions"},{"location":"using/subscriptions/#subscriptions-in-hyp3","text":"For monitoring applications, we provide a subscription service for On Demand processing, which automatically processes new Sentinel-1 acquisitions as they appear in ASF's data holdings. To create a subscription, users define the following parameters: Geographic Area of Interest Search Filters (file type, direction, path, platform, etc.) On Demand processing type (InSAR, RTC) On Demand processing options Project Name Date range extending into the future As ASF receives new acquisitions that meet the search criteria for the subscription, they will automatically be submitted to HyP3 for On Demand processing using the defined parameters. This service is particularly useful for ongoing monitoring efforts where users need regular updates over an area of interest, and want to make sure that they process all available images for that area. Without a subscription, a user would need to periodically search to find new images acquired through time and manually submit them for processing. Subscriptions can be created and managed in the Vertex interface. Refer to the Vertex On Demand Subscription documentation for more information.","title":"Subscriptions in HyP3"},{"location":"using/subscriptions/#guidelines-for-using-subscriptions","text":"Subscriptions are only necessary for applications that require imagery to be processed beyond the date of the initial search. If all of the images required for a project were acquired before the date of the search, they can be submitted directly for On Demand processing . Set an end date that is appropriate for your use case. You must define an end date; you cannot set an open-ended subscription. If your project extends beyond your original end date, you can change it, so there is no need to set it far into the future. Set your AOI to be as small and targeted as possible to minimize the number of granules submitted for processing. Consider applying additional filters to ensure that you are only processing the images you really want; it can be easy to exceed your processing quota if your parameters are set too broadly. On Demand products are deleted from storage 14 days after they are generated. Make sure to download the products generated by your subscription on a regular basis so that they don't expire before you access them. Deactivate your subscription when you no longer need it. If you want to restart your monitoring effort later, you can reactivate your subscription or create a new one.","title":"Guidelines for Using Subscriptions"},{"location":"using/subscriptions/#accessing-subscriptions","text":"Subscriptions can be created and managed in Vertex and the HyP3 API . Stay tuned for subscription support in the HyP3 Python SDK!","title":"Accessing Subscriptions"},{"location":"using/vertex/","text":"On Demand Sentinel-1 Processing in Vertex \u00b6 The Alaska Satellite Facility offers On Demand processing of Sentinel-1 datasets to Radiometric Terrain Correction (RTC) or Interferometric SAR (InSAR) products through Vertex , ASF's Data Search web portal. You can submit scenes to be processed into higher-level products, avoiding the cost and complexity of performing such processing yourself. On Demand Sentinel-1 products are generated using ASF's HyP3 processing platform, leveraging GAMMA Software. Products are distributed as UTM-projected GeoTIFFs. To learn more about the finished products, refer to the Product Guides: ASF Sentinel-1 RTC Product Guide ASF Sentinel-1 InSAR Product Guide Getting Started \u00b6 To request On Demand products, visit ASF Data Search - Vertex . Select your scenes - RTC processing is available for Sentinel-1 GRD-H and SLC scenes with a beam mode of IW. InSAR processing requires pairs of IW SLC scenes. Use the Geographic Search in Vertex to find individual scenes to submit for RTC processing, or reference scenes to use for generating InSAR pairs. For InSAR, once you find a reference scene, use either the Baseline or SBAS Search to find scene pairs to submit for processing. Submit your request - After selecting your scenes, access the On Demand queue to submit your processing request. You may process up to 1000 jobs per month. Monitor your request - The On Demand Products search type displays your running and completed requests. New requests are typically available for download within an hour, but wait time will depend on processing load. Download your data - Finished On Demand products can be downloaded after an On Demand Products search either directly or via your download queue . On Demand products are retained and available to download for two weeks after processing. Tutorials \u00b6 Refer to our step-by-step tutorials for ordering and accessing RTC and InSAR products in Vertex.","title":"Vertex"},{"location":"using/vertex/#on-demand-sentinel-1-processing-in-vertex","text":"The Alaska Satellite Facility offers On Demand processing of Sentinel-1 datasets to Radiometric Terrain Correction (RTC) or Interferometric SAR (InSAR) products through Vertex , ASF's Data Search web portal. You can submit scenes to be processed into higher-level products, avoiding the cost and complexity of performing such processing yourself. On Demand Sentinel-1 products are generated using ASF's HyP3 processing platform, leveraging GAMMA Software. Products are distributed as UTM-projected GeoTIFFs. To learn more about the finished products, refer to the Product Guides: ASF Sentinel-1 RTC Product Guide ASF Sentinel-1 InSAR Product Guide","title":"On Demand Sentinel-1 Processing in Vertex"},{"location":"using/vertex/#getting-started","text":"To request On Demand products, visit ASF Data Search - Vertex . Select your scenes - RTC processing is available for Sentinel-1 GRD-H and SLC scenes with a beam mode of IW. InSAR processing requires pairs of IW SLC scenes. Use the Geographic Search in Vertex to find individual scenes to submit for RTC processing, or reference scenes to use for generating InSAR pairs. For InSAR, once you find a reference scene, use either the Baseline or SBAS Search to find scene pairs to submit for processing. Submit your request - After selecting your scenes, access the On Demand queue to submit your processing request. You may process up to 1000 jobs per month. Monitor your request - The On Demand Products search type displays your running and completed requests. New requests are typically available for download within an hour, but wait time will depend on processing load. Download your data - Finished On Demand products can be downloaded after an On Demand Products search either directly or via your download queue . On Demand products are retained and available to download for two weeks after processing.","title":"Getting Started"},{"location":"using/vertex/#tutorials","text":"Refer to our step-by-step tutorials for ordering and accessing RTC and InSAR products in Vertex.","title":"Tutorials"}]}