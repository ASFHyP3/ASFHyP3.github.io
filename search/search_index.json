{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ASF HyP3 \u00b6 Alaska Satellite Facility's Hybrid Pluggable Processing Pipeline HyP3 is a service for processing Synthetic Aperture Radar (SAR) imagery that addresses many common issues for users of SAR data: Most SAR datasets require at least some processing to remove distortions before they are analysis-ready SAR processing is computing-intensive Software for SAR processing is complicated to use and/or prohibitively expensive Producing analysis-ready SAR data has a steep learning curve that acts as a barrier to entry HyP3 solves these problems by providing a free service where people can request SAR processing on-demand. These processing requests are picked up by automated systems, which handle the complexity of SAR processing on behalf of the user. HyP3 doesn't require users to have a lot of knowledge of SAR processing before getting started; users only need to submit the input data and set a few optional parameters if desired. With HyP3, analysis-ready products are just a few clicks away. How it Works \u00b6 HyP3 is built around three core concepts: Platform, Plugins, and Products. The HyP3 platform makes it easy for users to request processing, monitor their requests, and download processed products. The platform delegates each processing request to a plugin on the user's behalf. A deployment of the HyP3 platform can be integrated with any number of plugins. Plugins are the workhorses of HyP3. Each plugin implements a particular SAR processing workflow. When invoked, they marshal input data and generate an output product. Plugins are container-based and can be used independently of the platform. Products are the end result of processing, typically one or more data files. Contact Us \u00b6 Want to talk about HyP3? We would love to hear from you! Found a bug? Want to request a feature? open an issue General questions? Suggestions? Or just want to talk to the team? chat with us on gitter","title":"Home"},{"location":"#asf-hyp3","text":"Alaska Satellite Facility's Hybrid Pluggable Processing Pipeline HyP3 is a service for processing Synthetic Aperture Radar (SAR) imagery that addresses many common issues for users of SAR data: Most SAR datasets require at least some processing to remove distortions before they are analysis-ready SAR processing is computing-intensive Software for SAR processing is complicated to use and/or prohibitively expensive Producing analysis-ready SAR data has a steep learning curve that acts as a barrier to entry HyP3 solves these problems by providing a free service where people can request SAR processing on-demand. These processing requests are picked up by automated systems, which handle the complexity of SAR processing on behalf of the user. HyP3 doesn't require users to have a lot of knowledge of SAR processing before getting started; users only need to submit the input data and set a few optional parameters if desired. With HyP3, analysis-ready products are just a few clicks away.","title":"ASF HyP3"},{"location":"#how-it-works","text":"HyP3 is built around three core concepts: Platform, Plugins, and Products. The HyP3 platform makes it easy for users to request processing, monitor their requests, and download processed products. The platform delegates each processing request to a plugin on the user's behalf. A deployment of the HyP3 platform can be integrated with any number of plugins. Plugins are the workhorses of HyP3. Each plugin implements a particular SAR processing workflow. When invoked, they marshal input data and generate an output product. Plugins are container-based and can be used independently of the platform. Products are the end result of processing, typically one or more data files.","title":"How it Works"},{"location":"#contact-us","text":"Want to talk about HyP3? We would love to hear from you! Found a bug? Want to request a feature? open an issue General questions? Suggestions? Or just want to talk to the team? chat with us on gitter","title":"Contact Us"},{"location":"CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct \u00b6 Our Pledge \u00b6 We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards \u00b6 Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities \u00b6 Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope \u00b6 This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement by emailing the ASF APD/Tools team at UAF-asf-apd@alaska.edu . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines \u00b6 Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction \u00b6 Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning \u00b6 Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban \u00b6 Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban \u00b6 Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Code of Conduct"},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement by emailing the ASF APD/Tools team at UAF-asf-apd@alaska.edu . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"contributing/","text":"Contributing \u00b6 Thank you for your interest in helping make custom on-demand SAR processing accessible! We're excited you would like to contribute to HyP3! Whether you're finding bugs, adding new features, fixing anything broken, or improving documentation, get started by submitting an issue or pull request! Please read our Code of Conduct before contributing. Issues and Pull Requests are welcome \u00b6 If you have any questions or ideas, or notice any problems or bugs, and want to open an issue, great! We recommend first searching our open issues to see if the issue has already been submitted (we may already be working on it!). If you think your issue is new, you're welcome to create a new issue in our general issues tracker. If you know the specific repository that your issue pertains to, you can use its issues tracker (see our repositories list below). Found a typo, know how to fix a bug, want to update the docs, want to add a new feature? Even better! The smaller the PR, the easier it is to review and test and the more likely it is to be successful. For major contributions, consider opening an issue describing the contribution so we can help guide and breakup the work into digestible pieces. Pull Request Guidelines \u00b6 We ask that you follow these guidelines with your contributions Style We generally follow python community standards ( PEP8 ), except we allow line lengths up to 120 characters. We recommend trying to keep lines 80--100 characters long, but allow up to 120 when it improves readability. Documentation We are working to improve our documentation! For all public-facing functions/methods (not marked internal use ), please include type hints (when reasonable) and a docstring formatted Google style . Tests All of the automated tests for the project need to pass before your submission will be accepted. If you add new functionality, please consider adding tests for that functionality as well. Commits Make small commits that show the individual changes you are making Write descriptive commit messages that explain your changes Example of a good commit message: Improve contributing guidelines. Fixes #10 Improve contributing docs and consolidate them in the standard location https://help.github.com/articles/setting-guidelines-for-repository-contributors/","title":"Contributing"},{"location":"contributing/#contributing","text":"Thank you for your interest in helping make custom on-demand SAR processing accessible! We're excited you would like to contribute to HyP3! Whether you're finding bugs, adding new features, fixing anything broken, or improving documentation, get started by submitting an issue or pull request! Please read our Code of Conduct before contributing.","title":"Contributing"},{"location":"contributing/#issues-and-pull-requests-are-welcome","text":"If you have any questions or ideas, or notice any problems or bugs, and want to open an issue, great! We recommend first searching our open issues to see if the issue has already been submitted (we may already be working on it!). If you think your issue is new, you're welcome to create a new issue in our general issues tracker. If you know the specific repository that your issue pertains to, you can use its issues tracker (see our repositories list below). Found a typo, know how to fix a bug, want to update the docs, want to add a new feature? Even better! The smaller the PR, the easier it is to review and test and the more likely it is to be successful. For major contributions, consider opening an issue describing the contribution so we can help guide and breakup the work into digestible pieces.","title":"Issues and Pull Requests are welcome"},{"location":"contributing/#pull-request-guidelines","text":"We ask that you follow these guidelines with your contributions","title":"Pull Request Guidelines"},{"location":"dems/","text":"Digital Elevation Models (DEMs) \u00b6 ASF uses publicly-available Digital Elevation Models for processing SAR data. The DEM used will vary by scene location; the best available DEM with full coverage of the scene extent will be used for processing any given scene. DEM Coverage \u00b6 The source DEMs include: Resolution DEM Datum Area Posting Sampling High NED13 NAVD88 CONUS, Hawaii, parts of Alaska 1/3 arc seconds Resampled to product spacing, reprojected to WGS84 UTM Medium SRTMGL1 EGM96 60 N to 57 S latitude 1 arc second Resampled to product spacing, reprojected to WGS84 UTM Medium SRTM US1 EGM96 CONUS, Hawaii, parts of Alaska 1 arc second Resampled to product spacing, reprojected to WGS84 UTM Medium NED1 NAVD88 CONUS, Hawaii, parts of Alaska, Canada, Mexico 1 arc second Resampled to product spacing, reprojected to WGS84 UTM Medium NED2 NAVD88 Alaska 2 arc seconds Resampled to product spacing, reprojected to WGS84 UTM Note For terrain corrected products, the quality of the results is directly related to the quality of the digital elevation models (DEMs) used in the process of geometrically and radiometrically correcting the SAR imagery. The DEMs were pre-processed by ASF to a consistent raster format (GeoTIFF) from the original source formats: height ( *.hgt ), ESRI ArcGrid ( *.adf ), etc. Many of the NASA-provided DEMs were provided as orthometric heights with EGM96 vertical datum. These were converted by ASF to ellipsoid heights using the ASF MapReady tool named geoid_adjust . The pixel reference varied from the center (pixel as point) to a corner (pixel as area). For terrain corrected products, the GAMMA software uses pixel as area and adjusts DEM coordinates as needed. Where more than one DEM is available, the best-resolution DEM is used for processing. DEM coverage of at least 20% from a single DEM source is required for processing to proceed. Special Use DEMs \u00b6 AutoRIFT , a process developed by the NASA MEaSUREs ITS_LIVE project, processes use a custom Greenland and Antarctica DEM with a 240 m resolution. The DEM, associated process input files, and their details are available on the ITS_LIVE project website.","title":"Digital Elevation Models"},{"location":"dems/#digital-elevation-models-dems","text":"ASF uses publicly-available Digital Elevation Models for processing SAR data. The DEM used will vary by scene location; the best available DEM with full coverage of the scene extent will be used for processing any given scene.","title":"Digital Elevation Models (DEMs)"},{"location":"dems/#dem-coverage","text":"The source DEMs include: Resolution DEM Datum Area Posting Sampling High NED13 NAVD88 CONUS, Hawaii, parts of Alaska 1/3 arc seconds Resampled to product spacing, reprojected to WGS84 UTM Medium SRTMGL1 EGM96 60 N to 57 S latitude 1 arc second Resampled to product spacing, reprojected to WGS84 UTM Medium SRTM US1 EGM96 CONUS, Hawaii, parts of Alaska 1 arc second Resampled to product spacing, reprojected to WGS84 UTM Medium NED1 NAVD88 CONUS, Hawaii, parts of Alaska, Canada, Mexico 1 arc second Resampled to product spacing, reprojected to WGS84 UTM Medium NED2 NAVD88 Alaska 2 arc seconds Resampled to product spacing, reprojected to WGS84 UTM Note For terrain corrected products, the quality of the results is directly related to the quality of the digital elevation models (DEMs) used in the process of geometrically and radiometrically correcting the SAR imagery. The DEMs were pre-processed by ASF to a consistent raster format (GeoTIFF) from the original source formats: height ( *.hgt ), ESRI ArcGrid ( *.adf ), etc. Many of the NASA-provided DEMs were provided as orthometric heights with EGM96 vertical datum. These were converted by ASF to ellipsoid heights using the ASF MapReady tool named geoid_adjust . The pixel reference varied from the center (pixel as point) to a corner (pixel as area). For terrain corrected products, the GAMMA software uses pixel as area and adjusts DEM coordinates as needed. Where more than one DEM is available, the best-resolution DEM is used for processing. DEM coverage of at least 20% from a single DEM source is required for processing to proceed.","title":"DEM Coverage"},{"location":"dems/#special-use-dems","text":"AutoRIFT , a process developed by the NASA MEaSUREs ITS_LIVE project, processes use a custom Greenland and Antarctica DEM with a 240 m resolution. The DEM, associated process input files, and their details are available on the ITS_LIVE project website.","title":"Special Use DEMs"},{"location":"getting_started/","text":"Getting started \u00b6 Using Vertex \u00b6 Vertex , ASF's data search portal, is the easiest way to use HyP3. Vertex provides a friendly interface to request jobs and review previous jobs. Visit Using HyP3: Vertex to learn more. Using the Python SDK \u00b6 The HyP3 SDK is a Python library for using HyP3 programmatically. It is available on PyPI and can be installed with pip install hyp3-sdk . Visit Using HyP3: SDK to learn more. Warning The HyP3 SDK is experimental and still being developed. It should not yet be used in production systems. Using the API \u00b6 The HyP3 web API is the backbone behind Vertex and the SDK. You may also use the API directly. Visit Using HyP3: API to learn more.","title":"Getting started"},{"location":"getting_started/#getting-started","text":"","title":"Getting started"},{"location":"getting_started/#using-vertex","text":"Vertex , ASF's data search portal, is the easiest way to use HyP3. Vertex provides a friendly interface to request jobs and review previous jobs. Visit Using HyP3: Vertex to learn more.","title":"Using Vertex"},{"location":"getting_started/#using-the-python-sdk","text":"The HyP3 SDK is a Python library for using HyP3 programmatically. It is available on PyPI and can be installed with pip install hyp3-sdk . Visit Using HyP3: SDK to learn more. Warning The HyP3 SDK is experimental and still being developed. It should not yet be used in production systems.","title":"Using the Python SDK"},{"location":"getting_started/#using-the-api","text":"The HyP3 web API is the backbone behind Vertex and the SDK. You may also use the API directly. Visit Using HyP3: API to learn more.","title":"Using the API"},{"location":"plugins/","text":"Plugins \u00b6 Plugins are the science backbone of HyP3; they do all of the data processing and product generation. Plugins can be added to HyP3 to generate new science products, or support different tools/software/algorithms/options/etc that are not currently supported by HyP3. How plugins work \u00b6 At their most basic level, HyP3 plugins are Docker containers with an interface (entrypoint) HyP3 understands. Plugins handle the entire processing workflow for a single product, including: Marshaling the required input data performing any needed transformations and computations on the data creating the final product uploading the product to an AWS S3 bucket for distribution By encapsulating the entire workflow for generating a single product, HyP3 can arbitrarily scale to meet user need. Developing a plugin \u00b6 To create a new HyP3 plugin, we recommend starting from a Minimal Working Example (MWE) of generating the product you're plugin will generate. Importantly, the MWE should be entirely self contained, and include all the necessary data to generate the product. Once a MWE is developed, it's important to define your plugin's interface -- this is where HyP3 connects the product generation and users. When designing the interface, you may find it helpful to ask yourself: what options do I want to provide to users? what's the minimal set information I need to gather from users? is this information easily input by users? is this information serializable? For example, can the information be written in a JSON file? could I define this information more simply? Once a MWE is developed and an interface is defined, you can use our HyP3 plugin cookiecuter to help you build a plugin that conforms to the plugin requirements. Plugin requirements \u00b6 In order to be supported by HyP3, a plugin must meet a few requirements: the plugin must be a Docker image that is hosted in a repository where HyP3 will be able to pull it the plugin's entrypoint must minimally accept the following arguments --bucket BUCKET-NAME where BUCKET-NAME is the name of an AWS S3 bucket that output products will be uploaded to --bucket-prefix BUCKET-PREFIX where BUCKET-PREFIX is a string appended to the key of any file uploaded to AWS S3 (this is effectively a subfolder in AWS S3) --username USER where USER is the username used to authenticate to EarthData Login --password PASSWORD where PASSWORD is the password used to authenticate to EarthData Login any necessary user input should be able to be provided through entrypoint arguments when uploading files to the S3 Bucket products files must be tagged with filetype: product if you wish to upload thumbnails or browse images, they must be tagged filetype: thumbnail or filetype: browse respectively Note: the aws subpackage of hyp3lib provides helper functions for tagging and uploading files Add the plugin to HyP3 \u00b6 Once the plugin itself is created, it can be added to the HyP3 system by... TBD.","title":"Overview"},{"location":"plugins/#plugins","text":"Plugins are the science backbone of HyP3; they do all of the data processing and product generation. Plugins can be added to HyP3 to generate new science products, or support different tools/software/algorithms/options/etc that are not currently supported by HyP3.","title":"Plugins"},{"location":"plugins/#how-plugins-work","text":"At their most basic level, HyP3 plugins are Docker containers with an interface (entrypoint) HyP3 understands. Plugins handle the entire processing workflow for a single product, including: Marshaling the required input data performing any needed transformations and computations on the data creating the final product uploading the product to an AWS S3 bucket for distribution By encapsulating the entire workflow for generating a single product, HyP3 can arbitrarily scale to meet user need.","title":"How plugins work"},{"location":"plugins/#developing-a-plugin","text":"To create a new HyP3 plugin, we recommend starting from a Minimal Working Example (MWE) of generating the product you're plugin will generate. Importantly, the MWE should be entirely self contained, and include all the necessary data to generate the product. Once a MWE is developed, it's important to define your plugin's interface -- this is where HyP3 connects the product generation and users. When designing the interface, you may find it helpful to ask yourself: what options do I want to provide to users? what's the minimal set information I need to gather from users? is this information easily input by users? is this information serializable? For example, can the information be written in a JSON file? could I define this information more simply? Once a MWE is developed and an interface is defined, you can use our HyP3 plugin cookiecuter to help you build a plugin that conforms to the plugin requirements.","title":"Developing a plugin"},{"location":"plugins/#plugin-requirements","text":"In order to be supported by HyP3, a plugin must meet a few requirements: the plugin must be a Docker image that is hosted in a repository where HyP3 will be able to pull it the plugin's entrypoint must minimally accept the following arguments --bucket BUCKET-NAME where BUCKET-NAME is the name of an AWS S3 bucket that output products will be uploaded to --bucket-prefix BUCKET-PREFIX where BUCKET-PREFIX is a string appended to the key of any file uploaded to AWS S3 (this is effectively a subfolder in AWS S3) --username USER where USER is the username used to authenticate to EarthData Login --password PASSWORD where PASSWORD is the password used to authenticate to EarthData Login any necessary user input should be able to be provided through entrypoint arguments when uploading files to the S3 Bucket products files must be tagged with filetype: product if you wish to upload thumbnails or browse images, they must be tagged filetype: thumbnail or filetype: browse respectively Note: the aws subpackage of hyp3lib provides helper functions for tagging and uploading files","title":"Plugin requirements"},{"location":"plugins/#add-the-plugin-to-hyp3","text":"Once the plugin itself is created, it can be added to the HyP3 system by... TBD.","title":"Add the plugin to HyP3"},{"location":"products/","text":"Available HyP3 Products \u00b6 RTC \u00b6 SAR datasets inherently contain geometric and radiometric distortions due to terrain being imaged by a side-looking instrument. Radiometric terrain correction (RTC) removes these distortions and creates analysis-ready data suitable for use in GIS applications. RTC processing is a required first step for many amplitude-based SAR applications. Sentinel-1 RTC products are generated leveraging GAMMA Software. Products are distributed as UTM-projected GeoTIFFs with a pixel spacing of 30 meters. To learn more, visit the ASF Sentinel-1 RTC Product Guide . A Digital Elevation Model (DEM) is required for processing RTC. ASF uses the best publicly-available DEM with full coverage of the processing area. To learn more, visit Digital Elevation Models . InSAR (SDK + API only) \u00b6 Interferometric SAR (InSAR) uses the phase differences from repeat passes over the same area to identify regions where the distance between the sensor and the Earth's surface has changed. This allows for the detection and quantification of deformation or movement. Use caution when generating interferograms for areas with extensive/dense vegetation cover. Because Sentinel-1 is a C-band sensor, the waves will not penetrate very deeply into vegetation. Imagery of densely vegetated areas likely represents the top of the canopy rather than the actual terrain. In addition, vegetated areas tend to have low coherence, because plants can grow or move from one acquisition to the next. A Digital Elevation Model (DEM) is required for processing InSAR. ASF uses the best publicly-available DEM with full coverage of the processing area. To learn more, visit Digital Elevation Models . autoRIFT (SDK + API only) \u00b6 AutoRIFT produces a velocity map from observed motion using a feature tracking algorithm developed as part of the NASA MEaSUREs ITS_LIVE project. To learn more, visit the ITS_LIVE project website. A Digital Elevation Model (DEM) is required for autoRIFT processing. ASF uses the best publicly-available DEM with full coverage of the processing area. To learn more, visit Digital Elevation Models . Product usage guidelines \u00b6 When using this data in a publication or presentation, we ask that you include the acknowledgement provided with each product. DOIs are also provided for citation when discussing the HyP3 software or plugins. For multi-file products, the acknowledgement and relevant DOIs are included in the *.README.md.txt file. For netCDF products, the acknowledgement is included in the source global attribute and the DOIs are included in the references global attribute.","title":"Overview"},{"location":"products/#available-hyp3-products","text":"","title":"Available HyP3 Products"},{"location":"products/#rtc","text":"SAR datasets inherently contain geometric and radiometric distortions due to terrain being imaged by a side-looking instrument. Radiometric terrain correction (RTC) removes these distortions and creates analysis-ready data suitable for use in GIS applications. RTC processing is a required first step for many amplitude-based SAR applications. Sentinel-1 RTC products are generated leveraging GAMMA Software. Products are distributed as UTM-projected GeoTIFFs with a pixel spacing of 30 meters. To learn more, visit the ASF Sentinel-1 RTC Product Guide . A Digital Elevation Model (DEM) is required for processing RTC. ASF uses the best publicly-available DEM with full coverage of the processing area. To learn more, visit Digital Elevation Models .","title":"RTC"},{"location":"products/#insar-sdk-api-only","text":"Interferometric SAR (InSAR) uses the phase differences from repeat passes over the same area to identify regions where the distance between the sensor and the Earth's surface has changed. This allows for the detection and quantification of deformation or movement. Use caution when generating interferograms for areas with extensive/dense vegetation cover. Because Sentinel-1 is a C-band sensor, the waves will not penetrate very deeply into vegetation. Imagery of densely vegetated areas likely represents the top of the canopy rather than the actual terrain. In addition, vegetated areas tend to have low coherence, because plants can grow or move from one acquisition to the next. A Digital Elevation Model (DEM) is required for processing InSAR. ASF uses the best publicly-available DEM with full coverage of the processing area. To learn more, visit Digital Elevation Models .","title":"InSAR (SDK + API only)"},{"location":"products/#autorift-sdk-api-only","text":"AutoRIFT produces a velocity map from observed motion using a feature tracking algorithm developed as part of the NASA MEaSUREs ITS_LIVE project. To learn more, visit the ITS_LIVE project website. A Digital Elevation Model (DEM) is required for autoRIFT processing. ASF uses the best publicly-available DEM with full coverage of the processing area. To learn more, visit Digital Elevation Models .","title":"autoRIFT (SDK + API only)"},{"location":"products/#product-usage-guidelines","text":"When using this data in a publication or presentation, we ask that you include the acknowledgement provided with each product. DOIs are also provided for citation when discussing the HyP3 software or plugins. For multi-file products, the acknowledgement and relevant DOIs are included in the *.README.md.txt file. For netCDF products, the acknowledgement is included in the source global attribute and the DOIs are included in the references global attribute.","title":"Product usage guidelines"},{"location":"guides/rtc_atbd/","text":"RTC Algorithm Theoretical Basis \u00b6","title":"RTC ATBD"},{"location":"guides/rtc_atbd/#rtc-algorithm-theoretical-basis","text":"","title":"RTC Algorithm Theoretical Basis"},{"location":"guides/rtc_product_guide/","text":"RTC Product Guide \u00b6","title":"RTC Product Guide"},{"location":"guides/rtc_product_guide/#rtc-product-guide","text":"","title":"RTC Product Guide"},{"location":"plugins/autoRIFT/","text":"HyP3 autoRIFT Plugin \u00b6","title":"HyP3 autoRIFT Plugin"},{"location":"plugins/autoRIFT/#hyp3-autorift-plugin","text":"","title":"HyP3 autoRIFT Plugin"},{"location":"plugins/gamma/","text":"HyP3 GAMMA Plugin \u00b6","title":"HyP3 GAMMA Plugin"},{"location":"plugins/gamma/#hyp3-gamma-plugin","text":"","title":"HyP3 GAMMA Plugin"},{"location":"tools/arcgis_toolbox/","text":"ArcGIS Toolbox \u00b6 The ASF_Tools ArcGIS Python Toolbox can be used with either ArcGIS Desktop or ArcGIS Pro, and contains tools that perform geoprocessing tasks useful for working with Synthetic Aperture Radar (SAR) data. The tools were designed to be used with Sentinel-1 Radiometric Terrain Corrected (RTC) SAR datasets , such as those available on-demand using ASF's Data Search - Vertex portal, but several of the tools have the potential to be used with a variety of rasters, including non-SAR datasets. The Toolbox is distributed as a zipped archive including the .pyt Toolbox script and associated .xml files. There is an XML file for the toolbox itself and one for each of the tools it contains. These XML files contain the metadata displayed in the item descriptions and tool help windows in ArcGIS, and must be kept in the same directory as the Python Toolbox (.pyt) file, or the information they contain will no longer be accessible. Toolbox Contents \u00b6 Unzip Files Tool This tool assists in file management when downloading .zip files from ASF. It could be used to extract to a specified location any zip files with an additional internal directory containing the individual files. The tool deletes the original zip files once they are extracted, and is especially helpful when dealing with file paths that are so long that they are beyond the maximum allowed in default Windows unzip utilities. Scale Conversion Tool This tool converts pixel values in calibrated SAR datasets (such as RTC rasters) from power or amplitude scale into power, amplitude or dB scale. This is an application specific to SAR data values/scales. Reclassify RTC Tool This tool generates a raster that includes only those pixels below a user-defined threshold value, and is designed for isolating water pixels. While intended for RTC files in dB scale, this tool could be used for any application where the user is interested in generating a spatial mask for values below a given threshold in a single-band raster. Log Difference Tool This tool compares two rasters by calculating the log difference on a pixel-by-pixel basis to identify areas where backscatter values have changed over time. While intended for RTC files in amplitude scale, this tool could be used to compare the pixel values of any two single-band rasters, as long as there are no negative values (NoData values will be returned for pixels with a negative number in either of the datasets). RGB Decomposition Tool This tool generates an RGB image using the co- and cross-polarized datasets from an RTC product. Input datasets can be in either amplitude or power scale, and the primary polarization can be either vertical (VV/VH) or horizontal (HH/HV). Additional documentation is available regarding the calculations used and the interpretation of these false-color images. Prerequisites \u00b6 Users must have either ArcGIS Desktop (ArcMap) or ArcGIS Pro installed and licensed on their computer. The Toolbox has been tested with Desktop versions 10.6.1 and 10.7.1 and Pro versions 2.4.2, 2.5.x and 2.6.1, but it may work with earlier versions as well. Note that several of the tools require the Spatial Analyst extension. Users who do not have licensing for this extension in ArcGIS will not be able to use many of the included tools. To install the Toolbox \u00b6 Download the zip file and extract the contents to any directory accessible by the computer running ArcGIS. Ensure that the Spatial Analyst extension is licensed and enabled. ArcGIS Desktop (ArcMap) Click on the Customize menu in ArcMap and select Extensions\u2026 Check the box next to Spatial Analyst and click the Close button at the bottom of the Extensions window. If you are unable to check this box, you do not have access to the Spatial Analyst extension and will not be able to make use of tools requiring this extension. ArcGIS Pro Click on the Project tab and select the Licensing tab. In the list of Esri Extensions, scroll down to verify that the Spatial Analyst is licensed and enabled. If it is not, an organization administrator will need to enable the extension in your user account. If your organization does not have a license available for you to use, you will not be able to make use of tools requiring this extension. Using the Toolbox \u00b6 In the ArcMap Catalog window or the ArcGIS Pro Catalog pane/view, navigate to the directory containing the toolbox (create a new folder connection if necessary). - To open the Catalog window in ArcMap, click on the Windows menu and select Catalog. - To open the Catalog pane or view in ArcGIS Pro, click the View tab and click on either the Catalog Pane or Catalog View button. Note that if you explore the extracted contents of the zip file outside of the ArcGIS environment, the directory will contain one .pyt file and a number of .xml files. In the ArcGIS Catalog window/pane/view, only the Toolbox is displayed, and when it is expanded, all of the Tools contained in the Toolbox script are displayed. The XML files are automatically referenced when ArcGIS requires the information they contain, and do not appear as additional files in the ArcGIS Catalog environment. The XML files must remain in the same directory as the .pyt file, and their filenames should not be changed. Double-click the ASF_Tools.pyt file to display the Tools (Scripts) included in the toolbox. Double-click on a Tool (displayed with a Script icon) to launch the dialog box or geoprocessing pane, as you would for any other ArcGIS Tool/Script. Enter the parameters as prompted and click the OK button to execute the tool. Note that output products are not automatically added to a project by default. You must navigate to them in the Catalog window/pane/view (or using the Add Data dialog) and add them to your project if desired. Tool Help \u00b6 The XML files included in the zip file are accessed when a user views the metadata for the toolbox, individual tools, or even different fields within the tool dialog. Accessing Help from within the Tool Dialog Box ArcGIS Desktop Click on the Show Help button at the bottom of the tool window to open the help panel. This panel will display information about the tool in general if no field is activated. If the user clicks on any of the parameter fields, information specific to that parameter will be displayed. Click on the Tool Help button at the bottom of the Help pane to open another window that displays most of the information that would be displayed in the tool\u2019s Item Description. ArcGIS Pro When you hover over any of the parameter fields in the tool dialog, a blue i appears. Hover over or click the blue i icon to view helpful tips specific to that parameter. Hover over the blue question mark at the top of the geoprocessing pane to display information about the tool. Click on it to open the full tool description in a browser window. Accessing Help from the Catalog Interface ArcGIS Desktop ArcCatalog displays the information contained in the xml metadata files in the Description tab for the toolbox and each tool. In the ArcMap Catalog window, the Item Description for the toolbox or any of its constituent tools displays the xml content. - Right-click the toolbox or tool in the Catalog window and select Item Description to view the information. ArcGIS Pro The xml metadata is displayed in the Metadata tab in the Catalog view. - Right-click a tool in the Catalog pane and select View Metadata to open the Metadata tab for the item in the Catalog view. OR - Open the Catalog View directly to navigate to the tool and select the Metadata tab.","title":"ArcGIS Toolbox"},{"location":"tools/arcgis_toolbox/#arcgis-toolbox","text":"The ASF_Tools ArcGIS Python Toolbox can be used with either ArcGIS Desktop or ArcGIS Pro, and contains tools that perform geoprocessing tasks useful for working with Synthetic Aperture Radar (SAR) data. The tools were designed to be used with Sentinel-1 Radiometric Terrain Corrected (RTC) SAR datasets , such as those available on-demand using ASF's Data Search - Vertex portal, but several of the tools have the potential to be used with a variety of rasters, including non-SAR datasets. The Toolbox is distributed as a zipped archive including the .pyt Toolbox script and associated .xml files. There is an XML file for the toolbox itself and one for each of the tools it contains. These XML files contain the metadata displayed in the item descriptions and tool help windows in ArcGIS, and must be kept in the same directory as the Python Toolbox (.pyt) file, or the information they contain will no longer be accessible.","title":"ArcGIS Toolbox"},{"location":"tools/arcgis_toolbox/#toolbox-contents","text":"","title":"Toolbox Contents"},{"location":"tools/arcgis_toolbox/#prerequisites","text":"Users must have either ArcGIS Desktop (ArcMap) or ArcGIS Pro installed and licensed on their computer. The Toolbox has been tested with Desktop versions 10.6.1 and 10.7.1 and Pro versions 2.4.2, 2.5.x and 2.6.1, but it may work with earlier versions as well. Note that several of the tools require the Spatial Analyst extension. Users who do not have licensing for this extension in ArcGIS will not be able to use many of the included tools.","title":"Prerequisites"},{"location":"tools/arcgis_toolbox/#to-install-the-toolbox","text":"Download the zip file and extract the contents to any directory accessible by the computer running ArcGIS. Ensure that the Spatial Analyst extension is licensed and enabled.","title":"To install the Toolbox"},{"location":"tools/arcgis_toolbox/#using-the-toolbox","text":"In the ArcMap Catalog window or the ArcGIS Pro Catalog pane/view, navigate to the directory containing the toolbox (create a new folder connection if necessary). - To open the Catalog window in ArcMap, click on the Windows menu and select Catalog. - To open the Catalog pane or view in ArcGIS Pro, click the View tab and click on either the Catalog Pane or Catalog View button. Note that if you explore the extracted contents of the zip file outside of the ArcGIS environment, the directory will contain one .pyt file and a number of .xml files. In the ArcGIS Catalog window/pane/view, only the Toolbox is displayed, and when it is expanded, all of the Tools contained in the Toolbox script are displayed. The XML files are automatically referenced when ArcGIS requires the information they contain, and do not appear as additional files in the ArcGIS Catalog environment. The XML files must remain in the same directory as the .pyt file, and their filenames should not be changed. Double-click the ASF_Tools.pyt file to display the Tools (Scripts) included in the toolbox. Double-click on a Tool (displayed with a Script icon) to launch the dialog box or geoprocessing pane, as you would for any other ArcGIS Tool/Script. Enter the parameters as prompted and click the OK button to execute the tool. Note that output products are not automatically added to a project by default. You must navigate to them in the Catalog window/pane/view (or using the Add Data dialog) and add them to your project if desired.","title":"Using the Toolbox"},{"location":"tools/arcgis_toolbox/#tool-help","text":"The XML files included in the zip file are accessed when a user views the metadata for the toolbox, individual tools, or even different fields within the tool dialog.","title":"Tool Help"},{"location":"tools/hyp3lib/","text":"HyP3lib \u00b6","title":"HyP3lib"},{"location":"tools/hyp3lib/#hyp3lib","text":"","title":"HyP3lib"},{"location":"using/api/","text":"HyP3 API \u00b6 The HyP3 API is built on OpenAPI and Swagger . A friendly interface for exploring the API is available at: https://hyp3-api.asf.alaska.edu/ui/ Authentication Required If you get a 401 response back you need to sign in to Vertex to get the asf-urs session cookie. { \"detail\" : \"No authorization token provided\" , \"status\" : 401 , \"title\" : \"Unauthorized\" , \"type\" : \"about:blank\" }","title":"API"},{"location":"using/api/#hyp3-api","text":"The HyP3 API is built on OpenAPI and Swagger . A friendly interface for exploring the API is available at:","title":"HyP3 API"},{"location":"using/sdk/","text":"HyP3 SDK \u00b6 A python wrapper around the HyP3 API >>> from hyp3_sdk import HyP3 >>> hyp3 = HyP3 ( username = 'MyUsername' , password = 'MyPassword' ) >>> job = hyp3 . submit_rtc_job ( granule = 'S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8' , name = 'MyNewJob' ) >>> job = hyp3 . watch ( job ) >>> job . download_files () Install \u00b6 The HyP3 SDK can be installed via Anaconda/Miniconda : conda install -c conda-forge hyp3_sdk Or using pip : python -m pip install hyp3_sdk Quickstart \u00b6 There are 3 main classes that the SDK exposes: HyP3 : to perform HyP3 operations (find jobs, refresh job information, submitting new jobs) Job : to perform operations on single jobs (downloading products, check status) Batch : to perform operations on multiple jobs at once (downloading products, check status) An instance of the HyP3 class will be needed to interact with the external HyP3 API. from hyp3_sdk import HyP3 # Must either have credentials for urs.earthdata.nasa.gov in a .netrc hyp3 = HyP3 () # or provide them in the username and password keyword args hyp3 = HyP3 ( username = 'MyUsername' , password = 'MyPassword' ) Submitting Jobs \u00b6 hyp3 has member functions for submitting new jobs: rtc_job = hyp3 . submit_rtc_job ( 'granule_id' , 'job_name' ) insar_job = hyp3 . submit_insar_job ( 'reference_granule_id' , 'secondary_granule_id' , 'job_name' ) autorift_job = hyp3 . submit_autorift_job ( 'reference_granule_id' , 'secondary_granule_id' , 'job_name' ) Each of these functions will return an instance of the Job class that represents a new HyP3 job request. Finding Existing Jobs \u00b6 To find HyP3 jobs that were run previously, you can use the hyp3.find_jobs() batch = hyp3 . find_jobs () This will return a Batch instance representing all jobs owned by you. You can also pass parameters to query to a specific set of jobs Operations on Job and Batch \u00b6 If your jobs are not complete you can use the HyP3 instance to update them, and wait from completion batch = hyp3 . find_jobs () if not batch . complete (): # to get updated information batch = hyp3 . refresh ( batch ) # or to wait until completion and get updated information (which will take a fair bit) batch = hyp3 . watch ( batch ) Once you have complete jobs you can download the products to your machine batch . download_files () These operations also work on Job objects job = hyp3 . submit_rtc_job ( 'S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8' , 'MyJobName' ) job = hyp3 . watch ( job ) job . download_files () SDK API Reference \u00b6 \u00b6 A python wrapper around the HyP3 API exceptions \u00b6 Errors and exceptions to raise when the SDK runs into problems AuthenticationError \u00b6 Raise when authentication does not succeed HyP3Error \u00b6 Base Exception for Hyp3_sdk ValidationError \u00b6 Raise when jobs do not pass validation hyp3 \u00b6 HyP3 \u00b6 A python wrapper around the HyP3 API __init__ ( self , api_url = 'https://hyp3-api.asf.alaska.edu' , username = None , password = None ) special Parameters: Name Type Description Default api_url str Address of the HyP3 API 'https://hyp3-api.asf.alaska.edu' username Optional Username for authenticating to urs.earthdata.nasa.gov. Both username and password must be provided if either is provided. None password Optional Password for authenticating to urs.earthdata.nasa.gov. Both username and password must be provided if either is provided. None Source code in hyp3_sdk/hyp3.py def __init__ ( self , api_url : str = HYP3_PROD , username : Optional = None , password : Optional = None ): \"\"\" Args: api_url: Address of the HyP3 API username: Username for authenticating to urs.earthdata.nasa.gov. Both username and password must be provided if either is provided. password: Password for authenticating to urs.earthdata.nasa.gov. Both username and password must be provided if either is provided. \"\"\" self . url = api_url self . session = get_authenticated_session ( username , password ) check_quota ( self ) Returns: Type Description int The number of jobs left in your quota Source code in hyp3_sdk/hyp3.py def check_quota ( self ) -> int : \"\"\" Returns: The number of jobs left in your quota \"\"\" info = self . my_info () return info [ 'quota' ][ 'remaining' ] find_jobs ( self , start = None , end = None , status = None , name = None ) Gets a Batch of jobs from HyP3 matching the provided search criteria Parameters: Name Type Description Default start Optional[datetime.datetime] only jobs submitted after given time None end Optional[datetime.datetime] only jobs submitted before given time None status Optional[str] only jobs matching this status (SUCCEEDED, FAILED, RUNNING, PENDING) None name Optional[str] only jobs with this name None Returns: Type Description Batch A Batch object containing the found jobs Source code in hyp3_sdk/hyp3.py def find_jobs ( self , start : Optional [ datetime ] = None , end : Optional [ datetime ] = None , status : Optional [ str ] = None , name : Optional [ str ] = None ) -> Batch : \"\"\"Gets a Batch of jobs from HyP3 matching the provided search criteria Args: start: only jobs submitted after given time end: only jobs submitted before given time status: only jobs matching this status (SUCCEEDED, FAILED, RUNNING, PENDING) name: only jobs with this name Returns: A Batch object containing the found jobs \"\"\" params = {} if name is not None : params [ 'name' ] = name if start is not None : params [ 'start' ] = start . isoformat ( timespec = 'seconds' ) if start . tzinfo is None : params [ 'start' ] += 'Z' if end is not None : params [ 'end' ] = end . isoformat ( timespec = 'seconds' ) if end . tzinfo is None : params [ 'end' ] += 'Z' if status is not None : params [ 'status_code' ] = status response = self . session . get ( urljoin ( self . url , '/jobs' ), params = params ) try : response . raise_for_status () except HTTPError : raise HyP3Error ( f 'Error while trying to query { response . url } ' ) jobs = [ Job . from_dict ( job ) for job in response . json ()[ 'jobs' ]] return Batch ( jobs ) my_info ( self ) Returns: Type Description dict Your user information Source code in hyp3_sdk/hyp3.py def my_info ( self ) -> dict : \"\"\" Returns: Your user information \"\"\" try : response = self . session . get ( urljoin ( self . url , '/user' )) response . raise_for_status () except HTTPError : raise HyP3Error ( 'Unable to get user information from API' ) return response . json () refresh ( self , job_or_batch ) Refresh each jobs' information Parameters: Name Type Description Default job_or_batch Union[hyp3_sdk.jobs.Batch, hyp3_sdk.jobs.Job] A Batch of Job object to refresh required Returns: Type Description Union[hyp3_sdk.jobs.Batch, hyp3_sdk.jobs.Job] obj: A Batch or Job object with refreshed information Source code in hyp3_sdk/hyp3.py @singledispatchmethod def refresh ( self , job_or_batch : Union [ Batch , Job ]) -> Union [ Batch , Job ]: \"\"\"Refresh each jobs' information Args: job_or_batch: A Batch of Job object to refresh Returns: obj: A Batch or Job object with refreshed information \"\"\" raise NotImplementedError ( f 'Cannot refresh { type ( job_or_batch ) } type object' ) submit_autorift_job ( self , granule1 , granule2 , name = None ) Submit an autoRIFT job Parameters: Name Type Description Default granule1 str The first granule (scene) to use required granule2 str The second granule (scene) to use required name Optional[str] A name for the job (must be <= 20 characters) None Returns: Type Description Job A Batch object containing the autoRIFT job Source code in hyp3_sdk/hyp3.py def submit_autorift_job ( self , granule1 : str , granule2 : str , name : Optional [ str ] = None ) -> Job : \"\"\"Submit an autoRIFT job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job (must be <= 20 characters) Returns: A Batch object containing the autoRIFT job \"\"\" job_dict = { 'job_parameters' : { 'granules' : [ granule1 , granule2 ]}, 'job_type' : 'AUTORIFT' , } return self . submit_job_dict ( job_dict = job_dict , name = name ) submit_insar_job ( self , granule1 , granule2 , name = None , ** kwargs ) Submit an InSAR job Parameters: Name Type Description Default granule1 str The first granule (scene) to use required granule2 str The second granule (scene) to use required name Optional[str] A name for the job (must be <= 20 characters) None **kwargs Extra job parameters specifying custom processing options {} Returns: Type Description Job A Batch object containing the InSAR job Source code in hyp3_sdk/hyp3.py def submit_insar_job ( self , granule1 : str , granule2 : str , name : Optional [ str ] = None , ** kwargs ) -> Job : \"\"\"Submit an InSAR job Args: granule1: The first granule (scene) to use granule2: The second granule (scene) to use name: A name for the job (must be <= 20 characters) **kwargs: Extra job parameters specifying custom processing options Returns: A Batch object containing the InSAR job \"\"\" job_dict = { 'job_parameters' : { 'granules' : [ granule1 , granule2 ], ** kwargs }, 'job_type' : 'INSAR_GAMMA' , } return self . submit_job_dict ( job_dict = job_dict , name = name ) submit_rtc_job ( self , granule , name = None , ** kwargs ) Submit an RTC job Parameters: Name Type Description Default granule str The granule (scene) to use required name Optional[str] A name for the job (must be <= 20 characters) None **kwargs Extra job parameters specifying custom processing options {} Returns: Type Description Job A Batch object containing the RTC job Source code in hyp3_sdk/hyp3.py def submit_rtc_job ( self , granule : str , name : Optional [ str ] = None , ** kwargs ) -> Job : \"\"\"Submit an RTC job Args: granule: The granule (scene) to use name: A name for the job (must be <= 20 characters) **kwargs: Extra job parameters specifying custom processing options Returns: A Batch object containing the RTC job \"\"\" job_dict = { 'job_parameters' : { 'granules' : [ granule ], ** kwargs }, 'job_type' : 'RTC_GAMMA' , } return self . submit_job_dict ( job_dict = job_dict , name = name ) watch ( self , job_or_batch , timeout = 10800 , interval = 60 ) Watch jobs until they complete Parameters: Name Type Description Default job_or_batch Union[hyp3_sdk.jobs.Batch, hyp3_sdk.jobs.Job] A Batch or Job object of jobs to watch required timeout int How long to wait until exiting in seconds 10800 interval Union[int, float] How often to check for updates in seconds 60 Returns: Type Description A Batch or Job object with refreshed watched jobs Source code in hyp3_sdk/hyp3.py def watch ( self , job_or_batch : Union [ Batch , Job ], timeout : int = 10800 , interval : Union [ int , float ] = 60 ): \"\"\"Watch jobs until they complete Args: job_or_batch: A Batch or Job object of jobs to watch timeout: How long to wait until exiting in seconds interval: How often to check for updates in seconds Returns: A Batch or Job object with refreshed watched jobs \"\"\" end_time = datetime . now () + timedelta ( seconds = timeout ) while datetime . now () < end_time : job_or_batch = self . refresh ( job_or_batch ) if job_or_batch . complete (): return job_or_batch time . sleep ( interval ) raise HyP3Error ( 'Timeout occurred while waiting for jobs' ) jobs \u00b6 Batch \u00b6 any_expired ( self ) Check succeeded jobs for expiration Source code in hyp3_sdk/jobs.py def any_expired ( self ) -> bool : \"\"\"Check succeeded jobs for expiration\"\"\" for job in self . jobs : try : if job . expired (): return True except HyP3Error : continue return False complete ( self ) Returns: True if all jobs are complete, otherwise returns False Source code in hyp3_sdk/jobs.py def complete ( self ) -> bool : \"\"\" Returns: True if all jobs are complete, otherwise returns False \"\"\" for job in self . jobs : if not job . complete (): return False return True download_files ( self , location = '' ) Parameters: Name Type Description Default location Union[pathlib.Path, str] Directory location to put files into '' Returns: list of Path objects to downloaded files Source code in hyp3_sdk/jobs.py def download_files ( self , location : Union [ Path , str ] = '' ) -> List [ Path ]: \"\"\" Args: location: Directory location to put files into Returns: list of Path objects to downloaded files \"\"\" if not self . complete (): raise HyP3Error ( 'Incomplete jobs cannot be downloaded' ) downloaded_files = [] for job in self . jobs : downloaded_files . extend ( job . download_files ( location )) return downloaded_files filter_jobs ( self , succeeded = True , running = True , failed = False , include_expired = True ) Filter jobs by status. By default, only succeeded and still running jobs will be in the returned batch. Parameters: Name Type Description Default succeeded bool Include all succeeded jobs True running bool Include all running jobs True failed bool Include all failed jobs False include_expired bool Include expired jobs in the result True Returns: Type Description Batch batch: A batch object containing jobs matching all the selected statuses Source code in hyp3_sdk/jobs.py def filter_jobs ( self , succeeded : bool = True , running : bool = True , failed : bool = False , include_expired : bool = True , ) -> 'Batch' : \"\"\"Filter jobs by status. By default, only succeeded and still running jobs will be in the returned batch. Args: succeeded: Include all succeeded jobs running: Include all running jobs failed: Include all failed jobs include_expired: Include expired jobs in the result Returns: batch: A batch object containing jobs matching all the selected statuses \"\"\" filtered_jobs = [] for job in self . jobs : if job . succeeded () and succeeded : if include_expired or not job . expired (): filtered_jobs . append ( job ) elif job . running () and running : filtered_jobs . append ( job ) elif job . failed () and failed : filtered_jobs . append ( job ) return Batch ( filtered_jobs ) succeeded ( self ) Returns: True if all jobs have succeeded, otherwise returns False Source code in hyp3_sdk/jobs.py def succeeded ( self ) -> bool : \"\"\" Returns: True if all jobs have succeeded, otherwise returns False \"\"\" for job in self . jobs : if not job . succeeded (): return False return True Job \u00b6 download_files ( self , location = '' ) Parameters: Name Type Description Default location Union[pathlib.Path, str] Directory location to put files into '' Returns: list of Path objects to downloaded files Source code in hyp3_sdk/jobs.py def download_files ( self , location : Union [ Path , str ] = '' ) -> List [ Path ]: \"\"\" Args: location: Directory location to put files into Returns: list of Path objects to downloaded files \"\"\" location = Path ( location ) if not self . complete (): raise HyP3Error ( 'Incomplete jobs cannot be downloaded' ) downloaded_files = [] for file in self . files : download_url = file [ 'url' ] filename = location / file [ 'filename' ] try : downloaded_files . append ( download_file ( download_url , filename )) except Exception : raise HyP3Error ( 'unable to download file' ) return downloaded_files util \u00b6 Extra utilities for working with HyP3 download_file ( url , filepath , chunk_size = None , retries = 2 , backoff_factor = 1 ) \u00b6 Download a file Parameters: Name Type Description Default url str URL of the file to download required filepath Union[pathlib.Path, str] Location to place file into required chunk_size Size to chunk the download into None retries Number of retries to attempt 2 backoff_factor Factor for calculating time between retries 1 Returns: Type Description Path download_path: The path to the downloaded file Source code in hyp3_sdk/util.py def download_file ( url : str , filepath : Union [ Path , str ], chunk_size = None , retries = 2 , backoff_factor = 1 ) -> Path : \"\"\"Download a file Args: url: URL of the file to download filepath: Location to place file into chunk_size: Size to chunk the download into retries: Number of retries to attempt backoff_factor: Factor for calculating time between retries Returns: download_path: The path to the downloaded file \"\"\" filepath = Path ( filepath ) session = requests . Session () retry_strategy = Retry ( total = retries , backoff_factor = backoff_factor , status_forcelist = [ 429 , 500 , 502 , 503 , 504 ], ) session . mount ( 'https://' , HTTPAdapter ( max_retries = retry_strategy )) session . mount ( 'http://' , HTTPAdapter ( max_retries = retry_strategy )) with session . get ( url , stream = True ) as s : s . raise_for_status () with open ( filepath , \"wb\" ) as f : for chunk in s . iter_content ( chunk_size = chunk_size ): if chunk : f . write ( chunk ) session . close () return filepath get_authenticated_session ( username , password ) \u00b6 logs into hyp3 using credentials for urs.earthdata.nasa.gov from provided credentails or a .netrc file. Returns: Type Description Session An authenticated Session object from the requests library Source code in hyp3_sdk/util.py def get_authenticated_session ( username : str , password : str ) -> requests . Session : \"\"\"logs into hyp3 using credentials for urs.earthdata.nasa.gov from provided credentails or a .netrc file. Returns: An authenticated Session object from the requests library \"\"\" s = requests . Session () if hyp3_sdk . TESTING : return s if ( username and password ) is not None : try : response = s . get ( AUTH_URL , auth = ( username , password )) response . raise_for_status () except requests . HTTPError : raise AuthenticationError ( 'Was not able to authenticate with credentials provided \\n ' 'This could be due to invalid credentials or a connection error.' ) else : try : response = s . get ( AUTH_URL ) response . raise_for_status () except requests . HTTPError : raise AuthenticationError ( 'Was not able to authenticate with .netrc file and no credentials provided \\n ' 'This could be due to invalid credentials in .netrc or a connection error.' ) return s","title":"SDK"},{"location":"using/sdk/#hyp3-sdk","text":"A python wrapper around the HyP3 API >>> from hyp3_sdk import HyP3 >>> hyp3 = HyP3 ( username = 'MyUsername' , password = 'MyPassword' ) >>> job = hyp3 . submit_rtc_job ( granule = 'S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8' , name = 'MyNewJob' ) >>> job = hyp3 . watch ( job ) >>> job . download_files ()","title":"HyP3 SDK"},{"location":"using/sdk/#install","text":"The HyP3 SDK can be installed via Anaconda/Miniconda : conda install -c conda-forge hyp3_sdk Or using pip : python -m pip install hyp3_sdk","title":"Install"},{"location":"using/sdk/#quickstart","text":"There are 3 main classes that the SDK exposes: HyP3 : to perform HyP3 operations (find jobs, refresh job information, submitting new jobs) Job : to perform operations on single jobs (downloading products, check status) Batch : to perform operations on multiple jobs at once (downloading products, check status) An instance of the HyP3 class will be needed to interact with the external HyP3 API. from hyp3_sdk import HyP3 # Must either have credentials for urs.earthdata.nasa.gov in a .netrc hyp3 = HyP3 () # or provide them in the username and password keyword args hyp3 = HyP3 ( username = 'MyUsername' , password = 'MyPassword' )","title":"Quickstart"},{"location":"using/sdk/#submitting-jobs","text":"hyp3 has member functions for submitting new jobs: rtc_job = hyp3 . submit_rtc_job ( 'granule_id' , 'job_name' ) insar_job = hyp3 . submit_insar_job ( 'reference_granule_id' , 'secondary_granule_id' , 'job_name' ) autorift_job = hyp3 . submit_autorift_job ( 'reference_granule_id' , 'secondary_granule_id' , 'job_name' ) Each of these functions will return an instance of the Job class that represents a new HyP3 job request.","title":"Submitting Jobs"},{"location":"using/sdk/#finding-existing-jobs","text":"To find HyP3 jobs that were run previously, you can use the hyp3.find_jobs() batch = hyp3 . find_jobs () This will return a Batch instance representing all jobs owned by you. You can also pass parameters to query to a specific set of jobs","title":"Finding Existing Jobs"},{"location":"using/sdk/#operations-on-job-and-batch","text":"If your jobs are not complete you can use the HyP3 instance to update them, and wait from completion batch = hyp3 . find_jobs () if not batch . complete (): # to get updated information batch = hyp3 . refresh ( batch ) # or to wait until completion and get updated information (which will take a fair bit) batch = hyp3 . watch ( batch ) Once you have complete jobs you can download the products to your machine batch . download_files () These operations also work on Job objects job = hyp3 . submit_rtc_job ( 'S1A_IW_SLC__1SSV_20150621T120220_20150621T120232_006471_008934_72D8' , 'MyJobName' ) job = hyp3 . watch ( job ) job . download_files ()","title":"Operations on Job and Batch"},{"location":"using/sdk/#sdk-api-reference","text":"","title":"SDK API Reference"},{"location":"using/sdk/#hyp3_sdk","text":"A python wrapper around the HyP3 API","title":"hyp3_sdk"},{"location":"using/sdk/#hyp3_sdk.exceptions","text":"Errors and exceptions to raise when the SDK runs into problems","title":"exceptions"},{"location":"using/sdk/#hyp3_sdk.exceptions.AuthenticationError","text":"Raise when authentication does not succeed","title":"AuthenticationError"},{"location":"using/sdk/#hyp3_sdk.exceptions.HyP3Error","text":"Base Exception for Hyp3_sdk","title":"HyP3Error"},{"location":"using/sdk/#hyp3_sdk.exceptions.ValidationError","text":"Raise when jobs do not pass validation","title":"ValidationError"},{"location":"using/sdk/#hyp3_sdk.hyp3","text":"","title":"hyp3"},{"location":"using/sdk/#hyp3_sdk.hyp3.HyP3","text":"A python wrapper around the HyP3 API","title":"HyP3"},{"location":"using/sdk/#hyp3_sdk.jobs","text":"","title":"jobs"},{"location":"using/sdk/#hyp3_sdk.jobs.Batch","text":"","title":"Batch"},{"location":"using/sdk/#hyp3_sdk.jobs.Job","text":"","title":"Job"},{"location":"using/sdk/#hyp3_sdk.util","text":"Extra utilities for working with HyP3","title":"util"},{"location":"using/sdk/#hyp3_sdk.util.download_file","text":"Download a file Parameters: Name Type Description Default url str URL of the file to download required filepath Union[pathlib.Path, str] Location to place file into required chunk_size Size to chunk the download into None retries Number of retries to attempt 2 backoff_factor Factor for calculating time between retries 1 Returns: Type Description Path download_path: The path to the downloaded file Source code in hyp3_sdk/util.py def download_file ( url : str , filepath : Union [ Path , str ], chunk_size = None , retries = 2 , backoff_factor = 1 ) -> Path : \"\"\"Download a file Args: url: URL of the file to download filepath: Location to place file into chunk_size: Size to chunk the download into retries: Number of retries to attempt backoff_factor: Factor for calculating time between retries Returns: download_path: The path to the downloaded file \"\"\" filepath = Path ( filepath ) session = requests . Session () retry_strategy = Retry ( total = retries , backoff_factor = backoff_factor , status_forcelist = [ 429 , 500 , 502 , 503 , 504 ], ) session . mount ( 'https://' , HTTPAdapter ( max_retries = retry_strategy )) session . mount ( 'http://' , HTTPAdapter ( max_retries = retry_strategy )) with session . get ( url , stream = True ) as s : s . raise_for_status () with open ( filepath , \"wb\" ) as f : for chunk in s . iter_content ( chunk_size = chunk_size ): if chunk : f . write ( chunk ) session . close () return filepath","title":"download_file()"},{"location":"using/sdk/#hyp3_sdk.util.get_authenticated_session","text":"logs into hyp3 using credentials for urs.earthdata.nasa.gov from provided credentails or a .netrc file. Returns: Type Description Session An authenticated Session object from the requests library Source code in hyp3_sdk/util.py def get_authenticated_session ( username : str , password : str ) -> requests . Session : \"\"\"logs into hyp3 using credentials for urs.earthdata.nasa.gov from provided credentails or a .netrc file. Returns: An authenticated Session object from the requests library \"\"\" s = requests . Session () if hyp3_sdk . TESTING : return s if ( username and password ) is not None : try : response = s . get ( AUTH_URL , auth = ( username , password )) response . raise_for_status () except requests . HTTPError : raise AuthenticationError ( 'Was not able to authenticate with credentials provided \\n ' 'This could be due to invalid credentials or a connection error.' ) else : try : response = s . get ( AUTH_URL ) response . raise_for_status () except requests . HTTPError : raise AuthenticationError ( 'Was not able to authenticate with .netrc file and no credentials provided \\n ' 'This could be due to invalid credentials in .netrc or a connection error.' ) return s","title":"get_authenticated_session()"},{"location":"using/vertex/","text":"Radiometric Terrain Correction for Sentinel-1 in Vertex \u00b6 The Alaska Satellite Facility now offers Radiometric Terrain Correction of Sentinel-1 scenes through ASF Data Search - Vertex. You can now submit scenes to be processed into normalized radar backscatter products on your behalf, avoiding the cost and complexity of performing such processing yourself. SAR datasets inherently contain geometric and radiometric distortions due to terrain being imaged by a side-looking instrument. Radiometric terrain correction (RTC) removes these distortions and creates analysis-ready data suitable for use in GIS applications. RTC processing is a required first step for many amplitude-based SAR applications. Sentinel-1 RTC products are generated by ASF's HyP3 processing platform leveraging GAMMA Software. Products are distributed as UTM-projected GeoTIFFs with a pixel spacing of 30 meters. To learn more, visit the ASF Sentinel-1 RTC Product Guide . Getting Started \u00b6 To request RTC products, visit ASF Data Search - Vertex and sign in using your Earthdata profile. Select your scenes - RTC processing is available for Sentinel-1 GRD-H and SLC scenes with a beam mode of IW. When searching for Sentinel-1 scenes, an Add RTC to On Demand queue option appears for these files. Submit your request - After selecting your scenes, access the On Demand queue to submit your processing request. You may process up to 200 scenes per month. Monitor your request - The new On Demand Products search type displays your running and completed requests. New requests typically require 20-60 minutes before a finished RTC product is available. Download your data - Finished RTC products can be downloaded after an On Demand Products search either directly or via your download queue . On Demand products are retained and available to download for two weeks after processing. Feedback \u00b6 ASF would love to hear from you! Let us know what you liked, didn't like, or what we can do better. For data search and general feedback email ASF User Services . For RTC product and custom processing feedback, chat with us on gitter or open an issue . Not sure which? Email ASF User Services .","title":"Vertex"},{"location":"using/vertex/#radiometric-terrain-correction-for-sentinel-1-in-vertex","text":"The Alaska Satellite Facility now offers Radiometric Terrain Correction of Sentinel-1 scenes through ASF Data Search - Vertex. You can now submit scenes to be processed into normalized radar backscatter products on your behalf, avoiding the cost and complexity of performing such processing yourself. SAR datasets inherently contain geometric and radiometric distortions due to terrain being imaged by a side-looking instrument. Radiometric terrain correction (RTC) removes these distortions and creates analysis-ready data suitable for use in GIS applications. RTC processing is a required first step for many amplitude-based SAR applications. Sentinel-1 RTC products are generated by ASF's HyP3 processing platform leveraging GAMMA Software. Products are distributed as UTM-projected GeoTIFFs with a pixel spacing of 30 meters. To learn more, visit the ASF Sentinel-1 RTC Product Guide .","title":"Radiometric Terrain Correction for Sentinel-1 in Vertex"},{"location":"using/vertex/#getting-started","text":"To request RTC products, visit ASF Data Search - Vertex and sign in using your Earthdata profile. Select your scenes - RTC processing is available for Sentinel-1 GRD-H and SLC scenes with a beam mode of IW. When searching for Sentinel-1 scenes, an Add RTC to On Demand queue option appears for these files. Submit your request - After selecting your scenes, access the On Demand queue to submit your processing request. You may process up to 200 scenes per month. Monitor your request - The new On Demand Products search type displays your running and completed requests. New requests typically require 20-60 minutes before a finished RTC product is available. Download your data - Finished RTC products can be downloaded after an On Demand Products search either directly or via your download queue . On Demand products are retained and available to download for two weeks after processing.","title":"Getting Started"},{"location":"using/vertex/#feedback","text":"ASF would love to hear from you! Let us know what you liked, didn't like, or what we can do better. For data search and general feedback email ASF User Services . For RTC product and custom processing feedback, chat with us on gitter or open an issue . Not sure which? Email ASF User Services .","title":"Feedback"}]}